\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{sutton1998introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Agent-environment interaction in sequential decision problems.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sequential_decision_problem}{{1}{2}{Agent-environment interaction in sequential decision problems.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Basics of Reinforcement Learning}{2}{section.2}}
\newlabel{sec:basics_reinforcement_learning}{{2}{2}{Basics of Reinforcement Learning}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Markov Decision Processes}{2}{subsection.2.1}}
\newlabel{sec:markov_decision_processes}{{2.1}{2}{Markov Decision Processes}{subsection.2.1}{}}
\citation{sutton1998introduction}
\citation{szepesvari2010algorithms}
\citation{wiering2012reinforcement}
\citation{peters2008reinforcement}
\newlabel{eq:bellman_expectation_eq_V}{{7}{3}{Markov Decision Processes}{equation.2.7}{}}
\newlabel{eq:bellman_expectation_eq_Q}{{8}{3}{Markov Decision Processes}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Policy Gradient Methods}{3}{subsection.2.2}}
\citation{kushner2003stochastic}
\citation{sutton1999policy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Policy Gradient Theorem}{4}{subsubsection.2.2.1}}
\newlabel{thm:risk_neutral_policy_gradient}{{2.1}{4}{Policy Gradient}{theorem.2.1}{}}
\citation{peters2008reinforcement}
\citation{baxter2001infinite}
\citation{sehnke2008policy}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces GPOMDP\relax }}{5}{algorithm.1}}
\newlabel{algo:GPOMDP}{{1}{5}{GPOMDP\relax }{algorithm.1}{}}
\newlabel{eq:pg_theorem_baseline}{{25}{5}{Policy Gradient Theorem}{equation.2.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Parameter-Based Policy Gradient Methods}{5}{subsubsection.2.2.2}}
\citation{zhao2011analysis}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reinforcement Learning for Systematic Trading}{6}{section.3}}
\newlabel{sec:application_to_systematic_trading}{{3}{6}{Reinforcement Learning for Systematic Trading}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Asset Allocation With Transaction Costs}{6}{subsection.3.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Episodic PGPE algorithm\relax }}{7}{algorithm.2}}
\newlabel{algo:PGPE}{{2}{7}{Episodic PGPE algorithm\relax }{algorithm.2}{}}
\newlabel{eq:portfolio_return}{{34}{7}{Asset Allocation With Transaction Costs}{equation.3.34}{}}
\newlabel{eq:portfolio_return_benchmark}{{36}{8}{Asset Allocation With Transaction Costs}{equation.3.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Reinforcement Learning Application}{8}{subsection.3.2}}
\citation{pybrain2010jmlr}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces PyBrain standard architecture for an RL problem.\relax }}{9}{figure.caption.2}}
\newlabel{fig:pybrain}{{2}{9}{PyBrain standard architecture for an RL problem.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Python Prototype}{9}{section.4}}
\newlabel{sec:python_prototype}{{4}{9}{Python Prototype}{section.4}{}}
\citation{joshi2008c++}
\@writefile{toc}{\contentsline {section}{\numberline {5}C++ Implementation}{10}{section.5}}
\newlabel{sec:c++_implementation}{{5}{10}{C++ Implementation}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\lstinline {Environment}, \lstinline {Task}, \lstinline {Agent} and \lstinline {Experiment}}{10}{subsection.5.1}}
\newlabel{RF1}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Class architecture for the learning process in the asset allocation problem.\relax }}{11}{figure.caption.3}}
\newlabel{fig:class_architecture_base}{{3}{11}{Class architecture for the learning process in the asset allocation problem.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\lstinline {ARACAgent}}{12}{subsection.5.2}}
\newlabel{RF2}{13}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Class architecture for an Average Reward Actor-Critic agent (ARAC).\relax }}{13}{figure.caption.4}}
\newlabel{fig:class_architecture_arac}{{4}{13}{Class architecture for an Average Reward Actor-Critic agent (ARAC).\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Execution Pipeline}{14}{section.6}}
\newlabel{sec:execution_pipeline}{{6}{14}{Execution Pipeline}{section.6}{}}
\newlabel{RF3}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Execution flow of an asset allocation experiment via the \lstinline {experimentLauncher.py} script\relax }}{15}{figure.caption.5}}
\newlabel{fig:pybrain}{{5}{15}{Execution flow of an asset allocation experiment via the \lstinline {experimentLauncher.py} script\relax }{figure.caption.5}{}}
\bibstyle{unsrt}
\bibdata{Bibliography/bibliography}
\bibcite{sutton1998introduction}{1}
\bibcite{szepesvari2010algorithms}{2}
\bibcite{wiering2012reinforcement}{3}
\bibcite{peters2008reinforcement}{4}
\bibcite{kushner2003stochastic}{5}
\bibcite{sutton1999policy}{6}
\bibcite{baxter2001infinite}{7}
\bibcite{sehnke2008policy}{8}
\bibcite{zhao2011analysis}{9}
\bibcite{pybrain2010jmlr}{10}
\bibcite{joshi2008c++}{11}
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical Results}{16}{section.7}}
\newlabel{sec:numerical_results}{{7}{16}{Numerical Results}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{16}{section.8}}
\newlabel{sec:conclusion}{{8}{16}{Conclusion}{section.8}{}}
