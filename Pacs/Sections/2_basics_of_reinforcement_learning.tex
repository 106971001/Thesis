\section{Basics of Reinforcement Learning}
\label{sec:basics_reinforcement_learning}

\emph{Reinforcement Learning} (RL) is a general class of algorithms in the field of machine learning that allows an agent to learn how to behave in a stochastic and possibly unknown environment, where the only feedback consists of a scalar reward signal \cite{sutton1998introduction}. The goal of the agent is to learn by trial-and-error which actions maximize his long-run rewards. However, since the environment evolves stochastically and may be influenced by the actions chosen, the agent must balance his desire to obtain a large immediate reward by acting greedily and the opportunities that will be available in the future. Thus, RL algorithms can be seen as computational methods to solve sequential decision problems by directly interacting with the environment.\\

\subsection{Markov Decision Processes}
\label{sec:markov_decision_processes}
Sequential decision problems are typically formalized using \emph{Markov Decision Processes} (MDP). An MDP is a stochastic dynamical system specified by the tuple $<\S, \A, \calP, \calR, \gamma>$, where $(\S, \calS)$ is a measurable state space, $(\A, \calA)$ is a measurable action space, $\calP: \S \times \A \times \calS \to \R$ is a Markov transition kernel, $\calR: \S \times \A \to \R$ is a reward function and $0 < \gamma < 1$ is the discount factor. Suppose that at time $t$ the system is in state $S_t = s$ and that the agent takes action $A_t = a$, then, regardless of the previous history of the system, the probability to find the system in a state belonging to $B\in\calS$ at time $t+1$ is given by 
\begin{equation}
	\calP(s, a, B) = \P{S_{t+1} \in B | S_t = s, A_t = a}
\end{equation}
Following this random transition, the agent receives a stochastic reward
$R_{t+1}$. The reward function $\calR(s, a)$ gives the expected reward
obtained when action $a$ is taken in state $s$, i.e. 
\begin{equation}
	\calR(s, a) = \E{R_{t+1} | S_t = s, A_t = a}
\end{equation}
This feedback mechanism between the environment and the agent is illustrated in Figure \ref{fig:sequential_decision_problem}. At any time step, the agent selects his actions according to a certain policy $\pi: \S \times \calA \to \R$ such that for every $s \in \S$, $C \mapsto \pi(s,C)$ is a probability distribution over $(\A, \calA)$. Hence, a policy $\pi$ and an initial state $s_0 \in \S$ determine a random state-action-reward sequence ${\{(S_t, A_t, R_{t+1})\}}_{t\geq 0}$ with values on $\S \times \A \times \R$.
\begin{figure}[t]
	\centering
	\begin{tikzpicture}[node distance = 6em, auto, thick]
		\node [block] (Agent) {Agent};
		\node [block, below of=Agent] (Environment) {Environment};		    
		\path [line] (Agent.0) --++ (4em,0em) |- node [near start]{Action $a_t$} (Environment.0);
		\path [line] (Environment.190) --++ (-6em,0em) |- node [near start]{State  $s_{t}$} (Agent.170);
		\path [line] (Environment.170) --++ (-4.25em,0em) |- node [near start, right] {Reward $r_{t+1}$} (Agent.190);
	\end{tikzpicture}
	\caption{Agent-environment interaction in sequential decision problems.}
	\label{fig:sequential_decision_problem}
\end{figure}
In an infinite horizon task, the agent's performance is typically measured as the total discounted reward obtained following a specific policy
\begin{equation}
	G_t = \sum^{\infty}_{t=0} \gamma^t R_{t+k+1} 
\end{equation}
Since this gain is stochastic, the agent considers its expected value, which is typically called \emph{state-value function}
\begin{equation}
	V_\pi(s) = \E[\pi]{G_t|S_t = s}
\end{equation}
where the subscript in $\mathbb{E}_{\pi}$ indicates that all the actions are selected according to policy $\pi$. The state-value function measures how good it is for the agent to be in a given state and follow a certain policy. Similarly, we introduce the \emph{action-value function}
\begin{equation}
	Q_\pi(s,a) = \E[\pi]{G_t|S_t = s, A_t = a}
\end{equation}
We have the following relationship between $V_\pi$ and $Q_\pi$
\begin{equation}
	V_\pi(s) = \int_\A \pi(s,a) Q_\pi(s,a) da
\end{equation}
Almost all reinforcement learning algorithms are designed to estimate these 
value functions and are typically based on the Bellman equations.
\begin{equation}
	V_\pi(s) = \calR_\pi(s) + \gamma T_\pi V_\pi(s)	
	\label{eq:bellman_expectation_eq_V}
\end{equation}
\begin{equation}
		Q_\pi(s,a) = \calR(s,a) + \gamma T_a V_\pi(s)
		\label{eq:bellman_expectation_eq_Q}
\end{equation}
where we denoted by $T_a$ (resp. $T_\pi$) the transition operator for action 
$a$ (resp. for policy $\pi$)
\begin{equation}
	T_a F(s) = \E{F(S_{t+1})|S_t = s, A_t = a} = \int_\S \calP(s, a, s') F(s') ds'
\end{equation}
\begin{equation}
	T_\pi F(s) = \E[\pi]{F(S_{t+1})|S_t = s} = \int_\A \pi(s,a) \int_\S \calP(s,a,s') F(s') ds'	da
\end{equation}
These equations can be rewritten as fixed-point equations which, under some formal assumptions on the reward functions, admit a unique solution by the contraction mapping theorem. The agent's goal is to select a policy $\pi_*$ that maximizes his expected return in all possible states. Such a policy is called \emph{optimal} and the corrisponding value functions are called \emph{Optimal State-Value Function}
\begin{equation}
	V_*(s) = \sup_\pi V_\pi(s)
\end{equation}
and \emph{Optimal Action-Value Function}
\begin{equation}
	Q_*(s,a) = \sup_\pi Q_\pi(s,a)
\end{equation}
The optimal value functions satisfy the following Bellman equations.
\begin{equation}
	V_*(s) = \sup_a Q_*(s,a) = \sup_a \left\{\calR(s,a) + \gamma T_a V_*(s)\right\}
\end{equation}
\begin{equation}
	\begin{split}
		Q_*(s,a) &= \calR(s,a) + \gamma T_a V_*(s)\\
			 &= \calR(s,a) + \gamma \int_\S \calP(s,a,s') \sup_{a'} Q_*(s', a') ds'
	\end{split}
\end{equation}
Again, these are fixed-point equations for which the existence and uniqueness of a solution is guaranteed by the contraction mapping theorem. Given the optimal action-value function $Q_*$, an optimal policy is obtained by selecting in each state the action with maximizes $Q_*$
\begin{equation}
	a_* = \argsup_a Q_*(s,a)
\end{equation}
This greedy policy is deterministic and only depends on the current state of the system.

\subsection{Policy Gradient Methods}
The standard way to solve MDPs is through dynamic programming, which simply consists in solving the Bellman fixed-point equations discussed in the previous chapter. Following this approach, the problem of finding the optimal policy is transformed into the problem of finding the optimal value function. However, apart from the simplest cases where the MDP has a limited number of states and actions, dynamic programming becomes computationally infeasible. Moreover, this approach requires complete knowledge of the Markov transition kernel and of the reward function, which in many real-world applications might be unknown or too complex to use. \emph{Reinforcement Learning} (RL) is a subfield of Machine Learning which aims to turn the infeasible dynamic programming methods into practical algorithms that can be applied to large-scale problems. RL algorithms are based on two key ideas: the first is to use samples to compactly represent the unknown dynamics of the controlled system. The second idea is to use powerful function approximation methods to compactly estimate value functions and policies in high-dimensional state and action spaces. In this section we will only focus on a particular class of algorithms called \emph{Policy Gradient Methods}, which have proved successful in many applications. For a more complete introduction to RL, the reader may consult \cite{sutton1998introduction}, \cite{szepesvari2010algorithms} or \cite{wiering2012reinforcement}.\\
In \emph{policy gradient methods} \cite{peters2008reinforcement}, the optimal policy is approximated using a parametrized policy $\pi: \S \times \calA \times \Theta \to \R$ such that, given a parameter vector $\theta \in \Theta \subseteq \R^{D_\theta}$, $\pi(s, B; \theta) = \pi_\theta(s, B)$ gives the probability of selecting an action in $B \in \calA$ when the system is in state $s \in \S$.
The general goal of policy optimization in reinforcement learning is to
optimize the policy parameters $\theta \in \Theta$ so as to maximize a certain
objective function $J: \Theta \to \R$
\begin{equation}
	\theta^* = \argmax_{\theta \in \Theta} J(\theta)
\end{equation}
In the following, we will focus on gradient-based and model-free methods that exploit
the sequential structure of the the reinforcement learning problem. The idea of
policy gradient algorithms is to update the policy parameters using the gradient ascent direction of the objective function
\begin{equation}
	\theta_{k+1} = \theta_k + \alpha_k \nabla_\theta J\left(\theta_k\right)
\end{equation}
where $\{\alpha_k\}_{k\geq 0}$ is a sequence of learning rates. Typically, the
gradient of the objective function is not known and its approximation is the key component of every policy gradient algorithm. It is a well-know result from stochastic optimization \cite{kushner2003stochastic} that, if the gradient estimate is unbiased and the learning rates satisfy the \emph{Robbins-Monro conditions}
\begin{equation}
	\sum_{k=0}^\infty \alpha_k = \infty \;\;\;\;\;\; \sum^{\infty}_{k=0}
	\alpha_k^2 < \infty 
\end{equation}
the learning process is guaranteed to converge at least to a local optimum of
the objective function. In an episodic environment where the system always starts from an initial state $s_0$, the typical objective function is the start value.
\begin{equation}
	J_{\text{start}}(\theta) = V_{\pi_\theta}(s_0) = \E[\pi_\theta]{G_0 |
 S_0 = s_0}
\end{equation}
In a continuing environment, where no terminal state exists and the task might go on forever, it is common to use either the average value 
\begin{equation}
	J_{\text{avV}}(\theta) = \E[S \sim d^{\theta}]{V_{\pi_\theta}(S)} = \int_\S
	d^{\theta}(s) V_{\pi_\theta}(s) ds
\end{equation}
where $d^\theta$ is the stationary distribution of the Markov chain induced by $\pi_\theta$. Alternatively, one may use the average reward per time step
\begin{equation}
	J_{\text{avR}}(\theta) = \rho(\theta) = \E[\substack{S \sim d^{\theta}\\A \sim \pi_\theta}]{\calR(S,A)} 
	= \int_\S d^{\theta}(s) \int_\A \pi_\theta(s,a) \calR(s,a) da ds
\end{equation}
Luckily, the same methods apply with minor changes to the three objective functions. 

\subsubsection{Policy Gradient Theorem}
The \emph{policy gradient theorem} \cite{sutton1999policy} shows that the gradient can be rewritten in a form suitable for estimation from experience aided by an approximate action-value or advantage function.
\begin{theorem}[Policy Gradient]
\label{thm:risk_neutral_policy_gradient}
	Let $\pi_\theta$ be a differentiable policy. The policy gradient for the average reward formulation is given by
	\begin{equation}
		\nabla_\theta \rho(\theta) =
		\E[\substack{S \sim d^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
		\pi_\theta(S,A) Q_{\theta}(S, A)}
	\end{equation}
	where $d^\theta$ is the stationary distribution of the Markov chain induced by $\pi_\theta$. The policy gradient for the start value formulation is given by
	\begin{equation}
		\nabla_\theta J_{\text{start}}(\theta) =
		\E[\substack{S \sim d_\gamma^\theta(s_0, \cdot)\\A \sim \pi_\theta}]{\nabla_\theta\log
		\pi_\theta(S,A) Q_{\theta}(S, A)}
	\end{equation}
	where $d_\gamma^\theta(s_0, \cdot)$ is the $\gamma$-discounted visiting distribution over states starting from the initial state $s_0$ and following policy $\pi_\theta$
		\begin{equation}
			d_\gamma^\theta(s, x) = \sum_{k=0}^{\infty} \gamma^k \calP_\theta^{(k)}(s, x)
		\end{equation}
\end{theorem}
Let us notice that we can subtract a state-dependent baseline from the action-value function without changing the value of the expectation, indeed
\begin{equation*}
	\begin{split}
	\E[\substack{S \sim d^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
			\pi_\theta(S,A) B_\theta(S)} 
	&= \int_\S d^\theta(s) \int_\A \pi_\theta(s,a) \nabla_\theta\log
				\pi_\theta(s,a) B_\theta(s) da ds\\
	&= \int_\S d^\theta(s)  B_\theta(s) \int_\A \nabla_\theta \pi_\theta(s,a) da ds\\
	&= \int_\S d^\theta(s)  B_\theta(s)  \nabla_\theta  \underbrace{\int_\A  \pi_\theta(s,a) da}_{= 1} ds = 0
	\end{split}
\end{equation*}
Hence, the policy gradient theorem can be rewritten as 
\begin{equation}
\label{eq:pg_theorem_baseline}
	\nabla_\theta \rho(\theta) =
	\E[\substack{S \sim d^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
	\pi_\theta(S,A) \left(Q_{\pi_\theta}(S, A) - B_\theta(S)\right)}
\end{equation}
The baseline can be chosen so as to minimize the variance of the gradient estimate which can prove beneficial for the algorithm convergence \cite{peters2008reinforcement}. This result can be used as the starting point to derive several policy gradient methods that use different approximation of the action-value function, which is typically unknown. For instance, in an episodic MDP the action-value function can be estimated with the total return obtained on a sample trajectory
\begin{equation}
	Q_\theta(s_0,a_0) \approx \sum_{t=0}^{T^{(m)}} \gamma^t r_{t+1}^{(m)}
\end{equation}
Combining this remark with a Monte Carlo approximation of Eq. (\ref{eq:pg_theorem_baseline}), we obtain the GPOMDP algorithm \cite{baxter2001infinite} (also known as \emph{Monte Carlo Policy Gradient}) for which the pseudocode is reported in Algorithm \ref{algo:GPOMDP}.
\begin{algorithm}[t]
	\caption{GPOMDP}
	\label{algo:GPOMDP}
	\begin{algorithmic}[0]
		\Require{\\
			\begin{itemize}
				\item Initial policy parameters $\theta_0$
				\item Learning rate $\{\alpha_k\}$
				\item Number of trajectories $M$
			\end{itemize}
		} 
		\Ensure Approximation of the optimal policy $\pi_{\theta^*} \approx \pi_*$
		\begin{algorithmic}[1]
		\State Initialize $k = 0$
		\Repeat
			\State Sample $M$ trajectories $h^{(m)} = \{(s_t^{(m)}, a_t^{(m)})\}_{t = 0}^{T^{(m)}}$ of the MDP under policy $\pi_{\theta_k}$
			\State Compute the optimal baseline 
			\begin{equation}
				\widehat{b}_k^* = \frac{\sum^{M}_{m=1} \left[ \sum_{i=0}^{T^{(m)}} 
									\partial_{\theta_k} \log \pi_\theta\left(s_i^{(m)}, a_i^{(m)}\right) \right]^2 
									\sum^{T^{(m)}}_{j=0} \gamma^j r_{j+1}^{(m)}}{\sum^{M}_{m=1} \left[ \sum_{i=0}^{T^{(m)}} \partial_{\theta_k} \log \pi_\theta\left(s_i^{(m)}, a_i^{(m)}\right) \right]^2}
			\end{equation}
			\State Approximate policy gradient
			\begin{equation}
				\nabla_\theta J_{\text{start}}(\theta_k) \approx \widehat{g}_k = \frac{1}{M} \sum^{M}_{m=1} \sum_{i=0}^{T^{(m)}} 
					\nabla_\theta \log \pi_{\theta_k}\left(s_i^{(m)}, a_i^{(m)}\right) \left( 
					\sum^{T^{(m)}}_{j=i} \gamma^j r_{j+1}^{(m)} - \widehat{b}_k^* \right)
			\end{equation}
			\State Update actor parameters $\theta_{k+1} = \theta_k + \alpha_k \widehat{g}_k $. 
			
			\State $k \leftarrow k + 1$
		\Until{converged}
		\end{algorithmic}
	\end{algorithmic}
\end{algorithm}

\subsubsection{Parameter-Based Policy Gradient Methods}
In Monte Carlo Policy Gradient, trajectories are generated by sampling at each
time step an action according to a stochastic policy $\pi_\theta$ and the
objective function gradient is estimated by differentiating the policy with
respect to the parameters. However, sampling an action from the policy at each
time step leads to a large variance in the sampled histories and therefore in 
the gradient estimate, which can in turn slow down the convergence of the
learning process. To address this issue, in \cite{sehnke2008policy} the authors
propose the \emph{policy gradient with parameter-based exploration} (PGPE)
method, in which the search in the policy space is replaced with a direct
search in the model parameter space. Given an episodic MDP, PGPE considers a deterministic controller $F: \S \times
\Theta \to \A$ that, given a set of parameters $\theta \in \Theta \subseteq
\R^{D_\theta}$, maps a state $s \in \S$ to an action $a = F(s; \theta) =
F_\theta(s) \in \A$. The policy parameters are drawn from a probability distribution $p_\xi$, with hyper-parameters $\xi \in \Xi \subseteq \R^{D_\xi}$. Combining these two
hypotheses, the agent follows a stochastic policy $\pi_\xi$ defined by
\begin{equation}
	\forall B \in \calA ,\ \pi_\xi(s,B) = \pi(s, B; \xi) = \int_\Theta p_\xi(\theta) 
	\ind{F_{\theta}(s)\in B} d\theta
\end{equation}
In this setting, the policy gradient theorem can be reformulated in the following way 
\begin{theorem}[Parameter-Based Policy Gradient]
	Let $p_\xi$ be differentiable with respect to $\xi$, then the gradient of the average reward is given by
	\begin{equation}
		\nabla_\xi J(\xi) = \E[\substack{S \sim d^\xi\\\theta \sim p_\xi}]{\nabla_\xi \log p_\xi(\theta) Q_{\pi_\xi}(S, \theta)}
	\end{equation}
	where we denoted $Q_\xi(S, \theta) = Q_\xi(S, F_\theta(S))$.
\end{theorem}
This expression is very similar to the original policy gradient theorem, but
the expectation is taken over the controller parameters instead of the action space and we have the likelihood score the controller parameters distribution instead of that of the stochastic policy. Thus, we might interpret this result as if the agent directly selected the parameters $\theta$ according to a policy $p_\xi$, which then lead to an action through the deterministic mapping $F_\theta$. Therefore, it is as if the agent's policy was in the parameters space and not in the control space. As in the standard policy gradient methods, we can subtract a state-dependent baseline $B_\xi(S)$ to the gradient without increasing the bias
\begin{equation}
	\nabla_\xi J(\xi) = \E{\nabla_\xi \log p_\xi(\theta) \left(Q_{\pi_\xi}(S,
						\theta) - B_\xi(S)\right)}
\end{equation}
The gradient can be replaced by its Monte Carlo approximation, where the action-value function is estimated using the returns on a sampled trajectory of the MDP. This leads to the standard PGPE algorithm which is outlined in Algorithm \ref{algo:PGPE}. The advantage of this approach is that the controller is deterministic and therefore the actions do not need to be sampled at each time step, with a consequent reduction of the gradient estimate variance. Indeed, It is sufficient to sample the parameters $\theta$ once at the beginning of the episode and then generate an entire trajectory following the deterministic policy $F_\theta$. As an additional benefit, the parameter gradient is
estimated by direct parameter perturbations, without having to backpropagate
any derivatives, which allows to use non-differentiable controllers.\\

\begin{algorithm}[t]
	\caption{Episodic PGPE algorithm}
	\label{algo:PGPE}
	\begin{algorithmic}[0]
		\Require{\\
			\begin{itemize}
				\item Initial policy parameters $\theta_0$
				\item Learning rate $\{\alpha_k\}$
				\item Number of trajectories $M$
			\end{itemize}
		} 
		\Ensure Approximation of the optimal policy $F_{\xi^*} \approx \pi_*$
		\begin{algorithmic}[1]
			\State Initialize $k = 0$
			\Repeat
				\For {$m = 1, \ldots, M$}
					\State Sample controller parameters $\theta^{(m)} \sim p_{\xi_k}$ 
					\State Sample trajectory $h^{(m)} = \{(s_t^{(m)}, a_t^{(m)})\}_{t \geq 0}$ under policy $F_{\theta^{(m)}}$
				\EndFor
				\State Approximate policy gradient $\nabla_\xi J(\xi_k) \approx \widehat{g}_\text{PGPE}$ using Eq. (\ref{eq:pgpe_gradient})
				\State Update hyperparameters using gradient ascent $\xi_{k+1} = \xi_k + \alpha_k \widehat{g}_\text{PGPE}$
				\State $k \leftarrow k + 1$
			\Until{converged}
		\end{algorithmic}
	\end{algorithmic}
\end{algorithm}

