\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{bertsekas1978stochastic}
\citation{puterman1994markov}
\citation{bertsekas1995dynamic}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Discrete-Time Stochastic Optimal Control}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sequential Decision Problems}{3}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Markov Decision Processes}{4}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Agent-environment interaction in sequential decision problems.\relax }}{5}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sequential_decision_problem}{{2.1}{5}{Agent-environment interaction in sequential decision problems.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Policies}{5}{section.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Discounted Reward Formulation}{6}{section.2.4}}
\newlabel{eq:bellman_expectation_eq_V}{{2.9}{7}{Bellman Expectation Equations}{equation.2.4.9}{}}
\newlabel{eq:bellman_expectation_eq_Q}{{2.10}{7}{Bellman Expectation Equations}{equation.2.4.10}{}}
\citation{mahadevan1996average}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Average Reward Formulation}{9}{section.2.5}}
\citation{prashanth2014actor}
\newlabel{eq:VQ_equality}{{2.24}{10}{Average Reward Formulation}{equation.2.5.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Bellman Equations}{10}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Risk-Sensitive Average Reward Formulation}{11}{section.2.6}}
\newlabel{eq:UW_equality}{{2.35}{12}{Risk-Sensitive Average Reward Formulation}{equation.2.6.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Bellman Equations}{12}{subsection.2.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Risk-Sensitive Control Problem}{12}{subsection.2.6.2}}
\newlabel{eq:risk_sensitive_problem}{{2.38}{12}{Risk-Sensitive Control Problem}{equation.2.6.38}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Reinforcement Learning}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Reinforcement Learning Problem}{13}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Policy Gradient}{13}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Basics of Policy Gradient}{14}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Finite Differences}{15}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Monte Carlo Policy Gradient}{15}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Policy Gradient Theorem}{17}{subsection.3.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.1}Actor-Critic Policy Gradient}{17}{subsubsection.3.2.4.1}}
\citation{sehnke2008policy}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Natural Policy Gradient}{18}{subsection.3.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Policy Gradient with Parameter Exploration}{18}{subsection.3.2.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.6.1}Episodic PGPE}{18}{subsubsection.3.2.6.1}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian Parameter Distribution}{20}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Symmetric Sampling and Gain Normalization}{20}{section*.4}}
\citation{sehnke2012parameter}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.6.2}Infinite Horizon PGPE}{21}{subsubsection.3.2.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Asset Allocation}{23}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Reward Function}{23}{section.4.1}}
\newlabel{eq:portfolio_return}{{4.1}{24}{Reward Function}{equation.4.1.1}{}}
\newlabel{eq:portfolio_return_benchmark}{{4.3}{25}{Reward Function}{equation.4.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}States}{25}{section.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Actions}{26}{section.4.3}}
\citation{*}
\bibstyle{siam}
\bibdata{Bibliography/bibliography}
\bibcite{agarwal2010optimal}{1}
\bibcite{almgren2001optimal}{2}
\bibcite{basu2008learning}{3}
\bibcite{bauerle2011markov}{4}
\bibcite{bekiros2010heterogeneouseltit}{5}
\bibcite{bertoluzzo2012testing}{6}
\bibcite{corazza2014q}{7}
\bibcite{bertoluzzo2014reinforcement}{8}
\bibcite{bertsekas1995dynamic}{9}
\bibcite{bertsekas1978stochastic}{10}
\bibcite{bertsekas1996neuro}{11}
\bibcite{bhatnagar2009natural}{12}
\bibcite{bishop2006pattern}{13}
\bibcite{borkar2001sensitivity}{14}
\bibcite{borkar2002q}{15}
\bibcite{borkar2002risk}{16}
\bibcite{busoniu2010reinforcement}{17}
\bibcite{casqueiro2006neuro}{18}
\bibcite{chapados2001cost}{19}
\bibcite{choey1997nonlineareltit}{20}
\bibcite{chow2015risk}{21}
\bibcite{corazzaq}{22}
\bibcite{cumming2015investigation}{23}
\bibcite{dempster2006automated}{24}
\bibcite{dempster2002intraday}{25}
\bibcite{deng2016deep}{26}
\bibcite{deng2015sparse}{27}
\bibcite{du1algorithm}{28}
\bibcite{eldercreating}{29}
\bibcite{feldkamp1998enhanced}{30}
\bibcite{ganchev2010censored}{31}
\bibcite{gold2003FX}{32}
\bibcite{Goodfellow-et-al-2016-Book}{33}
\bibcite{hastie2009unsupervised}{34}
\bibcite{hendricks2014reinforcement}{35}
\bibcite{jaeger2002tutorial}{36}
\bibcite{joshi2008c++}{37}
\bibcite{kamijo1990stock}{38}
\bibcite{kearns2013machine}{39}
\bibcite{konda1999actor}{40}
\bibcite{NIPS2013_4917}{41}
\bibcite{laruelle2011optimal}{42}
\bibcite{laruelle2013optimal}{43}
\bibcite{li2007short}{44}
\bibcite{mahadevan1996average}{45}
\bibcite{miyamae2010natural}{46}
\bibcite{moody2001learning}{47}
\bibcite{moody2013reinforcement}{48}
\bibcite{moody1997optimization}{49}
\bibcite{moody1998performance}{50}
\bibcite{nevmyvaka2006reinforcement}{51}
\bibcite{nocedal2006numerical}{52}
\bibcite{o2006adaptive}{53}
\bibcite{peters2010relative}{54}
\bibcite{peters2006policy}{55}
\bibcite{peters2008reinforcement}{56}
\bibcite{prashanth2014actor}{57}
\bibcite{puterman1994markov}{58}
\bibcite{Saad1998comparative}{59}
\bibcite{sanderson2010armadillo}{60}
\bibcite{sato2000variance}{61}
\bibcite{Sato:2001:ARL:645530.757778}{62}
\bibcite{sehnke2012parameter}{63}
\bibcite{sehnke2008policy}{64}
\bibcite{sehnke2010parameter}{65}
\bibcite{silver2014deterministic}{66}
\bibcite{sutton1998introduction}{67}
\bibcite{sutton1999policy}{68}
\bibcite{szepesvari2010algorithms}{69}
\bibcite{tamar2013temporal}{70}
\bibcite{tamar2015policy}{71}
\bibcite{tamar2012policy}{72}
\bibcite{tamar2013variance}{73}
\bibcite{tan2011stock}{74}
\bibcite{tsay2005analysis}{75}
\bibcite{werbos1990backpropagation}{76}
\bibcite{wiering2012reinforcement}{77}
\bibcite{williams1989learning}{78}
\bibcite{Yang2012behavior}{79}
\bibcite{zhao2011analysis}{80}
\bibcite{zhao2015regularized}{81}
