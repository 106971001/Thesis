\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The March of the Machines}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Artificial Intelligence and Finance}{1}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure}{1}{section.1.3}}
\citation{bertsekas1978stochastic}
\citation{puterman1994markov}
\citation{bertsekas1995dynamic}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Discrete-Time Stochastic Optimal Control}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:discrete_time_stochastic_optimal_control}{{2}{3}{Discrete-Time Stochastic Optimal Control}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sequential Decision Problems}{3}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Markov Decision Processes}{4}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Agent-environment interaction in sequential decision problems.\relax }}{5}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sequential_decision_problem}{{2.1}{5}{Agent-environment interaction in sequential decision problems.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Policies}{5}{section.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Risk-Neutral Framework}{6}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Discounted Reward Formulation}{6}{subsection.2.4.1}}
\newlabel{eq:bellman_expectation_eq_V}{{2.6}{7}{Bellman Expectation Equations}{equation.2.4.6}{}}
\newlabel{eq:bellman_expectation_eq_Q}{{2.7}{7}{Bellman Expectation Equations}{equation.2.4.7}{}}
\citation{arapostathis1993discrete}
\citation{mahadevan1996average}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Average Reward Formulation}{9}{subsection.2.4.2}}
\newlabel{eq:VQ_equality}{{2.18}{10}{Average Reward Formulation}{equation.2.4.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Risk-Sensitive Framework}{10}{section.2.5}}
\newlabel{sec:risk_sensitive_formulation}{{2.5}{10}{Risk-Sensitive Framework}{section.2.5}{}}
\citation{sobel1982variance}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Discounted Reward Formulation}{11}{subsection.2.5.1}}
\citation{tamar2012policy}
\citation{prashanth2014actor}
\citation{prashanth2014actor}
\newlabel{eq:synthetic_reward_function}{{2.27}{12}{Bellman Expectation Equation}{equation.2.5.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Average Reward Formulation}{13}{subsection.2.5.2}}
\newlabel{eq:UW_equality}{{2.36}{14}{Average Reward Formulation}{equation.2.5.36}{}}
\newlabel{eq:risk_sensitive_problem}{{2.39}{14}{Average Reward Formulation}{equation.2.5.39}{}}
\citation{szepesvari2010algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Dynamic Programming Algorithms}{15}{section.2.6}}
\newlabel{sec:policy_evaluation}{{2.6}{15}{Dynamic Programming Algorithms}{section.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Value Iteration}{15}{subsection.2.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Policy Iteration}{15}{subsection.2.6.2}}
\citation{sutton1998introduction}
\citation{sutton1998introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Policy iteration algorithm}}{16}{figure.caption.7}}
\newlabel{fig:policy_iteration}{{2.2}{16}{Policy iteration algorithm}{figure.caption.7}{}}
\citation{sutton1998introduction}
\citation{szepesvari2010algorithms}
\citation{wiering2012reinforcement}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Reinforcement Learning}{17}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Reinforcement Learning Problem}{18}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Model-Free RL Methods}{18}{section.3.2}}
\citation{wiering2012reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Solution process for the control problem.\relax }}{19}{figure.caption.8}}
\newlabel{fig:control_dependences}{{3.1}{19}{Solution process for the control problem.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Model Approximation}{20}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Value Approximation}{20}{subsection.3.2.2}}
\citation{sutton1999policy}
\citation{konda1999actor}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Policy Approximation}{21}{subsection.3.2.3}}
\citation{peters2008reinforcement}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Policy Gradient}{23}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{kushner2003stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Basics of Policy Gradient Methods}{24}{section.4.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces General setup for a policy gradient algorithm.\relax }}{24}{algorithm.4.1}}
\newlabel{algo:policy_gradient}{{4.1}{24}{General setup for a policy gradient algorithm.\relax }{algorithm.4.1}{}}
\citation{tamar2012policy}
\citation{prashanth2014actor}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Risk-Neutral Framework}{25}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Risk-Sensitive Framework}{25}{subsection.4.1.2}}
\citation{markowitz1952portfolio}
\citation{sharpe1994sharpe}
\citation{pages2016introduction}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Finite Differences}{27}{section.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Likelihood Ratio Methods}{27}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Monte Carlo Policy Gradient}{28}{subsection.4.3.1}}
\newlabel{eq:gradient_MC}{{4.16}{28}{Monte Carlo Policy Gradient}{equation.4.3.16}{}}
\newlabel{eq:likelihood_bias}{{4.18}{29}{Monte Carlo Policy Gradient}{equation.4.3.18}{}}
\newlabel{eq:gradient_MC_baseline}{{4.19}{29}{Monte Carlo Policy Gradient}{equation.4.3.19}{}}
\newlabel{eq:reinforce_gradient}{{4.20}{29}{Monte Carlo Policy Gradient}{equation.4.3.20}{}}
\citation{baxter2001infinite}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces Episodic REINFORCE policy gradient estimate\relax }}{30}{algorithm.4.2}}
\newlabel{algo:reinforce}{{4.2}{30}{Episodic REINFORCE policy gradient estimate\relax }{algorithm.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1.1}Optimal Baseline}{30}{subsubsection.4.3.1.1}}
\newlabel{eq:optimal_baseline}{{4.22}{30}{Optimal Baseline}{equation.4.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}GPOMDP}{30}{subsection.4.3.2}}
\newlabel{eq:GPOMDP}{{4.23}{30}{GPOMDP}{equation.4.3.23}{}}
\citation{tamar2012policy}
\citation{tamar2013temporal}
\citation{tamar2013variance}
\citation{tamar2015policy}
\citation{chow2015risk}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Risk-Sensitive Monte Carlo Policy Gradient}{31}{subsection.4.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Stochastic Policies}{31}{subsection.4.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4.1}Boltzmann Exploration Policy}{31}{subsubsection.4.3.4.1}}
\newlabel{sec:softmax}{{4.3.4.1}{31}{Boltzmann Exploration Policy}{subsubsection.4.3.4.1}{}}
\citation{sehnke2008policy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4.2}Gaussian Exploration Policy}{32}{subsubsection.4.3.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Policy Gradient with Parameter Exploration}{32}{subsection.4.3.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5.1}Episodic PGPE}{32}{subsubsection.4.3.5.1}}
\citation{zhao2011analysis}
\newlabel{eq:pgpe_gradient}{{4.31}{33}{Episodic PGPE}{equation.4.3.31}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.3}{\ignorespaces Episodic PGPE algorithm\relax }}{34}{algorithm.4.3}}
\newlabel{algo:episodic_pgpe}{{4.3}{34}{Episodic PGPE algorithm\relax }{algorithm.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Independent Gaussian Parameter Distribution}{34}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian Parameter Distribution}{34}{section*.10}}
\citation{sehnke2012parameter}
\@writefile{toc}{\contentsline {paragraph}{Symmetric Sampling and Gain Normalization}{35}{section*.11}}
\citation{sutton1999policy}
\citation{konda1999actor}
\citation{sutton1999policy}
\citation{kakade2001natural}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5.2}Infinite Horizon PGPE}{36}{subsubsection.4.3.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Risk-Neutral Policy Gradient Theorem}{36}{section.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Theorem Statement and Proof}{37}{subsection.4.4.1}}
\newlabel{eq:pg_theorem_baseline}{{4.43}{39}{Theorem Statement and Proof}{equation.4.4.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}GPOMDP}{39}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Actor-Critic Policy Gradient}{39}{subsection.4.4.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.4}{\ignorespaces Generic structure for an online actor-critic algorithm.\relax }}{40}{algorithm.4.4}}
\newlabel{algo:actor_critic}{{4.4}{40}{Generic structure for an online actor-critic algorithm.\relax }{algorithm.4.4}{}}
\newlabel{eq:actor_critic_pg}{{4.44}{40}{Actor-Critic Policy Gradient}{equation.4.4.44}{}}
\citation{sutton1999policy}
\newlabel{eq:td_error}{{4.48}{41}{Actor-Critic Policy Gradient}{equation.4.4.48}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.5}{\ignorespaces TD($\lambda $) policy gradient algorithm.\relax }}{42}{algorithm.4.5}}
\newlabel{algo:actor_critic_td}{{4.5}{42}{TD($\lambda $) policy gradient algorithm.\relax }{algorithm.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Compatible Function Approximation}{42}{subsection.4.4.4}}
\citation{kakade2001natural}
\citation{peters2008reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}Natural Policy Gradient}{43}{subsection.4.4.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.5.1}Formalism of Natural Policy Gradients}{43}{subsubsection.4.4.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces ``Vanilla'' policy gradient vs. natural policy gradient}}{44}{figure.caption.12}}
\citation{miyamae2010natural}
\citation{prashanth2014actor}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Risk-Sensitive Policy Gradient Theorem}{45}{section.4.5}}
\citation{sutton1999policy}
\newlabel{eq:policy_gradient_theorem_Q}{{4.62}{46}{Policy Gradient}{equation.4.5.62}{}}
\newlabel{eq:policy_gradient_theorem_W}{{4.63}{46}{Policy Gradient}{equation.4.5.63}{}}
\citation{prashanth2014actor}
\citation{sutton1999policy}
\newlabel{eq:pg_advantage_Q}{{4.66}{47}{Risk-Sensitive Policy Gradient Theorem}{equation.4.5.66}{}}
\newlabel{eq:pg_advantage_W}{{4.67}{47}{Risk-Sensitive Policy Gradient Theorem}{equation.4.5.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Risk-Sensitive Actor-Critic Algorithm}{47}{subsection.4.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Parameter-Based Policy Gradient Theorem}{48}{section.4.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Risk-Neutral Setting}{48}{subsection.4.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Risk-Sensitive Setting}{48}{subsection.4.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Natural Policy Gradient}{48}{subsection.4.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.3.1}NPGPE}{48}{subsubsection.4.6.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.3.2}Risk-Sensitive NPGPE}{48}{subsubsection.4.6.3.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Financial Applications of Reinforcement Learning}{49}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Bibliographical Survey}{49}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Asset Allocation}{49}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Reward Function}{49}{subsection.5.2.1}}
\newlabel{eq:portfolio_return}{{5.1}{50}{Reward Function}{equation.5.2.1}{}}
\newlabel{eq:portfolio_return_benchmark}{{5.3}{51}{Reward Function}{equation.5.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}States}{52}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Actions}{52}{subsection.5.2.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Numerical Results for the Asset Allocation Problem}{55}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Single Risky Asset Case}{55}{section.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Simulated Data}{55}{subsection.6.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.1}ARAC}{55}{subsubsection.6.1.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.2}PGPE}{56}{subsubsection.6.1.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.3}NPGPE}{56}{subsubsection.6.1.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.4}Experimental Setup}{56}{subsubsection.6.1.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.5}Results}{56}{subsubsection.6.1.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Historic Data}{56}{subsection.6.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Multiple Risky Asset Case}{56}{section.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Simulated Data}{56}{subsection.6.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Historic Data}{56}{subsection.6.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Learning process for one synthetic risky asset}}{57}{figure.caption.13}}
\newlabel{fig:single_synthetic_convergence}{{6.1}{57}{Learning process for one synthetic risky asset}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Backtest performance with one synthetic risky asset}}{57}{figure.caption.14}}
\newlabel{fig:single_synthetic_performance}{{6.2}{57}{Backtest performance with one synthetic risky asset}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Backtest performance of NPGPE with one synthetic risky asset}}{58}{figure.caption.15}}
\newlabel{fig:single_synthetic_NPGPE}{{6.3}{58}{Backtest performance of NPGPE with one synthetic risky asset}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{59}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary}{59}{section.7.1}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Further Developments}{59}{section.7.2}}
\citation{*}
\bibstyle{siam}
\bibdata{Bibliography/bibliography}
\bibcite{agarwal2010optimal}{1}
\bibcite{almgren2001optimal}{2}
\bibcite{arapostathis1993discrete}{3}
\bibcite{basu2008learning}{4}
\bibcite{bauerle2011markov}{5}
\bibcite{baxter2001infinite}{6}
\bibcite{bekiros2010heterogeneouseltit}{7}
\bibcite{bertoluzzo2012testing}{8}
\bibcite{corazza2014q}{9}
\bibcite{bertoluzzo2014reinforcement}{10}
\bibcite{bertsekas1995dynamic}{11}
\bibcite{bertsekas1978stochastic}{12}
\bibcite{bertsekas1996neuro}{13}
\bibcite{bhatnagar2009natural}{14}
\bibcite{bishop2006pattern}{15}
\bibcite{borkar2001sensitivity}{16}
\bibcite{borkar2002q}{17}
\bibcite{borkar2002risk}{18}
\bibcite{busoniu2010reinforcement}{19}
\bibcite{casqueiro2006neuro}{20}
\bibcite{chapados2001cost}{21}
\bibcite{choey1997nonlineareltit}{22}
\bibcite{chow2015risk}{23}
\bibcite{corazzaq}{24}
\bibcite{cumming2015investigation}{25}
\bibcite{dempster2006automated}{26}
\bibcite{dempster2002intraday}{27}
\bibcite{deng2016deep}{28}
\bibcite{deng2015sparse}{29}
\bibcite{du1algorithm}{30}
\bibcite{eldercreating}{31}
\bibcite{feldkamp1998enhanced}{32}
\bibcite{ganchev2010censored}{33}
\bibcite{gold2003FX}{34}
\bibcite{Goodfellow-et-al-2016-Book}{35}
\bibcite{hastie2009unsupervised}{36}
\bibcite{hendricks2014reinforcement}{37}
\bibcite{jaeger2002tutorial}{38}
\bibcite{joshi2008c++}{39}
\bibcite{kakade2001natural}{40}
\bibcite{kamijo1990stock}{41}
\bibcite{kearns2013machine}{42}
\bibcite{konda1999actor}{43}
\bibcite{kushner2003stochastic}{44}
\bibcite{NIPS2013_4917}{45}
\bibcite{laruelle2011optimal}{46}
\bibcite{laruelle2013optimal}{47}
\bibcite{li2007short}{48}
\bibcite{mahadevan1996average}{49}
\bibcite{markowitz1952portfolio}{50}
\bibcite{miyamae2010natural}{51}
\bibcite{moody2001learning}{52}
\bibcite{moody2013reinforcement}{53}
\bibcite{moody1997optimization}{54}
\bibcite{moody1998performance}{55}
\bibcite{nevmyvaka2006reinforcement}{56}
\bibcite{nocedal2006numerical}{57}
\bibcite{o2006adaptive}{58}
\bibcite{pages2016introduction}{59}
\bibcite{peters2010relative}{60}
\bibcite{peters2006policy}{61}
\bibcite{peters2008reinforcement}{62}
\bibcite{prashanth2014actor}{63}
\bibcite{puterman1994markov}{64}
\bibcite{Saad1998comparative}{65}
\bibcite{sanderson2010armadillo}{66}
\bibcite{sato2000variance}{67}
\bibcite{Sato:2001:ARL:645530.757778}{68}
\bibcite{sehnke2012parameter}{69}
\bibcite{sehnke2008policy}{70}
\bibcite{sehnke2010parameter}{71}
\bibcite{sharpe1994sharpe}{72}
\bibcite{silver2014deterministic}{73}
\bibcite{sobel1982variance}{74}
\bibcite{sutton1998introduction}{75}
\bibcite{sutton1999policy}{76}
\bibcite{szepesvari2010algorithms}{77}
\bibcite{tamar2013temporal}{78}
\bibcite{tamar2015policy}{79}
\bibcite{tamar2012policy}{80}
\bibcite{tamar2013variance}{81}
\bibcite{tan2011stock}{82}
\bibcite{tsay2005analysis}{83}
\bibcite{werbos1990backpropagation}{84}
\bibcite{wiering2012reinforcement}{85}
\bibcite{williams1989learning}{86}
\bibcite{Yang2012behavior}{87}
\bibcite{zhao2011analysis}{88}
\bibcite{zhao2015regularized}{89}
