\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Reinforcement Learning}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Reinforcement Learning Problem}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Markov Decision Processes}{1}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Bellman Equations}{4}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Risk-Sensitive MDP}{4}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Policy Gradient}{4}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Basics of Policy Gradient}{4}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Finite Differences}{6}{subsection.1.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Monte Carlo Policy Gradient}{6}{subsection.1.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Policy Gradient Theorem}{7}{subsection.1.3.4}}
\citation{sehnke2008policy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4.1}Actor-Critic Policy Gradient}{8}{subsubsection.1.3.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Natural Policy Gradient}{8}{subsection.1.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Policy Gradient with Parameter Exploration}{8}{subsection.1.3.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.6.1}Episodic PGPE}{9}{subsubsection.1.3.6.1}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian Parameter Distribution}{10}{section*.1}}
\citation{sehnke2012parameter}
\@writefile{toc}{\contentsline {paragraph}{Symmetric Sampling and Gain Normalization}{11}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.6.2}Infinite Horizon PGPE}{11}{subsubsection.1.3.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Asset Allocation}{13}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Reward Function}{13}{section.2.1}}
\newlabel{eq:portfolio_return}{{2.1}{14}{Reward Function}{equation.2.1.1}{}}
\newlabel{eq:portfolio_return_benchmark}{{2.3}{15}{Reward Function}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}States}{15}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Actions}{16}{section.2.3}}
\citation{*}
\bibstyle{acm}
\bibdata{Bibliography/bibliography}
\bibcite{agarwal2010optimal}{1}
\bibcite{almgren2001optimal}{2}
\bibcite{bauerle2011markov}{3}
\bibcite{bekiros2010heterogeneouseltit}{4}
\bibcite{bertoluzzo2012testing}{5}
\bibcite{corazza2014q}{6}
\bibcite{bertoluzzo2014reinforcement}{7}
\bibcite{bertsekas1995dynamic}{8}
\bibcite{bertsekas1996neuro}{9}
\bibcite{bishop2006pattern}{10}
\bibcite{busoniu2010reinforcement}{11}
\bibcite{casqueiro2006neuro}{12}
\bibcite{chapados2001cost}{13}
\bibcite{choey1997nonlineareltit}{14}
\bibcite{chow2015risk}{15}
\bibcite{corazzaq}{16}
\bibcite{cumming2015investigation}{17}
\bibcite{dempster2006automated}{18}
\bibcite{dempster2002intraday}{19}
\bibcite{deng2016deep}{20}
\bibcite{deng2015sparse}{21}
\bibcite{du1algorithm}{22}
\bibcite{eldercreating}{23}
\bibcite{feldkamp1998enhanced}{24}
\bibcite{ganchev2010censored}{25}
\bibcite{gold2003FX}{26}
\bibcite{Goodfellow-et-al-2016-Book}{27}
\bibcite{hastie2009unsupervised}{28}
\bibcite{hendricks2014reinforcement}{29}
\bibcite{jaeger2002tutorial}{30}
\bibcite{kamijo1990stock}{31}
\bibcite{kearns2013machine}{32}
\bibcite{konda1999actor}{33}
\bibcite{laruelle2011optimal}{34}
\bibcite{laruelle2013optimal}{35}
\bibcite{li2007short}{36}
\bibcite{moody2001learning}{37}
\bibcite{moody2013reinforcement}{38}
\bibcite{moody1997optimization}{39}
\bibcite{moody1998performance}{40}
\bibcite{nevmyvaka2006reinforcement}{41}
\bibcite{nocedal2006numerical}{42}
\bibcite{o2006adaptive}{43}
\bibcite{peters2006policy}{44}
\bibcite{peters2008reinforcement}{45}
\bibcite{Saad1998comparative}{46}
\bibcite{sehnke2012parameter}{47}
\bibcite{sehnke2008policy}{48}
\bibcite{sehnke2010parameter}{49}
\bibcite{silver2014deterministic}{50}
\bibcite{sutton1998introduction}{51}
\bibcite{sutton1999policy}{52}
\bibcite{szepesvari2010algorithms}{53}
\bibcite{tamar2013temporal}{54}
\bibcite{tamar2015policy}{55}
\bibcite{tamar2012policy}{56}
\bibcite{tan2011stock}{57}
\bibcite{tsay2005analysis}{58}
\bibcite{werbos1990backpropagation}{59}
\bibcite{wiering2012reinforcement}{60}
\bibcite{williams1989learning}{61}
\bibcite{Yang2012behavior}{62}
