\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [0][-]{chapter.2}{Discrete-Time Stochastic Optimal Control}{}% 2
\BOOKMARK [1][-]{section.2.1}{Sequential Decision Problems}{chapter.2}% 3
\BOOKMARK [1][-]{section.2.2}{Markov Decision Processes}{chapter.2}% 4
\BOOKMARK [1][-]{section.2.3}{Policies}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.4}{Discounted Reward Formulation}{chapter.2}% 6
\BOOKMARK [1][-]{section.2.5}{Average Reward Formulation}{chapter.2}% 7
\BOOKMARK [1][-]{section.2.6}{Risk-Sensitive Formulation}{chapter.2}% 8
\BOOKMARK [1][-]{section.2.7}{Policy Evaluation and Policy Iteration}{chapter.2}% 9
\BOOKMARK [0][-]{chapter.3}{Reinforcement Learning}{}% 10
\BOOKMARK [1][-]{section.3.1}{The Reinforcement Learning Problem}{chapter.3}% 11
\BOOKMARK [1][-]{section.3.2}{Model-Free RL Methods}{chapter.3}% 12
\BOOKMARK [2][-]{subsection.3.2.1}{Model Approximation}{section.3.2}% 13
\BOOKMARK [2][-]{subsection.3.2.2}{Value Approximation}{section.3.2}% 14
\BOOKMARK [2][-]{subsection.3.2.3}{Policy Approximation}{section.3.2}% 15
\BOOKMARK [0][-]{chapter.4}{Policy Gradient}{}% 16
\BOOKMARK [1][-]{section.4.1}{Basics of Policy Gradient Methods}{chapter.4}% 17
\BOOKMARK [1][-]{section.4.2}{Finite Differences}{chapter.4}% 18
\BOOKMARK [1][-]{section.4.3}{Monte Carlo Policy Gradient}{chapter.4}% 19
\BOOKMARK [2][-]{subsection.4.3.1}{Optimal Baseline}{section.4.3}% 20
\BOOKMARK [2][-]{subsection.4.3.2}{Boltzmann Exploration Policy}{section.4.3}% 21
\BOOKMARK [2][-]{subsection.4.3.3}{Gaussian Exploration Policy}{section.4.3}% 22
\BOOKMARK [1][-]{section.4.4}{Policy Gradient Theorem}{chapter.4}% 23
\BOOKMARK [2][-]{subsection.4.4.1}{Actor-Critic Policy Gradient}{section.4.4}% 24
\BOOKMARK [1][-]{section.4.5}{Natural Policy Gradient}{chapter.4}% 25
\BOOKMARK [1][-]{section.4.6}{Policy Gradient with Parameter Exploration}{chapter.4}% 26
\BOOKMARK [2][-]{subsection.4.6.1}{Episodic PGPE}{section.4.6}% 27
\BOOKMARK [2][-]{subsection.4.6.2}{Infinite Horizon PGPE}{section.4.6}% 28
\BOOKMARK [1][-]{section.4.7}{Risk-Sensitive Policy Gradient}{chapter.4}% 29
\BOOKMARK [2][-]{subsection.4.7.1}{Risk-Sensitive Actor-Critic Algorithm}{section.4.7}% 30
\BOOKMARK [0][-]{chapter.5}{Asset Allocation}{}% 31
\BOOKMARK [1][-]{section.5.1}{Reward Function}{chapter.5}% 32
\BOOKMARK [1][-]{section.5.2}{States}{chapter.5}% 33
\BOOKMARK [1][-]{section.5.3}{Actions}{chapter.5}% 34
