\chapter{Reinforcement Learning}

The standard way to solve Markov decision processes is through dynamic programming, which simply consists in solving the Bellman fixed-point equations discussed in the previous chapter. Following this approach, the problem of finding the optimal policy is transformed into the problem of finding the optimal value function. However, apart from the simplest cases where the MDP has a limited number of states and actions, dynamic programming becomes computationally infeasible. Moreover, this approach requires complete knowledge of the Markov transition kernel and of the reward function, which in many real-world applications might be unknown or too complex to use. Reinforcement Learning (RL) is a subfield of Machine Learning which aims to turn the infeasible dynamic programming methods into practical algorithms that can be applied to large-scale problems. RL algorithms are based on two key ideas: the first is to use samples to compactly represent the unknown dynamics of the controlled system. The second key idea is to use powerful function approximation methods to compactly estimate value functions and policies in high-dimensional state and action spaces. In the following sections, we discuss the reinforcement learning problem in more depth in order to highlight the main differences with the standard discrete-time stochastic optimal control theory and with the other subfield of machine learning, such as supervised learning. The aim of this chapter is to provide a quick overview of the RL scenario and discuss the main features of this class of algorithms, which are propaedeutic to the following chapters. For a more thorough presentation, the reader may consult \cite{sutton1998introduction}, \cite{szepesvari2010algorithms} or \cite{wiering2012reinforcement}.

\section{The Reinforcement Learning Problem}
Reinforcement Learning (RL) is a general class of algorithms in the field of machine learning that allows an agent to learn how to behave in a stochastic and possibly unknown environment where the only feedback consists of a scalar reward signal. In order to maximize the reward signal in the long-run, the agent must learn which actions yield the most reward by trial-and-error. Therefore, RL algorithms can be seen as computational methods to solve Markov decision processes by directly interacting with the environment, for which a model may or may not be available. Trial-and-error search and a delayed reward signal can be seen as the most characteristic features of reinforcement learning.\\
Compared to supervised learning, on the main fields of machine learning, the feedback the learner receives is much less. In supervised learning, the agent is provided with examples of the correct or expected behavior by a knowledgeable external supervisor and his goal is to learn how to replicate these examples as well as possible. The main difficulty is whether this mapping generalizes to new examples. In reinforcement learning, the agent only receives a numerical reward that only gives a partial feedback of the goodness of his actions. This feedback system is evaluative rather than instructive. Therefore, it is much more difficult for the agent to learn from his own experience how to behave in uncharted territory, without any external guidance.\\ 
This particular framework generates some challenges that are not present in other kinds of learning. The first one is the trade-off between exploration and exploitation. In order to maximize his reward, an agent would greedily select actions that he has already tried in the past and found to be effective in producing rewards. However, to find these actions, he has to test actions that haven't been selected before in order to evaluate their potential. Clearly, this might result in worse performance in the short-term because the actions might be less good the current policy. However, without trying them, the agent might never find possible improvements and get stuck in a suboptimal policy. Thus, an agent must exploit what he already knows to obtain reward, but he also needs to explore to select better actions in the future. A second challenge is the credit assignment problem. Since rewards might be delayed in time, it will difficult for the agent to understand which actions are mostly responsible for the outcome. Finally, reinforcement learning extends ideas from optimal control theory and stochastic approximation to address the broader goal of artificial intelligence. 

\section{Model-Free RL Methods}


\subsection{Model Approximation}


\subsection{Value Approximation}


\subsection{Policy Approximation}






