\chapter{Reinforcement Learning}

\section{The Reinforcement Learning Problem}

%------------------------------------------------------------------------------
\section{Policy Gradient}
In policy gradient methods, we directly store and iteratively improve an 
approximation of the optimal policy. More formally, we consider a parametrized
policy $\pi: \S \times \calA \times \Theta \to \R$ such that, for every $s \in
\S$, $B \in \calA$ and a given policy parameter vector $\theta \in \Theta
\subseteq \R^{D_\theta}$, $\pi(s, B; \theta) = \pi_\theta(s, B)$ gives the
probability of selecting an action in $B$ when the system is in state $s$. This
policy is also called an $\emph{actor}$ and methods that directly approximate
the policy without exploiting an approximation of the optimal value function are 
called $\emph{actor-only}$. We will also see how to combine an approximation of 
the optimal policy with an approximation of the value function, in what are
commonly called $\emph{actor-critc}$ methods. As we have seen in the previous
sections, an optimal policy can be derived by simply acting greedily with
respect to the optimal action-value function. However, in large or continuous 
action spaces, this leads to a complex optimization problem that is 
computationally expensive to solve. Therefore, it can be beneficial to store an
explicit estimation of the optimal policy from which we can select actions.
Policy gradient methods have other advantages compared to standard value-based
approaches
\begin{enumerate}[label={\roman*)}]
	\item these methods have better convergence properties and are guaranteed
		to converge at least to a local optimum, which may be good enough in
		practice.  
	\item they are effective in high-dimensional or continuous action spaces.
	\item they can learn stochastic policies and not only deterministic ones.
	\item In many applications, the optimal policy has a more compact
		representation that the value function, so that it might be easier to
		approximate. 
\end{enumerate}
On the other hand, policy gradient methods have a large variance which may
hinder the converge speed. We will see in the following sections how different 
methods address this problem.  

\subsection{Basics of Policy Gradient}
The general goal of policy optimization in reinforcement learning is to
optimize the policy parameters $\theta \in \Theta$ so as to maximize a certain
objective function $J: \Theta \to \R$
\begin{equation}
	\max_{\theta \in \Theta} J(\theta)
\end{equation}
There are various choices for the objective function.
\begin{definition}[Start Value]
	In an episodic environment, the start value is the expected return that can
	be obtained starting from the start state $s^* \in \S$ and following policy
	$\pi_\theta$
	\begin{equation}
		J_{\text{start}}(\theta) = V_{\pi_\theta}(s^*) = \E[\pi_\theta]{G_t |
		   S_t = s^*}
	\end{equation}
\end{definition}
\begin{definition}[Average Value]
	In a continuing environment, the average value is the expected value that
	can be obtained following policy $\pi_\theta$ 
	\begin{equation}
		J_{\text{avV}}(\theta) = \E[S \sim \mu]{V_{\pi_\theta}(S)} = \int_\S
		V_{\pi_\theta}(s) \mu(s) ds
	\end{equation}
	where $\mu$ is a probability distribution over $(\S, \calS)$.
\end{definition}
\begin{definition}[Average Reward per Time Step]
	The average reward per time step is the expected reward that can be
	obtained over a single time step by following policy $\pi_\theta$ 
	\begin{equation}
		J_{\text{avR}}(\theta) = \E[\substack{S \sim \mu\\A \sim \pi_\theta}]{\calR(S,A)} 
		= \int_\S \mu(s) \int_\A \pi_\theta(s,a) \calR(s,a) da ds
	\end{equation}
	where $\mu$ is a probability distribution over $(\S, \calS)$.
\end{definition}
Fortunately, the same methods apply to the three formulations. In the
following, we will focus on gradient-based and model-free methods that exploit
the sequential structure of the the reinforcement learning problem. The idea of
policy gradient algorithms is to update the policy parameters using the
gradient ascent direction of the objective function
\begin{equation}
	\theta_{k+1} = \theta_k + \alpha_k \nabla_\theta J|_{\theta = \theta_k}
\end{equation}
where $\{\alpha_k\}_{k\geq 0}$ is a sequence of learning rates. Typically, the
gradient of the objective function is not know and needs to be estimated. It is
a well-know result from stochastic optimization that, if the gradient estimate
is unbiased and the learning rates satisfy the \emph{Robbins-Monro conditions}
\begin{equation}
	\sum_{k=0}^\infty \alpha_k = \infty \;\;\;\;\;\; \sum^{\infty}_{k=0}
	\alpha_k^2 < \infty 
\end{equation}
the learning process is guaranteed to converge at least to a local optimum of
the objective function. In the following sections, we describe various methods
of approximating the gradient. 

\subsection{Finite Differences}

\subsection{Monte Carlo Policy Gradient}
Let $h = {\{(s_t, a_t)\}}_{t\geq 0} \in \H$ be a given trajectory and let us 
denote by $p_\theta(h) = \P[\pi_\theta]{H = h}$ the probability of obtaining 
this trajectory by following policy $\pi_\theta$. Let $G(h)$ denote the expect
return obtained on trajectory $h$
\begin{equation}
	G(h) = \E{G_0 | H_t = h} = \sum_{t=1}^\infty \gamma^k \calR(s_{t-1},
	a_{t-1}) 
\end{equation}
For simplicity, let us consider the average value objective function which can
be rewritten as an expectation over all possible trajectories
\begin{equation}
	J_{\text{avV}}(\theta) = \int_\H p_\theta(h) G(h) dh
\end{equation}
We can compute its gradient using the likelihood ratio trick 
\begin{equation}
	\begin{split}
		\nabla_\theta J(\theta) &= \int_\H \nabla_\theta p_\theta(h) G(h) dh\\
								&= \int_\H p_\theta(h)\nabla_\theta \log
									p_\theta(h)G(h)dh\\
								&= \E{\nabla_\theta \log p_\theta(H)G(H)}
	\end{split}
\end{equation}
where the expectation is taken over all possible trajectories. The crucial
point is that $\nabla_\theta \log p_\theta(H)$ can be computed without
knowledge of the transition probability kernel $\calP$. Indeed
\begin{equation*}
	p_\theta(h) = \P{S_0 = s_0} \prod_{t=0}^\infty \pi_\theta(s_t, a_t)
	\calP(s_t, a_t, s_{t+1})
\end{equation*}
\begin{equation*}
	\log p_\theta(h) = \log \P(S_0 = s_0) + \sum_{t=0}^\infty \log 
	\pi_\theta(s_t, a_t) + \sum_{t=0}^\infty \log \calP(s_t, a_t, s_{t+1})
\end{equation*}
The only term depending on the parameters $\theta$ is the policy term, so that
\begin{equation}
	\nabla_\theta \log p_\theta(H) = \sum_{t=0}^\infty \nabla_\theta \log 
	\pi_\theta(s_t, a_t)
\end{equation}
So we do not need the transition model to compute the $\nabla_\theta \log 
p_\theta(H)$. However, this trick only works if the policy is stochastic. In
most cases this is not a big problem, since stochastic policies are needed
anyway to ensure sufficient exploration. There are two important classes of
stochastic policies that work well in this framework: softmax policies and
Gaussian policies [TODO]. 
Moreover, since
\begin{equation*}
	\int_\H \nabla_\theta p_\theta(h) dh = 0	
\end{equation*}
a constant baseline $b \in \R$ can always be added in the gradient formula
\begin{equation}
	\nabla_\theta J(\theta) = \E{\nabla_\theta \log p_\theta(H)(G(H)-b)}
\end{equation}
We will see how this baseline can be chosen in order to minimize the variance
of the estimator. In an episodic environment, we can derive an estimate of the 
objective function gradient by sampling $M$ trajectories $h^{(m)} = \{(s_t^{(m)},
a_t^{(m)})\}_{t = 0}^{T^{(m)}}$ from the MDP and by approximating the expected 
value via Monte Carlo
\begin{equation}
	g_{\text{RF}} = \frac{1}{M} \sum^{M}_{m=1} \left[ \sum_{i=0}^{T^{(m)}} 
	\nabla_\theta \log \pi_\theta(s_i^{(m)}, a_i^{(m)}) \right] \left[
	\sum^{T^{(m)}}_{j=0} \gamma^j r_{j+1}^{(m)} - b \right]  
\end{equation}
This method is known in the literature as the REINFORCE algorithm and is
guaranteed to converge to the true gradient at a pace of $O(M^{-1/2})$. In
practice, we can obtain an approximation of the gradient using only one sample
which leads to a stochastic gradient ascent method
\begin{equation}
	g_{\text{SRF}} = \left[ \sum_{i=0}^{T} \nabla_\theta \log \pi_\theta(s_i, 
	a_i) \right] \left[ \sum^{T}_{j=0} \gamma^j r_{j+1} - b \right]  
\end{equation}
This method is very easy and works well on many problems. However, the gradient
estimate is characterized by a large variance which can hamper the convergence
rate of the algorithm. A first approach to adress this issue is to optimally
set the benchmark to reduce the estimate variance. [TODO]

\subsection{Policy Gradient Theorem}
In the last section, we said that the REINFORCE gradient estimate is
characterized by a large variance, which may slow the method's convergence. 
To improve the estimate, it is sufficient to notice that future actions do not
depend on past rewards, unless the policy has been changed. Therefore, 
\begin{equation}
	\E[\pi_\theta]{\nabla_\theta\log \pi_\theta(S_t, A_t) \calR(S_s, A_s)} = 0
	\;\;\;\;\; \forall t>s
\end{equation}
From this trivial remark, we can derive two estimates of the objective function
gradient
\begin{equation}
	g_{\text{PG}} = \frac{1}{M} \sum^{M}_{m=1} \sum_{i=0}^{T^{(m)}} 
	\nabla_\theta \log \pi_\theta(s_i^{(m)}, a_i^{(m)}) \left( 
	\sum^{T^{(m)}}_{j=i} \gamma^j r_{j+1}^{(m)} - b \right)
\end{equation}
\begin{equation}
	g_{\text{GMDP}} = \frac{1}{M} \sum^{M}_{m=1} \sum_{j=0}^{T^{(m)}} \left[ 
	\sum_{i=j}^{T^{(m)}} \nabla_\theta \log \pi_\theta(s_i^{(m)}, a_i^{(m)})
	\right] \left(\gamma^j r_{j+1}^{(m)} - b \right)  
\end{equation}
The two estimates are exactly equivalent. This simple trick greatly reduces the
estimate variance and this can speed up convergence. These algorithms can all 
be derived from a more general result: the policy gradient theorem. 
\begin{theorem}[Policy Gradient]
	For any differentiable policy $\pi_\theta$, for any of the policy objective
	functions $J = J_{\text{start}}$, $J_{\text{avV}}$, $J_{\text{avR}}$,
	$\frac{1}{1-\gamma} J_{\text{avV}}$, the policy gradient is 
	\begin{equation}
		\nabla_\theta J(\theta) =
		\E[\substack{S \sim \mu\\A \sim \pi_\theta}]{\nabla_\theta\log
		\pi_\theta(S,A) Q_{\pi_\theta}(S, A)}
	\end{equation}
\end{theorem}
The problem is that the action-value function is typically unknown and needs to
be approximated. For instance, REINFORCE replaces it with the realized return
achieved on a sample trajectory, that are an unbiased estimate of the
action-value function. However, the theorem can be used as the starting point
to derive many other policy gradient methods that use different approximation
of the action-value function.

\subsubsection{Actor-Critic Policy Gradient}
Actor-Critic Policy Gradient employs a \emph{critic}, that is a parametric
approximation $\widehat{Q}: \S \times \A \times \Psi \to \R$, where $\Psi \in
\R^{D_\psi}$ such that $\widehat{Q}_\psi(s, a) = \widehat{Q}(s, a; \psi)
\approx Q_{\pi_\theta}(s,a)$. Therefore these methods maintain two sets of
parameters: a \emph{critic} that updates the action-value function parameters
$\psi$ and an \emph{actor} that updates the policy parameters $\theta$ in the
direction suggested by the critic. More formally, given $\widehat{Q}_\psi$, the
current state of the system $s_t$ and the action $a_t$ selected using policy
$\pi_\theta$, the policy parameters are updated in the approximated gradient 
direction
\begin{equation}
	\theta_{k+1} = \theta_k + \alpha_k \nabla_\theta \pi_\theta(s_t, a_t) 
	\widehat{Q}_\psi(s_t, a_t) 
\end{equation}
On the other hand, the action-value function parameters $\psi$ can be updated
using any value-based approach, for instance TD(0). This leads to the QAC
algorithm, for which the pseudo code is reported in \ref{algo:QAC}.

[TODO: TD policy gradient,
compatible function approximation, advantage function and actor-critic policy
gradient]

\subsection{Natural Policy Gradient}

\subsection{Policy Gradient with Parameter Exploration}
In Monte Carlo Policy Gradient, trajectories are generated by sampling at each
time step an action according to a stochastic policy $\pi_\theta$ and the
objective function gradient is estimated by differentiating the policy with
respect to the parameters. However, sampling an action from the policy at each
time step leads to a large variance in the sampled histories and therefore in 
the gradient estimate, which can in turn slow down the convergence of the
learning process. To address this issue, in \cite{sehnke2008policy} the authors
propose the \emph{policy gradient with parameter-based exploration} (PGPE)
method, in which the search in the policy space is replaced with a direct
search in the model parameter space. We start by presenting the episodic case
and we will later generalize this approach to the infinite horizon setting.  

\subsubsection{Episodic PGPE}
Given an episodic MDP, PGPE considers a determinstic policy $F: \S \times
\Theta \to \A$ that, given a set of parameters $\theta \in \Theta \subseteq
\R^{D_\theta}$, maps a state $s \in \S$ to an action $a = F(s; \theta) =
F_\theta(s) \in \A$. The policy parameters $\theta$ are random variables 
distributed according to a probability distribution parametrized by some 
hyperparameters $\xi \in \Xi \subseteq \R^{D_\xi}$, i.e.  $\theta \sim 
p(\theta; \xi) = p_\xi(\theta)$. Combining these two hypotheses, we obtain a
stochastic policy parametrized by the hyperparameters $\xi$ 
\begin{equation}
	\pi_\xi(s, a) = \pi(s, a; \xi) = \int_\Theta p_\xi(\theta)
	\ind{F_\theta(s) = a} d\theta
\end{equation}
The advantage of this approach is that the policy is deterministic and
therefore the actions do not need to be sampled at each time step, with a
consequent reduction of the gradient estimate variance. Indeed, It is
sufficient to sample the parameters $\theta$ once at the beginning of the
episode and then generate an entire trajectory following the deterministic 
policy $F_\theta$. As an additional benefit, the parameter gradient is
estimated by direct parameter perturbations, without having to backpropagate
any derivatives, which allows to use non-differentiable controllers. The 
hyperparameters $\xi$ will be updated following the gradient of the expected 
reward, which can be rewritten as 
\begin{equation}
	\begin{split}
		J(\xi) &= \int_\Theta \int_\H p_\xi(\theta, h) G(h) dh d\theta\\
	\end{split}
\end{equation}
Hence, using the fact that $h$ is conditionally independent from $\xi$ given
$\theta$, so that $p_\xi(\theta, h) = p_\xi(\theta) p_\theta(h)$, and the 
likelihood trick 
\begin{equation}
	\begin{split}
		\nabla_\xi J(\xi) &= \int_\Theta \int_\H \nabla_\xi p_\xi(\theta) 
						     p_\theta(h) G(h) dh d\theta\\
						  &= \int_\Theta \int_\H p_\xi(\theta) p_\theta(h) 
							 \nabla_\xi \log p_\xi(\theta) G(h) dh d\theta\\
						  &= \E{\nabla_\xi \log p_\xi(\theta) G(H)}
	\end{split}
\end{equation}
Again, we can subtract a constant baseline $b \in \R$ from the total return 
\begin{equation}
	\nabla_\xi J(\xi) = \E{\nabla_\xi \log p_\xi(\theta)\left(G(H) - b\right)}
\end{equation}
the hyperparameters can then be updated by gradient ascent
\begin{equation}
	\xi_{k+1} = \xi_k + \alpha_k \nabla_\xi J(\xi)
\end{equation}
The gradient can be approximated via Monte Carlo by sampling $\theta \sim
p_\xi$ and then generating $M$ trajectories $h^{(m)} = \{(s_t^{(m)},
a_t^{(m)})\}_{t \geq 0}$ following the deterministic policy $F_\theta$ 
\begin{equation}
	g_{\text{PGPE}} = \frac{1}{M} \sum^{M}_{m=1} \nabla_\xi \log p_\xi(\theta) 
	\left[G\left(h^{(m)}\right)-b\right] 
\end{equation}
Alternatively, we can use a fully stochastic approximation by sampling a single
trajectory and approximating the gradient as 
\begin{equation}
	g_{\text{PGPE}} = \nabla_\xi \log p_\xi(\theta) \left[G(h)-b\right] 
\end{equation}
In the following paragraphs, we discuss some possible choices for the policy
parameters distribution $p_\xi$, which is the last component of the algorithm. 

\paragraph{Gaussian Parameter Distribution}
A simple approach is to assume that all the components of the parameter vector
$\theta$ are independent and normally distributed with mean $\mu_i$ and
variance $\sigma_i^2$, i.e. $\theta_i \sim \mathcal{N}(\mu_i, \sigma_i^2)$, the
gradient with respect to the hyperparameters $\xi = (\mu_1, \ldots, 
\mu_{D_\theta}, \sigma_1, \ldots, \sigma_{D_\theta})^T$ is given by 
\begin{equation}
	\begin{split}
		\pardev{\log p_\xi(\theta)}{\mu_i} &= \frac{\theta_i -
		\mu_i}{\sigma_i^2}\\	
		\pardev{\log p_\xi(\theta)}{\sigma_i} &= \frac{(\theta_i - \mu_i)^2 -
		\sigma_i^2}{\sigma_i^3}
	\end{split}
\end{equation}
Using a constant learning rate $\alpha_i = \alpha \sigma_i^2$, the gradient
updates takes the following form
\begin{equation}
	\begin{split}
		\mu_i^{k+1} &= \mu_i^k + \alpha \left[G(h) - b\right] (\theta_i - \mu_i)\\
		\sigma_i^{k+1} &= \sigma_i^k + \alpha \left[G(h) - b\right] 
		\frac{(\theta_i -\mu_i)^2}{\sigma_i}
	\end{split}
\end{equation}
where $b$ can be computed as a moving average of the past returns. Intuitively,
if $G(h) > b$ we adjust $\xi$ so as to increase the probability of $\theta$
while if $G(h) < b$ we do the opposite. 

\paragraph{Symmetric Sampling and Gain Normalization} 
In some settings, comparing the gain with a baseline can be misleading. In
their original work, the authors propose a symmetric sampling technique similar
to antithetic variates that further improves the convergence of the method.
More in detail, a more robust gradient estimate can be obtained by measuring
the difference in reward between two symmetric samples on either side of the
current mean. That is, we sample a random perturbation $\epsilon \sim
\mathcal{N}(0, \Sigma)$, where $\Sigma = \text{diag}(\sigma_1^2, \ldots,
\sigma_{D_\theta}^2)$, and we define $\theta^+ = \mu + \epsilon$ and $\theta^- =
\mu - \epsilon$. Denoting by $G^+$ (resp. $G^-$) the gains obtained on the
trajectory associated to $\theta^+$ (resp. $\theta^-$), the objective function
gradient can be approximated with
\begin{equation}
	\begin{split}
		\nabla_{\mu_i} J(\xi) &\approx \frac{\epsilon_i (G^+ - G^-)}{2
		\sigma_i^2}\\
		\nabla_{\sigma_i} J(\xi) &\approx \frac{\epsilon_i^2 -
	\sigma_i^2}{\sigma_i^3}\left(\frac{G^+ + G^-}{2} - b\right)
	\end{split}
\end{equation}
Hence, by choosing $\alpha_i^k = 2 \alpha \sigma_i^2$, we have the following 
update rules
\begin{equation}
	\begin{split}
		\mu_i^{k+1} &= \mu_i^k + \alpha \epsilon_i (G^+ - G^-)\\
		\sigma_i^{k+1} &= \sigma_i^k + \alpha \frac{\epsilon_i^2 - \sigma_i^2}
		{\sigma_i}\left(G^+ + G^- - 2b\right)
	\end{split}
\end{equation}
In addition, the authors propose to normalize the gains in order to make the
updates independent of the scale of the rewards. For instance, we could modify
the hyperparameters updates as follows
\begin{equation}
	\begin{split}
		\mu_i^{k+1} &= \mu_i^k + \alpha \epsilon_i \frac{(G^+ - G^-)}{2m - G^+
	- G^-}\\
		\sigma_i^{k+1} &= \sigma_i^k + \alpha \frac{\epsilon_i^2 - \sigma_i^2}
	{\sigma_i}\frac{\left(G^+ + G^- - 2b\right)}{m - b}
	\end{split}
\end{equation}
where $m$ might be the maximum gain the agent can receive, if known, or
alternatively the maximum gain achieved so far. Symmetric sampling and gain
normalization can drastically improve the gradient estimate quality and 
consequently the convergence time.  

\subsubsection{Infinite Horizon PGPE}
In this section we discuss the extension of the PGPE method to the infinite
horizon case \cite{sehnke2012parameter}. While in the episodic PGPE the
parameters $\theta$ are sampled only at the beginning of each episode, in
\emph{Infinite Horizon PGPE} (IHPGPE) the parameters and learning are carried 
out simultaneously, while interacting with the environmnent. Let $0 <
\varepsilon < 1$ the probability of updating the policy parameters, the 
parameters $\theta_t$ can be sampled consecutively as follows
\begin{equation}
	p_\xi(\theta_{i,t+1}) = \varepsilon \mathcal{N}(\mu_{i,t}, \sigma_{i,t}^2) 
							+ (1-\varepsilon) \delta_{\theta_{i,t}}
\end{equation}
In practice, $\varepsilon$ should be chosen so that the expected frequency of
changing a single parameter is coherent with the typical episode length in the
episodic framework. Alternatively, one could sample all the parameters at a
certain time step simultaneously
\begin{equation}
	p_\xi(\theta_{t+1}) = \varepsilon \mathcal{N}(\mu_{t}, \Sigma_t) 
							+ (1-\varepsilon) \delta_{\theta_{t}}
\end{equation}
This is equivalent to splitting the state-action space into artificial
episodes. However, updating parameters asynchronously changes the policy only
slightly thus introducing less noise in the process. Again, parameters can be
updated at every time step by gradient ascent
\begin{equation}
	\begin{split}
		\mu_{i,t+1} &= \mu_{i,t} + \alpha \left[G_t(h) - b\right] (\theta_{i,t}
		- \mu_{i,t})\\
		\sigma_{i,t+1} &= \sigma_{i,t} + \alpha \left[G_t(h) - b\right] 
		\frac{(\theta_{i,t} -\mu_{i,t})^2}{\sigma_{i,t}}
	\end{split}
\end{equation}
Similarly to the episodic case, we can improve the gradient estimate by
symmetric sampling and gain normalization. 


