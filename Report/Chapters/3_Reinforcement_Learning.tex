\chapter{Reinforcement Learning}
\label{ch:reinforcement_learning}

The standard way to solve Markov decision processes is through dynamic programming, which simply consists in solving the Bellman fixed-point equations discussed in the previous chapter. Following this approach, the problem of finding the optimal policy is transformed into the problem of finding the optimal value function. However, apart from the simplest cases where the MDP has a limited number of states and actions, dynamic programming becomes computationally infeasible. Moreover, this approach requires complete knowledge of the Markov transition kernel and of the reward function, which in many real-world applications might be unknown or too complex to use. \glsfirst{RL} is a subfield of Machine Learning which aims to turn the infeasible dynamic programming methods into practical algorithms that can be applied to large-scale problems. \gls{RL} algorithms are based on two key ideas: the first is to use samples to compactly represent the unknown dynamics of the controlled system. The second idea is to use powerful function approximation methods to compactly estimate value functions and policies in high-dimensional state and action spaces. In the following sections, we present the \gls{RL} problem in more depth and discuss how it relates to the standard discrete-time stochastic optimal control theory and to the other subfields of machine learning, such as supervised learning. In particular, we will see how RL extends ideas from optimal control theory and stochastic approximation to address the broader goal of artificial intelligence. This quick overview of RL is propaedeutic to the following chapters, where we will present in more detail a particular class of algorithms, called policy gradient methods, which are well suited for continuous action spaces. For a more thorough presentation, the reader may consult \cite{sutton1998introduction}, \cite{szepesvari2010algorithms} or \cite{wiering2012reinforcement}.

\section{The Reinforcement Learning Problem}
Reinforcement Learning (RL) is a general class of algorithms in the field of machine learning that allows an agent to learn how to behave in a stochastic and possibly unknown environment, where the only feedback consists of a scalar reward signal. In order to maximize the long-run reward, the agent must learn which actions are the most profitable by trial-and-error. Therefore, RL algorithms can be seen as computational methods to solve Markov decision processes by directly interacting with the environment, for which a model may or may not be available. Trial-and-error search and a delayed reward signal can be seen as the most characteristic features of reinforcement learning.\\
Compared to supervised learning, one of the main branches of machine learning, the feedback the learner receives is much less. In supervised learning, the agent is provided with examples of the correct or expected behavior by a knowledgeable external supervisor and the agent's goal is to learn how to replicate these examples as well as possible and possibly generalize this knowledge to new examples. In reinforcement learning, the agent only receives a numerical reward that only gives a partial feedback of the goodness of actions taken. Therefore, this feedback system is evaluative rather than instructive and it is much more difficult for the agent to learn how to behave in uncharted territory without any external guidance.\\ 
This particular framework generates some challenges that are not present in other kinds of learning. The first one is the trade-off between exploration and exploitation. In order to maximize his reward, an agent would greedily select actions that have already been tried in the past and found to be effective in producing rewards. However, to find these actions, the agent must also test actions that have not been chosen before in order to evaluate their potential. Clearly, this might result in worse performance in the short-term because the actions might be suboptimal. However, without trying them, the agent might not be able to find possible improvements. Thus, an agent must exploit what is known to obtain rewards but also needs to explore to select better actions in the future. A second challenge is the credit assignment problem. Since rewards might be delayed in time, it will be difficult for the agent to understand which actions are mostly responsible for the outcome. 

\section{Model-Free RL Methods}
In Section \ref{sec:policy_evaluation}, we discussed the policy iteration method for computing an optimal policy for an \gls{MDP} in a finite state and action spaces. This algorithm belongs to the class of \emph{model-based} methods, since it requires perfect knowledge of the Markov transition kernel and reward function, which consists of a model of the MDP. RL is primarily concerned with how to obtain an optimal policy when such a model is not available. In this section, we discuss some classes of \emph{model-free} methods which do not rely on the MDP model. The lack of a model generates the need to sample the MDP to gather statistical knowledge about this unknown model. In the control setting, the goal is to approximate the optimal policy, which depends on the optimal value, which in turn depends on the model of the MDP, as shown in Figure \ref{fig:control_dependences}. Indeed, we have already seen that a policy which is greedy with respect to the optimal action-value funtion $Q_*$, namely for which holds
\begin{equation}
	\int_{\A} \pi_*(s,a) Q_*(s,a) da = \sup_a Q_*(s,a)
\end{equation} 
is optimal. Therefore, we can derive the following three methodologies that differ in which part of the solution process is approximated
\begin{enumerate}[label={\roman*)}]
	\item \emph{Model-approximation} algorithms approximate the MDP model and compute an estimate of the optimal policy by dynamic programming.
	\item \emph{Value-approximation} algorithms use samples to directly approximate $V_*$ or $Q_*$, from which an estimate of $\pi_*$ can be derived by acting greedily.
	\item \emph{Policy-approximation} algorithms directly try to estimate the optimal policy.
\end{enumerate}
It should be noticed that these approaches are not mutually exclusive and can be combined to derive hybrid algorithms. In the following sections we discuss these three classes of algorithm in more detail, following closely \cite{wiering2012reinforcement}.

\begin{figure}[t]
	\centering
	\begin{tikzpicture}[node distance = 6em, auto, thick]
		\node [block] (Model) {Model\\$\calP$, $\calR$};
		\node [block, below of=Model] (Value) {Value functions\\$V_*$, $Q_*$};
		\node [block, below of=Value] (Policy) {Policy\\$\pi_*$};		    
		\path [line] (Model) edge (Value);
		\path [line] (Value) edge (Policy);
	\end{tikzpicture}
	\caption{Solution process for the control problem.}
	\label{fig:control_dependences}
\end{figure}


\subsection{Model Approximation}
Model-approximation algorithms approximate the MDP model and compute an optimal policy by dynamic programming, using the techniques discussed in the previous chapter. Since $\S$, $\A$ and $\gamma$ are assumed to be known, these methods are based on learning an approximation of the Markov transition kernel $\calP$ and the reward function $\calR$. Thanks to the Markov property, these quantities only depend on the current state and action, so that their approximation corresponds to a density estimation problem and a regression problem respectively, which are fairly standard supervised learning problems. Learning the model may not be trivial, but it is in general easier than learning the value of a policy or optimizing the policy directly. The major drawback of model-based algorithms in continuous-state MDPs is that, even if the model is available, it is in general infeasible to compute the value functions by dynamic programming and to extract an optimal policy for all states by acting greedily. Alternatively, a transition model estimate may be used to generate sample trajectories from the MDP, which can then be used to estimate the value function or directly improve the policy. However, the value function and the policy estimated using these samples can only be as accurate as the learned model, so that in many cases it may be easier to directly approximate the value function or the policy using the methods described below. 

\subsection{Value Approximation}
Value-approximation algorithms use samples from the MDP to approximate $V_*$ or $Q_*$ directly and then derive an estimate of the optimal policy by acting greedily with respect to $Q_*$. Typically, when the state and action spaces are large, the value functions are estimated using a parametric function approximator, whose parameters are iteratively updated given the observed samples. Many reinforcement learning algorithms fall into this category and they can be distinguishes based on whether they are on-policy or off-policy and whether they update the value function estimates online or offline. \emph{On-policy} algorithms approximate the state-value function $V_\pi$ or the action-value function $Q_\pi$ from samples of the MDP obtained by following the same policy $\pi$ to be evaluated. Although the optimal policy $\pi_*$ is initially unknown, such algorithms can eventually approximate the optimal value functions $V_*$ or $Q_*$ from which an approximation of the optimal policy can be derived. On the other hand, \emph{off-policy} algorithms can learn the value of a policy different from the one use for obtaining the MDP samples. \emph{Online} algorithms adapt their value approximation after each observed sample while \emph{Offline} algorithms operate on batches of samples. Online algorithms typically require less computation per sample but their convergence is slower. 
%Online on-policy algorithms include temporal-difference (TD) algorithms, such as TD-learning, Sarsa. Offline on-policy algorithms include least-squares approaches, such least-squares temporal difference (LSTD), least-squares policy evaluation (LSPE) and least-squares policy iteration (LSPI). The most known model-free online off-policy algorithm is Q-learning.

\subsection{Policy Approximation}
Policy-approximation algorithms only store a policy and update this policy to maximize a given performance measure and eventually converge to the optimal policy. Since these algorithms only use an estimate of the optimal policy and do not rely on a value function approximation, they are also referred to as \emph{direct policy-search} or \emph{actor-only} algorithms. Algorithms that store both a policy and a value function are commonly known as \emph{actor-critic} algorithms \cite{sutton1999policy}, \cite{konda1999actor}. Policy gradient algorithms, the most studied policy-approximation methods, will be discussed in much more detail in the next chapter. 





