\chapter{Conclusions}
\label{ch:conlusions}

In this chapter, we take the time to revisit what has been achieved in this thesis and to sum up our original contributions to the reinforcement learning literature. Moreover, we draw some conclusions about the impact that these modern optimization techniques can have on the financial industry and the challenges that must be overcome to make it possible. Finally, we suggest some possible axis for future research. 

\section{What Has Been Achieved}
In this thesis, we presented an innovative application of some state-of-the-art policy gradient algorithms to the classical asset allocation problem with transaction costs.\\ In Chapter \ref{ch:discrete_time_stochastic_optimal_control} we introduced the basic concepts of stochastic optimal control in discrete time, which is the standard theoretical framework used to model sequential decision problems. In particular, we presented both the traditional risk-neutral formulation, in which the goal of the agent is to maximize its total returns, and the less common risk-sensitive formulation, in which the agent wishes instead to optimize the trade-off between the rewards and the risk required to achieve them. This chapter laid the theoretical foundations and introduced the notation used in the rest of the thesis.\\  
In Chapter \ref{ch:reinforcement_learning} we presented the main ideas of \glsfirst{RL}, a general class of algorithms in the field of \glsfirst{ML} that allows an agent to learn how to behave in a stochastic and possibly unknown environment only by trial-and-error. This algorithms are typically based on the feedback mechanism between the stochastic environment and the agent, which receives a numerical reward for his actions. After having discussed the characteristic features of the \gls{RL} problem, we provided a high-level overview of the different typologies of algorithms.\\
In Chapter \ref{ch:policy_gradient} we gave an in-depth presentation of Policy Gradient algorithms for the risk-neutral control problem. After having introduced the key ideas of these methods, we provided a thorough review of the state-of-the-art algorithms that can be found in the literature. For episodic environments, most of these methods are based on the well-known likelihood-ratio technique from stochastic optimization  that allows to rewrite the policy gradient as an expected value. For non-episodic environments, the main result from which most of the learning algorithms are derived is the policy gradient theorem.\\ 
In Chapter \ref{ch:risk_sensitive_policy_gradient} we discussed policy gradient methods for the risk-sensitive control problem, which is still an active field of research. In particular, we presented an extension of the policy gradient theorem to the risk-sensitive framework, both in the average reward and in the discounted reward formulations. To the best of our knowledge, this is the first time that a risk-sensitive policy gradient theorem is proved for a general discounted Markov decision process, in which rewards might as well depend on the next state of the system.\\
In Chapter \ref{ch:parameter_based_policy_gradient} we proposed an original parameter-based version of the policy gradient theorem, both for the risk-neutral and the risk-sensitive framework. This extension allowed us to derive efficient online learning algorithms similar in spirit to the well-known \gls{PGPE} algorithm, which was originally conceived only for episodic environments. Moreover, these new algorithms can be easily enhanced using a critic or the natural policy gradient idea. This chapter undoubtedly represents the most innovative contribution of this thesis to the \gls{RL} literature.\\ 
In Chapter \ref{ch:financial_applications_of_reinforcement_learning}, after a brief discussion about why finance represents an extremely challenging field of research, we provided a bibliographical survey of successful financial applications of \gls{RL} techniques. In particular, we focused on the classical asset allocation problem with transaction cost, which has been used to numerically test the algorithms proposed in the previous chapters.\\
In Chapter \ref{ch:numerical_results} we presented the numerical results for the asset allocation problem. We showed that the learning algorithms proposed in the previous chapters perform extremely well on synthetic data, consistently outperforming a simple buy-and-hold strategy and automatically adapting to transaction costs. On the other hand, the algorithms encountered more difficulties on historical data and we tried to provide an explanation for this behavior.\\

\section{Final Remarks}
The main contribution of this thesis to the \gls{RL} literature is the development of some original, online and parameter-based policy gradient algorithms both for the traditional risk-neutral framework and the less common risk-sensitive framework. These algorithms are highly versatile and can be easily combined with some powerful techniques such as the use of a critic or the use of a natural policy gradient.\\
The numerical application of this innovative algorithms to the classical financial problem of determining a profitable long-short trading strategy confirmed their potential. For a synthetic asset, the strategies learned by the algorithms outperformed the simple Buy \& Hold strategy, even when investment decisions were only based on extremely basic autoregressive features. Contrarily to standard prediction-based trading systems, the learning algorithms were able to adapt as expected to the introduction of transaction costs by reducing the frequency of reallocation and of short positions.\\
The algorithms encountered more difficulties in learning a profitable strategy from historical data. In this case, the algorithms failed to converge to a profitable trading strategy probably because of the low signal-to-noise ratio of the historical price series considered and the low capabilities of representation of the controller used. However, even if we considered a more complex controller success would not be a-priori guaranteed. This is due to the inherent complexity of financial markets. Indeed, while identifying patterns in historical data is not particularly hard, finding signals that work reliably in the real world is. Man AHL, a quant unit of Man Group Plc, needed three years of work to gain enough confidence in a machine learning strategy to devote client money to it. Therefore, it is clear that there is still much work to be done to be able to beat the markets with a strategy produced by a \gls{RL}  algorithm.\\
Despite the difficulties encountered in applying the algorithms developed in this thesis to historical data, we feel confident that in the future these techniques will have a large impact on financial markets. The first reason is that these algorithms have already proven successful in solving very difficult decision problems, such as mastering the game of Go or controlling robots in complex environments. The second reason is the versatility of these techniques, which can be applied with only minor modifications to very different domains. Finally, their modularity allows them to be combined with other modern techniques, such as \glsfirst{DL}. These features have attracted the interest of the financial community, with investment banks and hedge funds investing large amount of resources to develop innovative solutions  based on \gls{ML} methods to classical problems. 
 
\section{Further Developments}
To conclude, let us suggest some research directions that could be explored to improve the work presented in this thesis.\\ 
The first idea is to develop more complex features so as to allow the agent to take more informed decisions. A first approach would be to construct the indicators typically used in the technical analysis of stock prices. These measures embed the expert knowledge acquired by financial analysts over decades of activity and could help in guiding the agent towards better decisions. Another approach would be to employ some \gls{DL} techniques to learn more powerful features directly from data. A first possibility would be to extract new features using a deep auto-encoder in a completely unsupervised way that would then be fed into a simple parametric controller. A second possibility would be to directly replace the simple autoregressive controller considered in this thesis with a deep neural network. Using a deep neural network was one of the key ingredient of many of the recent successes of \gls{RL}. For a modern presentation of these techniques, the interested reader may refer to \cite{bengio2015deep}.\\
The second idea would be to consider high-frequency data, such as intra-day prices and traded volumes. It is well documented that high-frequency data show some characteristic patterns that could be captured by a deep reinforcement learning trading system. An example of application of \gls{RL} to high-frequency data can be found in \cite{nevmyvaka2006reinforcement}, where the authors applied some basic algorithms such as Q-learning to the optimal trade execution problem in a limit-order book. It would thus be interesting to apply more advanced techniques to this problem.\\
As the \gls{RL} and \gls{DL} literatures grows at a frenetic rhythm, new research directions keep opening up and new applications in different fields provide plenty of food for thought. Given the versatility of these techniques, we are convinced that a breakthrough application to finance is not far away. 


