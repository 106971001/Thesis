\chapter{Risk-Sensitive Policy Gradient}
\label{ch:risk_sensitive_policy_gradient}

Risk aware decision making plays a crucial role in many fields, such as finance and process control. In this chapter we discuss some policy gradient methods for the risk-sensitive formulation of a sequential decision problem. This extension presents some difficulties which have attracted the interest of many researchers in the last years. In particular, while in the risk-neutral framework the policy gradient theorem represents the keystone for all the learning algorithms, in the risk-sensitive framework the approaches for the episodic setting, the discounted reward and the average reward formulations are quite different. The goal of this chapter is to give an overview of the methods found in the literature and to try to unify them in a way similar to the risk-neutral framework. This chapter and the following represent the main contribution of this thesis to the policy gradient literature.    

\section{Risk-Sensitive Framework}
In the risk-sensitive framework, in addition to maximizing the rewards, the agent also wants to control the risk necessary to achieve it. It is thus necessary to introduce a function $\Lambda: \Theta \to \R$ such that $\Lambda(\theta)$ measures the risk associated with the policy $\pi_\theta$. In an episodic framework where the system always starts from the same initial state $s_0$, the variance of the total return can be used \cite{tamar2012policy}.
\begin{definition}[Start Variance]
	The start variance is the variance of the return that can be obtained starting from the start state $s_0 \in \S$ and following policy $\pi_\theta$
	\begin{equation}
		\Lambda_{\text{start}}(\theta) = \Var[\pi_\theta]{G_0\ |\ S_0 = s_0}
			= U_{\pi_\theta} (s_0) - V_{\pi_\theta} (s_0)^2 
	\end{equation}
\end{definition}
In many application one may want control only the downside risk, that is the risk of the actual return being below the expected return or the uncertainty about the magnitude of that difference. This risk may be measured by the semivariance.
\begin{definition}[Semivariance]
	\begin{equation}
		\Lambda_\textbf{down}(\theta) = \text{SVar}(\theta) = \E[\pi_\theta]{\min\left\{G_0 - \E[\pi_\theta]{G_0}, 0\right\}^2 | S_0 = s_0}
	\end{equation}
\end{definition}	
The square root of this quantity is called semideviation. In a continuing environment, we might consider the long-run variance \cite{prashanth2014actor} defined in Section \ref{sec:risk_sensitive_formulation}.
\begin{definition}[Long-Run Variance]
	The long-run variance $\Lambda_\pi$ under policy $\pi$ is defined as
	\begin{equation}
		\begin{split}
			\Lambda_{\text{long-run}}(\theta) &= \lim_{T \to \infty} \frac{1}{T} \E[\pi_\theta]{
			\sum^{T-1}_{t=0} \left(R_{t+1} - \rho(\theta)\right)^2}\\
		\end{split}
	\end{equation}
\end{definition}
In order to formalize the trade-off between reward and risk from a mathematical point of view, we borrow two standard risk-sensitive performance measures from the financial literature: the mean-variance criterion \cite{markowitz1952portfolio} and the Sharpe ratio \cite{sharpe1994sharpe}.
\begin{definition}[Mean-Variance Criterion]
	The mean-variance criterion is defined as 
	\begin{equation}
		J_\text{MV}(\theta) = J(\theta) - \chi \Lambda(\theta)
	\end{equation}
	where $\chi > 0$ is a constant that controls the trade-off between reward and risk. 
\end{definition} 

\begin{definition}[Sharpe Ratio]
	The Sharpe ratio is defined as 
	\begin{equation}
		\Sh(\theta) = \frac{J(\theta)}{\sqrt{\Lambda(\theta)}}
	\end{equation}
\end{definition} 
Let us remark that in the financial literature the ratio of the expected return and the semideviation is called Sortino ratio. In a risk-sensitive policy gradient algorithm, we try to approximate the optimal parameters that maximize these objective functions by updating the parameters following the gradient ascent directions. In the average-reward formulation, the ascent directions are given by 
\begin{equation}
	\label{eq:gradient_mean_variance}
	\nabla_\theta J_{\text{MV}}(\theta) = \nabla_\theta \rho(\theta) - \chi \nabla_\theta \Lambda(\theta)
\end{equation}
for the mean-variance criterion, where 
\begin{equation}
	\nabla_\theta \Lambda(\theta) =	\nabla_\theta \eta(\theta) - 2 \rho(\theta) \nabla_\theta \rho(\theta)
\end{equation} 
and by 
\begin{equation} 
	\label{eq:gradient_sharpe_ratio}
	\nabla_\theta \Sh(\theta) = \frac{\eta(\theta) \nabla_\theta \rho(\theta) - \frac{1}{2} \rho(\theta) \nabla_\theta \eta(\theta)}{\Lambda(\theta) \sqrt{\Lambda(\theta)}}
\end{equation}
for the Sharpe ratio. Hence, in the risk-sensitive framework it is necessary to estimate to different gradients: $\nabla_\theta \rho(\theta)$ and $\nabla_\theta \eta(\theta)$. The equivalent expressions for the episodic setting can be obtained by replacing $\rho(\theta)$ (resp. $\eta(\theta)$) with $V_\theta(s_0)$ (resp. $U_\theta(s_0)$). The estimation of $\nabla_\theta \rho(\theta)$ (resp. $V_\theta(s_0)$) has been the focus of the last chapter. In the next sections, we discuss instead several techniques to estimate the new gradient $\eta(\theta)$ (resp. $U_\theta(s_0)$).

\section{Monte Carlo Policy Gradient}
For an episodic environment, the REINFORCE algorithm can be easily extended to the risk-sensitive framework described above \cite{tamar2012policy}. Indeed, it is sufficient to adapt its derivation for the average return to the second-moment of return
\begin{equation}
	U(\theta) = \E[H\sim p_\theta]{G(H)^2}
\end{equation}
Applying the likelihood ratio technique, we obtain
\begin{equation}
	\nabla_\theta U(\theta) = \E[H\sim p_\theta]{\nabla_\theta \log p_\theta(H) G(H)^2}
\end{equation}
Similarly to the risk-neutral framework, we can introduce a baseline without affecting the value of the gradient
\begin{equation}
	\nabla_\theta U(\theta) = \E[H\sim p_\theta]{\nabla_\theta \log p_\theta(H) (G(H)^2 - b)}
\end{equation}
In an episodic environment, we can estimate the gradient via its Monte Carlo estimate
\begin{equation}
	\label{eq:reinforce_gradient_2}
	\nabla_\theta U(\theta) \approx \frac{1}{M} \sum_{m = 0}^{M} \nabla_\theta \log p_\theta\left(h^{(m)}\right) \left[G\left(h^{(m)}\right)^2 - b\right]
\end{equation}
where $h^{(m)} = \{(s_t^{(m)}, a_t^{(m)})\}_{t = 0}^{T^{(m)}}$ are $M$ trajectories of the MDP sampled under policy $\pi_\theta$. When using a single trajectory, we obtain the following stochastic gradient estimate 
\begin{equation}
	\nabla_\theta U(\theta) \approx \nabla_\theta \log p_\theta\left(h\right) \left[G\left(h\right)^2 - b\right]
\end{equation}
Again the baseline can be set so as to minimize the variance of the gradient estimate
\begin{equation}
	\label{eq:optimal_baseline_2}
	\widehat{b}_k^* = \frac{\sum^{M}_{m=1} \left[\partial_{\theta_k} \log p_\theta\left(h^{(m)}\right)\right]^2 G(h^{(m)})^2}{\sum^{M}_{m=1} \left[ \partial_{\theta_k} \log p_\theta\left(h^{(m)}\right)\right]^2}
\end{equation}
Combining this estimate with the one for the average return discussed in Section \ref{sec:MCPG} yields the risk-sensitive Monte Carlo Policy Gradient method, which is outlined in Algorithm \ref{algo:RSreinforce}.\\
In an episodic setting, as long as we can rewrite the objective function as expected values on all possible trajectories of the MDP, the likelihood ratio technique yields an analytical expression for its gradient. Hence, this approach can be generalized to more complex measures of risk commonly used in finance, such as the semivariance introduced above. Indeed, the semivariance can be rewritten as 
\begin{equation}
	\text{SVar}(\theta) = \E[H \sim p_\theta]{\min\left\{G(H) - \E[H \sim p_\theta]{G(H)}, 0\right\}^2}
\end{equation}
Hence, by the likelihood ratio technique
\begin{equation}
	\nabla_\theta \text{SVar}(\theta) = \E[H \sim p_\theta]{\nabla_\theta \log p_\theta(H) \min\left\{G(H) - \E[H \sim p_\theta]{G(H)}, 0\right\}^2}
\end{equation}
From which we can easily derive a Monte Carlo estimate. Since the problems we will consider in the next chapters are not episodic. In the episodic setting, the extension of policy gradient algorithms to the risk-sensitive formulation doesn't present particular difficulties. For a more thorough presentation as well as some more advanced learning algorithms, we refer the interested reader to the extensive work of Tamar et Al.  \cite{tamar2013temporal}, \cite{tamar2013variance}, \cite{tamar2015policy}, \cite{chow2015risk}.
\begin{algorithm}[t]
	\caption{Risk-sensitive REINFORCE policy gradient estimate}
	\label{algo:RSreinforce}
	\begin{algorithmic}[1]
		\Require Policy parameterization $\theta$, number of trajectories $M$
		\Ensure Risk-sensitive REINFORCE policy gradient estimate
		\State Sample $M$ trajectories of the MDP following policy $\pi_\theta$
		\State Compute the optimal baseline for the return via Eq. (\ref{eq:optimal_baseline})
		\State Compute the optimal baseline for the squared return via Eq. (\ref{eq:optimal_baseline_2})
		\State Compute the gradient of the expected return via Eq. (\ref{eq:reinforce_gradient})
		\State Compute the gradient of the expected squared return via Eq. (\ref{eq:reinforce_gradient_2})
		\State Compute the risk-sensitive policy gradient via either Eq. (\ref{eq:gradient_mean_variance}) or (\ref{eq:gradient_sharpe_ratio})
	\end{algorithmic}
\end{algorithm}

\section{Policy Gradient Theorem}
In this section the policy gradient theorem is extended to the risk-sensitive framework. In the average reward formulation of the control problem, the theorem and its derivation are analogous to those for the risk-neutral framework. This will allow us to derive in a trivial way the risk-sensitive versions of all the learning algorithms seen in the previous chapter. The risk-sensitive policy gradient theorem for the average-reward formulation was first derived in \cite{prashanth2014actor} and the presentation of this section closely follows the original article.\\
On the other hand, obtaining an equivalent theorem for the discounted reward formulation is more challenging. In the article cited above, the authors prove the theorem under a very strong assumption on the dependence of rewards obtained at different time steps which is not verified in many applications, among which is the asset allocation problem that we will consider for the numerical applications. In the following sections, we will generalize this result by assuming that the reward can depend on both the initial state and the final state of the system. Furthermore, we will discuss the problems arising in the discounted setting and why it is not easy to derive an online policy gradient theorem in this case.\\
 In the original article the authors mostly considered the mean-variance criterion since all the algorithms they propose are easily adapted to the Sharpe ratio criterion. Here we take the opposite direction and present the algorithms for the Sharpe ratio, referring to their article for the mean-variance counterparts.\\   
Let us consider a family of parametrized policies $\pi_\theta$, with $\theta
\in \Theta \subseteq \R^{D_\theta}$. The optimization problem then becomes
\begin{equation}
	\max_\theta \text{Sh}(\theta) = \frac{\rho(\theta)}{\sqrt{\Lambda(\theta)}}
\end{equation}
where we denoted by $\rho(\theta) = \rho_{\pi_\theta}$ and similarly for the other quantities. Using a policy gradient approach, the policy parameters are updated following the gradient ascent direction.

\subsection{Average Reward Formulation}
In the average reward formulation the gradient of the Sharpe ratio is
\begin{equation}
	\nabla_\theta \text{Sh}(\theta) = \frac{\eta(\theta) \nabla_\theta \rho(\theta) - \frac{1}{2} \rho(\theta) \nabla_\theta \eta(\theta)}{\Lambda(\theta) \sqrt{\Lambda(\theta)}}
\end{equation}
Hence, to compute the update direction, it is sufficient to estimate the various quantities appearing in this formula. For instance, the average reward $\rho(\theta)$, the average square reward $\eta(\theta)$ and the reward variance $\Lambda(\theta)$ can be approximated using exponentially weighted moving averages. On the other hand, the gradient of the average reward $\nabla_\theta \rho(\theta)$ is given by the standard policy gradient theorem \ref{thm:risk_neutral_policy_gradient}. The only term remaining is the gradient of the average square reward $\nabla_\theta \eta(\theta)$, which is provided by the risk-sensitive policy gradient theorem 
\begin{theorem}[Risk-Sensitive Policy Gradient]
	Let $\pi_\theta$ be a differentiable policy. The policy gradient for the average square reward is given by
	\begin{equation}\label{eq:policy_gradient_theorem_W}
		\begin{split}
			\nabla_\theta \eta(\theta) &= \E[\substack{S\sim d_\theta \\ 
			A \sim \pi_\theta}]{\nabla_\theta \log \pi_\theta(S,A) W_\theta(S,A)}
		\end{split}
	\end{equation}
	where $d^\theta$ is the stationary distribution of the Markov chain induced by $\pi_\theta$. 
\end{theorem}
\begin{proof}
	The proof is analogous to that of the risk-neutral version of theorem. From the basic relation between state-value function and action-value function, we have
		\begin{equation*}
			\begin{split}
				\nabla_\theta U_\theta(s) &= \nabla_\theta \int_{\A} \pi_\theta(s,a) W_\theta(s,a) da\\
					&= \int_{\A} \left[ \nabla_\theta \pi_\theta(s,a) W_\theta(s,a) + \pi_\theta(s,a) \nabla_\theta W_\theta(s,a)\right] da
			\end{split}
		\end{equation*} 
		Using the Bellman expectation equation for $W_\theta$ 
		\begin{equation*}
			\begin{split}
				\nabla_\theta W_\theta(s,a) &= \nabla_\theta \left[ \calM(s,a) - \eta_\theta + \int_{\S} \calP(s,a,s') U_\theta(s') ds' \right]\\
				&= -\nabla_\theta \eta_\theta + \int_{\S} \calP(s,a,s') \nabla_\theta U_\theta(s') ds'
			\end{split}
		\end{equation*}
		Hence, plugging in the first equation 
		\begin{equation*}
				\nabla_\theta U_\theta(s) = \int_{\A} \nabla_\theta \pi_\theta(s,a) W_\theta(s,a) da - \nabla_\theta \eta_\theta + \int_\A \pi_\theta(s,a) \int_{\S} \calP(s,a,s') \nabla_\theta U_\theta(s') ds' 
		\end{equation*} 	
		Integrating both sides with respect to the stationary distribution $d^\theta$ and noting that, because of stationarity,  
		\begin{equation*}
			\int_{\S} d^\theta(s) \int_{\A} \pi(s,a) \int_{\S} \calP(s,a,s') \nabla_\theta U(s') ds' da ds = \int_{\S} d^\theta(s) \nabla_\theta U_\theta(s) ds
		\end{equation*}
		we obtain the result 
		\begin{equation*}
			\begin{split}
			\nabla_\theta \eta_\theta &= \int_{\S} d^\theta(s) \int_{\A} \nabla_\theta \pi_\theta(s,a) W_\theta(s,a) da ds\\
			&= \int_{\S} d^\theta(s) \int_{\A} \pi_\theta(s,a) \nabla_\theta \log\pi_\theta(s,a) W_\theta(s,a) da ds\\
			&= \E[\substack{S \sim d^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
					\pi_\theta(S,A) W_{\theta}(S, A)} 
			\end{split}
		\end{equation*}
\end{proof}
The result is identical to that for the average reward, provided that we replace the action-value function $Q_\theta(s,a)$ by the square action-value function $W_\theta(s,a)$. As in the standard risk-neutral case, a state-dependent baseline can be introduced in the gradient without changing the result. In particular, by 
using the average-adjusted square value function as baseline, we can replace the 
average adjusted action-value functions with the following square advantage function
\begin{equation}
	B_\theta(s,a) = W_\theta(s,a) - U_\theta(s)
\end{equation}
Thus, the gradients can be written as 
\begin{equation}\label{eq:pg_advantage_W}
	\nabla_\theta \eta(\theta) = \E[\substack{S\sim d_\pi \\ 
A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) B_\theta(S,A)}
\end{equation}
From this result, following the exact same reasoning used in the risk-neutral framework, we can design a variety of risk-sensitive actor-critic algorithms, which
employ an approximation of the standard and of the square advantage functions to obtain a more accurate estimate of the objective function. 

\subsection{Risk-Sensitive Actor-Critic Algorithm}
In \cite{prashanth2014actor}, starting from Eqs. (\ref{eq:actor_critic_pg}) and 
(\ref{eq:pg_advantage_W}), the authors propose a TD(0) risk-sensitive actor-critic 
algorithm for the average reward setting. The algorithm maintains two critics that estimate the average adjusted value-functions $V_\theta(S)$ and $U_\theta(S)$ respectively and are updated via a TD(0) temporal difference scheme. Let $\delta_n^A$ and $\delta_n^B$ be the TD errors for residual value
and square value functions
\begin{equation}
	\begin{split}
		\delta_t^A &= R_{t+1} - \widehat{\rho}_{t+1} + \widehat{V}(S_{t+1}) -
		\widehat{V}(S_t)\\
		\delta_t^B &= R_{t+1}^2 - \widehat{\eta}_{t+1} + \widehat{U}(S_{t+1}) -
		\widehat{U}(S_t)\\
	\end{split}
\end{equation}
where $\widehat{V}$, $\widehat{U}$, $\widehat{\rho}$ and $\widehat{\eta}$ are
unbiased estimate of $V_\theta$, $U_\theta$, $\rho(\theta)$ and $\eta(\theta)$
respectively. It is easy to show that $\delta_t^A$ and $\delta_t^B$ are
unbiased estimates of the advantage functions.
\begin{equation*}
	\begin{split}
		\E[\theta]{\delta_t^A|S_t = s, A_t = a} &= A_\theta(s, a)\\
		\E[\theta]{\delta_t^B|S_t = s, A_t = a} &= B_\theta(s, a)\\
	\end{split}
\end{equation*}
An estimate of the gradients can be obtained by replacing the advantage functions with the TD errors
\begin{equation}
	\begin{split}
		\nabla_\theta \rho(\theta) &\approx \nabla_\theta \log \pi_\theta(S_t, A_t) \delta_t^A\\
		\nabla_\theta \eta(\theta) &\approx \nabla_\theta \log \pi_\theta(S_t, A_t) \delta_t^B\\
	\end{split}
\end{equation}
The value functions are linearly approximated using some feature vectors
$\Phi_V: \S \to \R^{D_V}$ and $\Phi_U: \S \to \R^{D_U}$ as follows
\begin{equation}
	\begin{split}
		\widehat{V}(s) &= \psi_V^T \Phi_V(s)\\
		\widehat{U}(s) &= \psi_U^T \Phi_U(s)\\
	\end{split}
\end{equation}
Combining all these ingredients lead to the Risk-Sensitive Average Reward Actor-Critic algorithm (RSARAC), the pseudocode of which is reported in Algorithm \ref{algo:RSARAC}.
Let us notice that the algorithm is a three time-scale stochastic approximation algorithm, where the learning rates, in addition to the usual Robbins-Monro conditions, should satisfy
\begin{equation*}
	\alpha_k < \beta_k < \gamma_k
\end{equation*}

\begin{algorithm}[t!]
	\caption{Risk-Sensitive Average Reward Actor-Critic algorithm}
	\label{algo:RSARAC}
	\begin{algorithmic}[]
		\Require {\\ 
			\begin{itemize} 
				\item Initial actor parameters $\theta^0$
				\item Initial critics parameters $\psi_V^0$ and $\psi_U^0$
				\item Actor learning rate $\{\alpha_k\}$
				\item Critics learning rate $\{\beta_k\}$
				\item Averages learning rate $\{\gamma_k\}$
			\end{itemize}}
		\Ensure Approximation of the optimal policy $\pi_{\theta^*} \approx \pi_*$
		\begin{algorithmic}[1]
		\Repeat
			\State Observe tuple $<s_k, a_k, r_{k+1}, s_{k+1}>$ sampled from the MDP.
			\State Update averages 
				\begin{equation*}
					\begin{split}
					\widehat{\rho}_{k+1} &= (1 - \gamma_k) \widehat{\rho}_k + 	\gamma_k r_{k+1}\\
					\widehat{\eta}_{k+1} &= (1 - \gamma_k) \widehat{\rho}_k + 	\gamma_k r_{k+1}^2\\
					\end{split}
				\end{equation*}
			\State Compute TD errors	
				\begin{equation*}
					\begin{split}
						\delta_k^A &= r_{k+1} - \widehat{\rho}_{k+1} + (\psi_V^k)^T \Phi_V(s_{k+1}) - (\psi_V^k)^T \Phi_V(s_k)\\
						\delta_k^B &= r_{k+1}^2 - \widehat{\eta}_{k+1} + (\psi_U^k)^T \Phi_U(s_{k+1}) - (\psi_U^k)^T \Phi_U(s_k)\\
					\end{split}
				\end{equation*}
			\State Update critic parameters 
				\begin{equation*}
					\begin{split}
						\psi_V^{k+1} &= \psi_V^k + \beta_k \delta_k^A \Phi_V(s_k)\\
						\psi_U^{k+1} &= \psi_U^k + \beta_k \delta_k^B \Phi_U(s_k)\\
					\end{split}
				\end{equation*}
			\State Update actor parameters $\theta^{k+1} = \theta^k + \alpha_k \widehat{\text{Sh}}(\theta_k) $. 
			\State $k \leftarrow k + 1$
		\Until{converged}
		\end{algorithmic}
	\end{algorithmic}
\end{algorithm}

\subsection{Discounted Reward Formulation}
In the discounted reward formulation, we need to adapt the definition of the Sharpe ratio associated to a policy. Let us suppose that the system always start in the same initial state $s_0$, then we can introduce the start-state Sharpe ratio that can be achieved following policy $\pi_\theta$ as
\begin{equation}
	\text{Sh}(\theta) = \frac{V_\theta(s_0)}{\sqrt{\Lambda_\theta(s_0)}}
\end{equation}
where $V_\theta$ and $\Lambda_\theta$ are the state-value function and the variance-function introduced in Chapter \ref{ch:discrete_time_stochastic_optimal_control}. 
Hence, the gradient of the Sharpe ratio is given by
\begin{equation}
	\nabla_\theta \text{Sh}(\theta) = \frac{U_\theta(s_0) \nabla_\theta V_\theta(s_0) - \frac{1}{2} V_\theta(s_0) \nabla_\theta U_\theta(s_0)}{\Lambda_\theta(s_0) \sqrt{\Lambda_\theta(s_0)}}
\end{equation}
The gradient of the state-value function $\nabla_\theta V_\theta(s_0)$ is given by the risk-neutral policy gradient theorem. Therefore, in order to approximate the gradient of the Sharpe ratio, we need to estimate the value functions $V_\theta(s_0)$, $U_\theta(s_0)$, the variance $\Lambda_\theta(s_0$), and the gradient of the square state-value function $U_\theta (s_0)$. The first three terms might be easily approximated using moving averages. On the other hand, estimating $\nabla_\theta U_\theta(s_0)$ in the same spirit of the policy gradient theorem is much more delicate. 
\begin{theorem}[Risk-Sensitive Policy Gradient]
	Let $\pi_\theta$ be a differentiable policy. The policy gradient for the square state-value function is given by
	\begin{equation}
		\begin{split}
			\nabla_\theta U_\theta(s_0) =
			\mathbb{E}_{\substack{S \sim d_{\gamma^2}^\theta(s_0, \cdot)\\A \sim \pi_\theta(S,\cdot)\\S' \sim \calP(S,A,\cdot)}} [\nabla_\theta &\log	\pi_\theta(S,A) W_{\theta}(S, A)\\
			&+ 2 \gamma \calR(S,A) \nabla_\theta V_\theta(S') + 2 \gamma C_\theta(S, A) ]
		\end{split}
	\end{equation}
	where $d_{\gamma^2}^\theta(s_0, \cdot)$ is the $\gamma^2$-discounted visiting distribution over states starting from the initial state $s_0$ and following policy $\pi_\theta$
		\begin{equation}
			d_{\gamma^2}^\theta(s_0, x) = \sum_{k=0}^{\infty} \gamma^{2k} \calP_\theta^{(k)}(s_0, x)
		\end{equation}
\end{theorem}
\begin{proof}
	From the basic relation between square state-value function and the square action-value function, we have
	\begin{equation*}
		\begin{split}
			\nabla_\theta U_\theta(s) &= \nabla_\theta \int_{\A} \pi_\theta(s,a) W_\theta(s,a) da\\
				&= \int_{\A} \left[ \nabla_\theta \pi_\theta(s,a) W_\theta(s,a) + \pi_\theta(s,a) \nabla_\theta W_\theta(s,a)\right] da
		\end{split}
	\end{equation*} 
	Hence, using the Bellman expectation equation for $W_\theta$ 
	\begin{equation*}
		\begin{split}
			\nabla_\theta W_\theta(s,a) &= \nabla_\theta \left[ \calM(s,a) + 2 \gamma \calR(s,a) T_a V_\pi(s) + 2 \gamma C_\theta(s, a) + \gamma^2 T_a U_\theta(s)\right]\\ 
			&= 2 \gamma \calR(s,a) \nabla_\theta T_a V_\theta(s) + 2 \gamma \nabla_\theta C_\theta(s, a) + \gamma^2 \nabla_\theta T_a U_\theta(s)\\
			&= \int_{\S} \calP(s,a,s') \left[2 \gamma \calR(s,a) \nabla_\theta V_\theta(s') + 2 \gamma \nabla_\theta C_\theta(s, a) + \nabla_\theta U_\theta(s')\right] ds'
		\end{split}
	\end{equation*}
	where we assumed to be able to exchange the gradient and the integral. Plugging in the first equation and exploiting the fact that $\int_\S \calP(s,a,s') ds' = 1$, we obtain
	\begin{equation*}
		\begin{split}
			\nabla_\theta U_\theta(s) = \int_{\A} \pi_\theta(s,a) &\int_{\S} \calP(s,a,s') [ \nabla_\theta \log \pi_\theta(s,a) W_\theta(s,a)\\ 
			&+ 2 \gamma \calR(s,a) \nabla_\theta V_\theta(s') + 2 \gamma \nabla_\theta C_\theta(s, a) + \nabla_\theta U_\theta(s') ] ds' da
		\end{split}
	\end{equation*} 
	Unrolling $\nabla_\theta U_\theta$ infinite times and denoting by $\calP_\theta^{(k)}(s, x)$ the probability of going from state $s$ to state $x$ in $k$ steps under policy $\pi_\theta$, we obtain
	\begin{equation*}
		\begin{split}
			\nabla_\theta U_\theta(s) = \sum_{k=0}^\infty \gamma^{2k} \int_\S \calP_\theta^{(k)}(s, x) &\int_\A \pi_\theta(x,a) \int_{\S} \calP(x,a,x') [\nabla_\theta \log \pi_\theta(x,a) W_\theta(x,a)\\ 
						&+ 2 \gamma \calR(x,a) \nabla_\theta V_\theta(x') + 2 \gamma \nabla_\theta C_\theta(x, a)]  dx' da dx
		\end{split}
	\end{equation*} 
	Defining the $\gamma^2$-discounted visiting distribution of state $x$ starting from state $s$ as
	\begin{equation*}
		d_{\gamma^2}^\theta(s, x) = \sum_{k=0}^{\infty} \gamma^{2k} \calP_\theta^{(k)}(s, x)
	\end{equation*}
	we have the result
	\begin{equation*}
		\begin{split}
			\begin{split}
				\nabla_\theta U_\theta(s) = \int_\S d_{\gamma^2}^\theta(s, x) &\int_\A \pi_\theta(x,a) \int_{\S} \calP(x,a,x') [\nabla_\theta \log \pi_\theta(x,a) W_\theta(x,a)\\ 
							&+ 2 \gamma \calR(x,a) \nabla_\theta V_\theta(x') + 2 \gamma \nabla_\theta C_\theta(x, a)]  dx' da dx
			\end{split}
		\end{split}
	\end{equation*} 
\end{proof}
Compared to risk-sensitive policy gradient theorem in the average reward formulation, we have two additional terms: one term depending on the gradient of the state-value function at the next state and another term depending on the covariance between the one-step reward and the successive return. These terms are very difficult to approximate online in a continuing environment. Therefore, the result is of practical interest only for an episodic environment where the experiments have a finite (possibly random) lifespan. Since the application we considered does not fall in this category, we will not discuss any risk-sensitive algorithm for the discounted formulation. 
