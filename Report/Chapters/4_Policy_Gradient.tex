\chapter{Policy Gradient}

Policy gradient methods directly store and iteratively improve a parametric approximation of the optimal policy. This policy is commonly referred to as an $\emph{actor}$ and methods that directly approximate the policy without exploiting an approximation of the optimal value function are called $\emph{actor-only}$. Algorithms that combine an approximation of the optimal policy with an approximation of the value function are commonly called $\emph{actor-critic}$ methods.\\
As discussed in Chapter \ref{ch:discrete_time_stochastic_optimal_control}, an optimal policy can be obtained by simply acting greedily with respect to the optimal action-value function. However, in large or continuous action spaces, this leads to a complex optimization problem that is computationally expensive to solve. Therefore, it can be beneficial to store an explicit estimation of the optimal policy from which we can select actions. Policy gradient methods have other advantages compared to standard value-based approaches. In many applications, a good policy has a more compact		representation than the value function so that it may be easier to approximate. Moreover, the policy parametrization can be chosen such that is relevant for the task and prior knowledge can be directly incorporated in the policy. Another advantage is that these methods can learn stochastic policies and not only deterministic ones, which might be useful in multiplayer frameworks or in partially observable environments. Finally, these methods have better convergence properties and are guaranteed to converge at least to a local optimum, which may be good enough in practice. On the other hand, policy gradient methods are typically characterized by a large variance which may hinder the converge speed.\\
In the following sections, we present the basics of policy gradient methods closely following the thorough overviews provided in \cite{peters2008reinforcement}. Then, we discuss some state-of-the-art policy gradient algorithms and we propose some original extensions of these methods. 

\section{Basics of Policy Gradient Methods}
In policy gradient methods, the optimal policy is approximated using a parametrized 
policy $\pi: \S \times \calA \times \Theta \to \R$ such that, given a parameter vector $\theta \in \Theta \subseteq \R^{D_\theta}$, $\pi(s, B; \theta) = \pi_\theta(s, B)$ gives the probability of selecting an action in $B \in \calA$ when the system is in state $s \in \S$.
The general goal of policy optimization in reinforcement learning is to
optimize the policy parameters $\theta \in \Theta$ so as to maximize a certain
objective function $J: \Theta \to \R$
\begin{equation}
	\max_{\theta \in \Theta} J(\theta)
\end{equation}
Depending on the application, different objective functions can be used to measure the performance of a policy. In an episodic environment where the system always starts from an initial state $s^*$, the typical objective function is the start value.
\begin{definition}[Start Value]
	The start value is the expected return that can be obtained starting from the start state $s^* \in \S$ and following policy $\pi_\theta$
	\begin{equation}
		J_{\text{start}}(\theta) = V_{\pi_\theta}(s^*) = \E[\pi_\theta]{G_t\ |\
		   S_t = s^*}
	\end{equation}
\end{definition}
In a continuing environment, it is common to use either the average value or the average reward per time step.
\begin{definition}[Average Value]
	The average value is the expected value that can be obtained following policy $\pi_\theta$ 
	\begin{equation}
		J_{\text{avV}}(\theta) = \E[S \sim d^{\theta}]{V_{\pi_\theta}(S)} = \int_\S
		d^{\theta}(s) V_{\pi_\theta}(s) ds
	\end{equation}
	where $d^\theta$ is the stationary distribution of the Markov chain induced by $\pi_\theta$.
\end{definition}
\begin{definition}[Average Reward per Time Step]
	The average reward per time step is the expected reward that can be
	obtained over a single time step by following policy $\pi_\theta$ 
	\begin{equation}
		J_{\text{avR}}(\theta) = \E[\substack{S \sim d^{\theta}\\A \sim \pi_\theta}]{\calR(S,A)} 
		= \int_\S d^{\theta}(s) \int_\A \pi_\theta(s,a) \calR(s,a) da ds
	\end{equation}
	where $d^\theta$ is the stationary distribution of the Markov chain induced by $\pi_\theta$.
\end{definition}
Fortunately, the same methods apply to the three formulations. In the
following, we will focus on gradient-based and model-free methods that exploit
the sequential structure of the the reinforcement learning problem. The idea of
policy gradient algorithms is to update the policy parameters using the
gradient ascent direction of the objective function
\begin{equation}
	\theta^{k+1} = \theta^k + \alpha_k \nabla_\theta J\left(\theta^k\right)
\end{equation}
where $\{\alpha_k\}_{k\geq 0}$ is a sequence of learning rates. Typically, the
gradient of the objective function is not known and needs to be estimated. The general setup for a policy gradient algorithm is thus shown in Algorithm \ref{algo:policy_gradient}. It is a well-know result from stochastic optimization that, if the gradient estimate is unbiased and the learning rates satisfy the \emph{Robbins-Monro conditions}
\begin{equation}
	\sum_{k=0}^\infty \alpha_k = \infty \;\;\;\;\;\; \sum^{\infty}_{k=0}
	\alpha_k^2 < \infty 
\end{equation}
the learning process is guaranteed to converge at least to a local optimum of
the objective function. In the following sections, we describe various methods
of approximating the gradient. 

\begin{algorithm}[t]
	\caption{General setup for a policy gradient algorithm.}
	\label{algo:policy_gradient}
	\begin{algorithmic}[1]
		\Require Initial parameters $\theta_0$, learning rate $\{\alpha_k\}$
		\Ensure Approximation of the optimal policy $\pi_*$
		\Repeat
			\State Obtain estimate $\widehat{g}$ of policy gradient $\nabla_\theta J\left(\theta^k\right)$
			\State Update parameters using gradient ascent $\theta^{k+1} = \theta^k + \alpha_k \widehat{g}$
			\State $k \leftarrow k + 1$
		\Until{converged}
	\end{algorithmic}
\end{algorithm}

\section{Finite Differences}
The most straightforward way to estimate the objective function gradient consists in replacing the partial derivatives with the corresponding finite differences. In other words, the $k$-th gradient component is approximated by
\begin{equation}
	\frac{\partial J(\theta)}{\partial\theta_k} \approx \frac{J(\theta + \epsilon e_k) - J(\theta)}{\epsilon}
\end{equation}
where $\epsilon$ is a small constant and $e_k \in \R^{D_\theta}$ is the $k$-th element of the canonical basis. Hence, to compute the gradient it is sufficient to estimate the objective function for $D_\theta + 1$ different parameters combinations. Since the objective function is unknown, it must be estimated from sample trajectories simulated following the policies associated to the parameterizations appearing in the finite differences. For this reason, this method is computationally demanding and the gradient estimate obtained in this way is extremely noisy, which may slow down or even prevent the convergence of the algorithm. Still, this approach is sometimes effective and works for arbitrary, even non-differentiable, policies. Moreover, finite differences are often used to check gradient estimates during debugging.

\section{Monte Carlo Policy Gradient}
In this section, we describe one of the oldest policy gradient methods which is based on a standard stochastic optimization technique called the \emph{likelihood trick}. 
Let $h = {\{(s_t, a_t)\}}_{t\geq 0} \in \H$ be a given trajectory and let us 
denote by $p_\theta(h) = \P[\pi_\theta]{H = h}$ the probability of obtaining 
this trajectory by following policy $\pi_\theta$. Let $G(h)$ denote the expected
return obtained on trajectory $h$
\begin{equation}
	G(h) = \E{G_0 | H_t = h} = \sum_{t=1}^\infty \gamma^k \calR(s_{t-1},
	a_{t-1}) 
\end{equation}
For simplicity, let us consider the average value objective function which can
be rewritten as an expectation over all possible trajectories
\begin{equation}
	J_{\text{avV}}(\theta) = \int_\H p_\theta(h) G(h) dh
\end{equation}
We can compute its gradient using the likelihood ratio trick 
\begin{equation}
	\begin{split}
		\nabla_\theta J(\theta) &= \int_\H \nabla_\theta p_\theta(h) G(h) dh\\
								&= \int_\H p_\theta(h)\nabla_\theta \log
									p_\theta(h)G(h)dh\\
								&= \E{\nabla_\theta \log p_\theta(H)G(H)}
	\end{split}
\end{equation}
where the expectation is taken over all possible trajectories. The crucial
point is that $\nabla_\theta \log p_\theta(H)$ can be computed without
knowledge of the transition probability kernel $\calP$. Indeed, by the Markov property
\begin{equation*}
	p_\theta(h) = \P{S_0 = s_0} \prod_{t=0}^\infty \pi_\theta(s_t, a_t)
	\calP(s_t, a_t, s_{t+1})
\end{equation*}
hence
\begin{equation*}
	\log p_\theta(h) = \log \P{S_0 = s_0} + \sum_{t=0}^\infty \log 
	\pi_\theta(s_t, a_t) + \sum_{t=0}^\infty \log \calP(s_t, a_t, s_{t+1})
\end{equation*}
The only term depending on the parameters $\theta$ is the policy term, so that
\begin{equation}
	\nabla_\theta \log p_\theta(H) = \sum_{t=0}^\infty \nabla_\theta \log 
	\pi_\theta(s_t, a_t)
\end{equation}
So we do not need the transition model to compute the $\nabla_\theta \log 
p_\theta(H)$. However, this trick only works if the policy is differentiable with respect to $\theta$ and it is stochastic. In most cases this is not a big problem, since stochastic policies are needed anyway to ensure sufficient exploration. Moreover, since
\begin{equation*}
	\int_\H \nabla_\theta p_\theta(h) dh = 0	
\end{equation*}
a constant baseline $b \in \R$ can always be added in the gradient formula
\begin{equation}
	\nabla_\theta J(\theta) = \E{\nabla_\theta \log p_\theta(H)(G(H)-b)}
\end{equation}
We will see how this baseline can be chosen in order to minimize the variance
of the estimator. In an episodic environment, we can derive an estimate of the 
objective function gradient by sampling $M$ trajectories $h^{(m)} = \{(s_t^{(m)},
a_t^{(m)})\}_{t = 0}^{T^{(m)}}$ from the MDP and by approximating the expected 
value via Monte Carlo
\begin{equation}
\label{eq:reinforce_gradient}
	\widehat{g}_{\text{RF}} = \frac{1}{M} \sum^{M}_{m=1} \left[ \sum_{i=0}^{T^{(m)}} 
	\nabla_\theta \log \pi_\theta(s_i^{(m)}, a_i^{(m)}) \right] \left[
	\sum^{T^{(m)}}_{j=0} \gamma^j r_{j+1}^{(m)} - b \right]  
\end{equation}
This method, synthetized in Algorithm \ref{algo:reinforce}, is known in the literature as the REINFORCE algorithm and is guaranteed to converge to the true gradient at a pace of $O(M^{-1/2})$. In practice, we can obtain an approximation of the gradient using only one sample which leads to a stochastic gradient ascent method
\begin{equation}
	\widehat{g}_{\text{SRF}} = \left[ \sum_{i=0}^{T} \nabla_\theta \log \pi_\theta(s_i, 
	a_i) \right] \left[ \sum^{T}_{j=0} \gamma^j r_{j+1} - b \right]  
\end{equation}
This method is very easy and works well on many problems. However, the gradient
estimate is characterized by a large variance which can hamper the convergence
rate of the algorithm. A first approach to address this issue is to optimally
set the benchmark to reduce the estimate variance and will be discussed in detail in the next section.

\begin{algorithm}[t]
	\caption{Episodic REINFORCE policy gradient estimate}
	\label{algo:reinforce}
	\begin{algorithmic}[1]
		\Require Policy parameterization $\theta$, number of trajectories $M$
		\Ensure REINFORCE policy gradient estimate $\widehat{g}_{RF} \approx \nabla_\theta J(\theta)$
		\State Sample $M$ trajectories of the MDP following policy $\pi_\theta$
		\State Compute the return baseline $b$
		\State Compute $\widehat{g}_{\text{RF}}$ according to Eq. (\ref{eq:reinforce_gradient})
	\end{algorithmic}
\end{algorithm}

\subsection{Optimal Baseline}
A standard variance reduction technique, called control variate, consists in setting the baseline so as to minimize the gradient estimate variance. More in detail, the optimal baseline for the $k$-th gradient component $\widehat{g}_{k}$ solves
\begin{equation}
	b_k^* = \argmin_b \Var{\widehat{g}_{k}}
\end{equation} 
It is easy to show that 
\begin{equation}
	b_k^* = \frac{\E{G(H) \left(\partial_{\theta_k} \log p_\theta(H)\right)^2  }}{\E{\left(\partial_{\theta_k} \log p_\theta(H)\right)^2}}
\end{equation}
which can be approximated by 
\begin{equation}
	\widehat{b}_k^* = \frac{\sum^{M}_{m=1} \left[ \sum_{i=0}^{T^{(m)}} 
		\partial_{\theta_k} \log \pi_\theta\left(s_i^{(m)}, a_i^{(m)}\right) \right]^2 
		\sum^{T^{(m)}}_{j=0} \gamma^j r_{j+1}^{(m)}}{\sum^{M}_{m=1} \left[ \sum_{i=0}^{T^{(m)}} \partial_{\theta_k} \log \pi_\theta\left(s_i^{(m)}, a_i^{(m)}\right) \right]^2}
\end{equation}

\subsection{Boltzmann Exploration Policy}
In discrete action spaces, the Boltzmann exploration policy, also known as softmax policy, is a common choice. In state $s$, this policy selects an action $a$ with probability 
\begin{equation}
	\pi_\theta(s,a) = \frac{e^{\theta^T \Phi(s,a)}}{\sum_{b \in \A} e^{\theta^T \Phi(s,b)}}
\end{equation}    
where $\Phi(s,a) \in \R^{D_\theta}$ is a given feature vector corresponding to state $s$ and action $a$. the likelihood score for this policy is thus given by 
\begin{equation}
\nabla_\theta \log \pi_\theta(s,a) = \Phi(s,a) - \sum_{b\in\A} \pi_\theta(s,a) \Phi(s,b)
\end{equation}

\subsection{Gaussian Exploration Policy} 
In continuous action spaces, a Gaussian exploration policy is commonly used. According to this policy, in a state $s$, actions are sampled from a Gaussian distribution with a parametric state-dependent mean $\mu_\psi(s) \in \R^{D_a}$, with $\psi \in \R^{D_\psi}$, and a covariance matrix $\Sigma \in \R^{D_a \times D_a}$. The policy parameters consist of $\theta = {\psi, \Sigma}$ and the likelihood score are thus given by 
\begin{equation}
	\nabla_\psi \log \pi_\theta(s,a) = \left(\frac{\partial \mu_\psi(s)}{\partial \psi}\right)^T \Sigma^{-1} (a - \mu_\psi(s))
\end{equation}
\begin{equation}
	\nabla_\Sigma \log \pi_\theta(s,a) = \frac{1}{2}\left[\Sigma^{-1} \left(a - \mu_\psi(s)\right) \left(a - \mu_\psi(s)\right)^T \Sigma^{-1} - \Sigma^{-1}\right]
\end{equation}
where $\frac{\partial \mu_\psi(s)}{\partial \psi}$ denotes the Jacobian matrix of $\mu_\psi$ with respect to $\psi$.


\section{Policy Gradient Theorem}
In the last section, we said that the REINFORCE gradient estimate is
characterized by a large variance, which may slow the method's convergence. 
To improve the estimate, it is sufficient to notice that future actions do not
depend on past rewards, unless the policy has been changed. Therefore, 
\begin{equation}
	\E[\pi_\theta]{\nabla_\theta\log \pi_\theta(S_t, A_t) \calR(S_s, A_s)} = 0
	\;\;\;\;\; \forall t>s
\end{equation}
From this trivial remark, we can derive two estimates of the objective function
gradient
\begin{equation}
	g_{\text{PG}} = \frac{1}{M} \sum^{M}_{m=1} \sum_{i=0}^{T^{(m)}} 
	\nabla_\theta \log \pi_\theta(s_i^{(m)}, a_i^{(m)}) \left( 
	\sum^{T^{(m)}}_{j=i} \gamma^j r_{j+1}^{(m)} - b \right)
\end{equation}
\begin{equation}
	g_{\text{GPOMDP}} = \frac{1}{M} \sum^{M}_{m=1} \sum_{j=0}^{T^{(m)}} \left[ 
	\sum_{i=j}^{T^{(m)}} \nabla_\theta \log \pi_\theta(s_i^{(m)}, a_i^{(m)})
	\right] \left(\gamma^j r_{j+1}^{(m)} - b \right)  
\end{equation}
These two estimates are exactly equivalent. This simple trick greatly reduces the
estimate variance and this can speed up convergence. These algorithms can be derived in an alternative way from a more general result: the policy gradient theorem \cite{sutton1999policy} 
\begin{theorem}[Policy Gradient]
	For any differentiable policy $\pi_\theta$, for any of the policy objective
	functions $J = J_{\text{start}}$, $J_{\text{avV}}$, $J_{\text{avR}}$,
	$\frac{1}{1-\gamma} J_{\text{avV}}$, the policy gradient is 
	\begin{equation}
		\nabla_\theta J(\theta) =
		\E[\substack{S \sim d^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
		\pi_\theta(S,A) Q_{\pi_\theta}(S, A)}
	\end{equation}
\end{theorem}
\begin{proof}
	We first prove the result for the average-reward formulation and then for the start state formulation. From the basic relation between state-value function and action-value function, we have
	\begin{equation*}
		\begin{split}
			\nabla_\theta V_\theta(s) &= \nabla_\theta \int_{\A} \pi_\theta(s,a) Q_\theta(s,a) da\\
				&= \int_{\A} \left[ \nabla_\theta \pi_\theta(s,a) Q_\theta(s,a) + \pi_\theta(s,a) \nabla_\theta Q_\theta(s,a)\right] da
		\end{split}
	\end{equation*} 
	Using the Bellman expectation equation for $Q_\theta$ 
	\begin{equation*}
		\begin{split}
			\nabla_\theta Q_\theta(s,a) &= \nabla_\theta \left[ \calR(s,a) - \rho_\theta + \int_{\S} \calP(s,a,s') V_\theta(s') ds' \right]\\
			&= -\nabla_\theta \rho_\theta + \int_{\S} \calP(s,a,s') \nabla_\theta V_\theta(s') ds'
		\end{split}
	\end{equation*}
	Hence, plugging in the first equation 
	\begin{equation*}
		\begin{split}
			\nabla_\theta V_\theta(s) &= \int_{\A} \nabla_\theta \pi_\theta(s,a) Q_\theta(s,a) da - \nabla_\theta \rho_\theta + \int_\A \pi_\theta(s,a) \int_{\S} \calP(s,a,s') \nabla_\theta V_\theta(s') ds' 
		\end{split}
	\end{equation*} 	
	Integrating both sides with respect to the stationary distribution $d^\theta$ and noting that, because of stationarity,  
	\begin{equation*}
		\int_{\S} d^\theta(s) \int_{\A} \pi(s,a) \int_{\S} \calP(s,a,s') \nabla_\theta(s') ds' da ds = \int_{\S} d^\theta(s) \nabla_\theta V_\theta(s) ds
	\end{equation*}
	we obtain the result 
	\begin{equation*}
		\begin{split}
		\nabla_\theta \rho_\theta &= \int_{\S} d^\theta(s) \int_{\A} \nabla_\theta \pi_\theta(s,a) Q_\theta(s,a) da ds\\
		&= \int_{\S} d^\theta(s) \int_{\A} \pi_\theta(s,a) \nabla_\theta \log\pi_\theta(s,a) Q_\theta(s,a) da ds\\
		&= \E[\substack{S \sim d^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
				\pi_\theta(S,A) Q_{\pi_\theta}(S, A)} 
		\end{split}
	\end{equation*}
	Let us now prove the theorem for the start state formulation. The first step is exactly the same. Using the Bellman expectation equation for $Q_\theta$, 
	\begin{equation*}
		\begin{split}
			\nabla_\theta Q_\theta(s,a) &= \nabla_\theta \left[ \calR(s,a) + \gamma \int_{\S} \calP(s,a,s') V_\theta(s') ds' \right]\\ 
			&= \gamma \int_{\S} \calP(s,a,s') \nabla_\theta V_\theta(s') ds'
		\end{split}
	\end{equation*}
	Hence,
	\begin{equation*}
		\begin{split}
			\nabla_\theta V_\theta(s) &= \int_{\A} \left[ \nabla_\theta \pi_\theta(s,a) Q_\theta(s,a) + \gamma \int_{\S} \calP(s,a,s') \nabla_\theta V_\theta(s') ds' \right] da\\
			&= \int_\S  \sum_{k=0}^{\infty} \gamma^k \calP_\theta^{(k)}(s, x) \int_{\A} \nabla_\theta \pi_\theta(x,a) Q_\theta(x,a) da dx 
		\end{split}
	\end{equation*} 
	after unrolling $\nabla_\theta V_\theta$ infinite times and denoting by $\calP_\theta^{(k)}(s, x)$ the probability of going from state $s$ to state $x$ in $k$ steps under policy $\pi_\theta$.
	Defining the $\gamma$-discounted visiting distribution of state $x$ starting from state $s$ as
	\begin{equation*}
		d_\gamma^\theta(s, x) = \sum_{k=0}^{\infty} \gamma^k \calP_\theta^{(k)}(s, x)
	\end{equation*}
	we have the result
	\begin{equation*}
		\begin{split}
			\nabla_\theta V_\theta(s) &= \int_\S d_\gamma^\theta(s, x) \int_{\A} \nabla_\theta \pi_\theta(x,a) Q_\theta(x,a) da dx\\
			&= \E[\substack{S \sim d_\gamma^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
							\pi_\theta(S,A) Q_{\pi_\theta}(S, A)}  
		\end{split}
	\end{equation*} 
	
\end{proof}

The action-value function is typically unknown and needs to
be approximated. For instance, REINFORCE and GPOMDP replace it with the realized return
achieved on a sample trajectory, that are an unbiased estimate of the
action-value function. However, the theorem can be used as the starting point
to derive many other policy gradient methods that use different approximation
of the action-value function.

\subsection{Actor-Critic Policy Gradient}
Actor-Critic Policy Gradient employs a \emph{critic}, that is a parametric
approximation $\widehat{Q}: \S \times \A \times \Psi \to \R$, where $\Psi \in
\R^{D_\psi}$ such that $\widehat{Q}_\psi(s, a) = \widehat{Q}(s, a; \psi)
\approx Q_{\pi_\theta}(s,a)$. Therefore these methods maintain two sets of
parameters: a \emph{critic} that updates the action-value function parameters
$\psi$ and an \emph{actor} that updates the policy parameters $\theta$ in the
direction suggested by the critic. More formally, given $\widehat{Q}_\psi$, the
current state of the system $s_t$ and the action $a_t$ selected using policy
$\pi_\theta$, the policy parameters are updated in the approximated gradient 
direction
\begin{equation}
	\theta_{k+1} = \theta_k + \alpha_k \nabla_\theta \pi_\theta(s_t, a_t) 
	\widehat{Q}_\psi(s_t, a_t) 
\end{equation}
On the other hand, the action-value function parameters $\psi$ can be updated
using any value-based approach, for instance TD(0). This leads to the QAC
algorithm, for which the pseudo code is reported in \ref{algo:QAC}.

[TODO: TD policy gradient,
compatible function approximation, advantage function and actor-critic policy
gradient]

\section{Natural Policy Gradient}

\section{Policy Gradient with Parameter Exploration}
In Monte Carlo Policy Gradient, trajectories are generated by sampling at each
time step an action according to a stochastic policy $\pi_\theta$ and the
objective function gradient is estimated by differentiating the policy with
respect to the parameters. However, sampling an action from the policy at each
time step leads to a large variance in the sampled histories and therefore in 
the gradient estimate, which can in turn slow down the convergence of the
learning process. To address this issue, in \cite{sehnke2008policy} the authors
propose the \emph{policy gradient with parameter-based exploration} (PGPE)
method, in which the search in the policy space is replaced with a direct
search in the model parameter space. We start by presenting the episodic case
and we will later extend this approach to the infinite horizon setting.  

\subsection{Episodic PGPE}
Given an episodic MDP, PGPE considers a determinstic policy $F: \S \times
\Theta \to \A$ that, given a set of parameters $\theta \in \Theta \subseteq
\R^{D_\theta}$, maps a state $s \in \S$ to an action $a = F(s; \theta) =
F_\theta(s) \in \A$. The policy parameters $\theta$ are random variables 
distributed according to a probability distribution parametrized by some 
hyperparameters $\xi \in \Xi \subseteq \R^{D_\xi}$, i.e.  $\theta \sim 
p(\theta; \xi) = p_\xi(\theta)$. Combining these two hypotheses, we obtain a
stochastic policy parametrized by the hyperparameters $\xi$ 
\begin{equation}
	\pi_\xi(s, a) = \pi(s, a; \xi) = \int_\Theta p_\xi(\theta)
	\ind{F_\theta(s) = a} d\theta
\end{equation}
The advantage of this approach is that the policy is deterministic and
therefore the actions do not need to be sampled at each time step, with a
consequent reduction of the gradient estimate variance. Indeed, It is
sufficient to sample the parameters $\theta$ once at the beginning of the
episode and then generate an entire trajectory following the deterministic 
policy $F_\theta$. As an additional benefit, the parameter gradient is
estimated by direct parameter perturbations, without having to backpropagate
any derivatives, which allows to use non-differentiable controllers. The 
hyperparameters $\xi$ will be updated following the gradient of the expected 
reward, which can be rewritten as 
\begin{equation}
	\begin{split}
		J(\xi) &= \int_\Theta \int_\H p_\xi(\theta, h) G(h) dh d\theta\\
	\end{split}
\end{equation}
Hence, using the fact that $h$ is conditionally independent from $\xi$ given
$\theta$, so that $p_\xi(\theta, h) = p_\xi(\theta) p_\theta(h)$, and the 
likelihood trick 
\begin{equation}
	\begin{split}
		\nabla_\xi J(\xi) &= \int_\Theta \int_\H \nabla_\xi p_\xi(\theta) 
						     p_\theta(h) G(h) dh d\theta\\
						  &= \int_\Theta \int_\H p_\xi(\theta) p_\theta(h) 
							 \nabla_\xi \log p_\xi(\theta) G(h) dh d\theta\\
						  &= \E{\nabla_\xi \log p_\xi(\theta) G(H)}
	\end{split}
\end{equation}
Again, we can subtract a constant baseline $b \in \R$ from the total return 
\begin{equation}
	\nabla_\xi J(\xi) = \E{\nabla_\xi \log p_\xi(\theta)\left(G(H) - b\right)}
\end{equation}
the hyperparameters can then be updated by gradient ascent
\begin{equation}
	\xi_{k+1} = \xi_k + \alpha_k \nabla_\xi J(\xi)
\end{equation}
The gradient can be approximated via Monte Carlo by sampling $\theta \sim
p_\xi$ and then generating $M$ trajectories $h^{(m)} = \{(s_t^{(m)},
a_t^{(m)})\}_{t \geq 0}$ following the deterministic policy $F_\theta$ 
\begin{equation}
	g_{\text{PGPE}} = \frac{1}{M} \sum^{M}_{m=1} \nabla_\xi \log p_\xi(\theta) 
	\left[G\left(h^{(m)}\right)-b\right] 
\end{equation}
Alternatively, we can use a fully stochastic approximation by sampling a single
trajectory and approximating the gradient as 
\begin{equation}
	g_{\text{PGPE}} = \nabla_\xi \log p_\xi(\theta) \left[G(h)-b\right] 
\end{equation}
In the following paragraphs, we discuss some possible choices for the policy
parameters distribution $p_\xi$, which is the last component of the algorithm. 

\subsubsection{Gaussian Parameter Distribution}
A simple approach is to assume that all the components of the parameter vector
$\theta$ are independent and normally distributed with mean $\mu_i$ and
variance $\sigma_i^2$, i.e. $\theta_i \sim \mathcal{N}(\mu_i, \sigma_i^2)$, the
gradient with respect to the hyperparameters $\xi = (\mu_1, \ldots, 
\mu_{D_\theta}, \sigma_1, \ldots, \sigma_{D_\theta})^T$ is given by 
\begin{equation}
	\begin{split}
		\pardev{\log p_\xi(\theta)}{\mu_i} &= \frac{\theta_i -
		\mu_i}{\sigma_i^2}\\	
		\pardev{\log p_\xi(\theta)}{\sigma_i} &= \frac{(\theta_i - \mu_i)^2 -
		\sigma_i^2}{\sigma_i^3}
	\end{split}
\end{equation}
Using a constant learning rate $\alpha_i = \alpha \sigma_i^2$, the gradient
updates takes the following form
\begin{equation}
	\begin{split}
		\mu_i^{k+1} &= \mu_i^k + \alpha \left[G(h) - b\right] (\theta_i - \mu_i)\\
		\sigma_i^{k+1} &= \sigma_i^k + \alpha \left[G(h) - b\right] 
		\frac{(\theta_i -\mu_i)^2}{\sigma_i}
	\end{split}
\end{equation}
where $b$ can be computed as a moving average of the past returns. Intuitively,
if $G(h) > b$ we adjust $\xi$ so as to increase the probability of $\theta$
while if $G(h) < b$ we do the opposite. 

\subsubsection{Symmetric Sampling and Gain Normalization} 
In some settings, comparing the gain with a baseline can be misleading. In
their original work, the authors propose a symmetric sampling technique similar
to antithetic variates that further improves the convergence of the method.
More in detail, a more robust gradient estimate can be obtained by measuring
the difference in reward between two symmetric samples on either side of the
current mean. That is, we sample a random perturbation $\epsilon \sim
\mathcal{N}(0, \Sigma)$, where $\Sigma = \text{diag}(\sigma_1^2, \ldots,
\sigma_{D_\theta}^2)$, and we define $\theta^+ = \mu + \epsilon$ and $\theta^- =
\mu - \epsilon$. Denoting by $G^+$ (resp. $G^-$) the gains obtained on the
trajectory associated to $\theta^+$ (resp. $\theta^-$), the objective function
gradient can be approximated with
\begin{equation}
	\begin{split}
		\nabla_{\mu_i} J(\xi) &\approx \frac{\epsilon_i (G^+ - G^-)}{2
		\sigma_i^2}\\
		\nabla_{\sigma_i} J(\xi) &\approx \frac{\epsilon_i^2 -
	\sigma_i^2}{\sigma_i^3}\left(\frac{G^+ + G^-}{2} - b\right)
	\end{split}
\end{equation}
Hence, by choosing $\alpha_i^k = 2 \alpha \sigma_i^2$, we have the following 
update rules
\begin{equation}
	\begin{split}
		\mu_i^{k+1} &= \mu_i^k + \alpha \epsilon_i (G^+ - G^-)\\
		\sigma_i^{k+1} &= \sigma_i^k + \alpha \frac{\epsilon_i^2 - \sigma_i^2}
		{\sigma_i}\left(G^+ + G^- - 2b\right)
	\end{split}
\end{equation}
In addition, the authors propose to normalize the gains in order to make the
updates independent of the scale of the rewards. For instance, we could modify
the hyperparameters updates as follows
\begin{equation}
	\begin{split}
		\mu_i^{k+1} &= \mu_i^k + \alpha \epsilon_i \frac{(G^+ - G^-)}{2m - G^+
	- G^-}\\
		\sigma_i^{k+1} &= \sigma_i^k + \alpha \frac{\epsilon_i^2 - \sigma_i^2}
	{\sigma_i}\frac{\left(G^+ + G^- - 2b\right)}{m - b}
	\end{split}
\end{equation}
where $m$ might be the maximum gain the agent can receive, if known, or
alternatively the maximum gain achieved so far. Symmetric sampling and gain
normalization can drastically improve the gradient estimate quality and 
consequently the convergence time.  

\subsection{Infinite Horizon PGPE}
In this section we discuss the extension of the PGPE method to the infinite
horizon case \cite{sehnke2012parameter}. While in the episodic PGPE the
parameters $\theta$ are sampled only at the beginning of each episode, in
\emph{Infinite Horizon PGPE} (IHPGPE) the parameters and learning are carried 
out simultaneously, while interacting with the environmnent. Let $0 <
\varepsilon < 1$ the probability of updating the policy parameters, the 
parameters $\theta_t$ can be sampled consecutively as follows
\begin{equation}
	p_\xi(\theta_{i,t+1}) = \varepsilon \mathcal{N}(\mu_{i,t}, \sigma_{i,t}^2) 
							+ (1-\varepsilon) \delta_{\theta_{i,t}}
\end{equation}
In practice, $\varepsilon$ should be chosen so that the expected frequency of
changing a single parameter is coherent with the typical episode length in the
episodic framework. Alternatively, one could sample all the parameters at a
certain time step simultaneously
\begin{equation}
	p_\xi(\theta_{t+1}) = \varepsilon \mathcal{N}(\mu_{t}, \Sigma_t) 
							+ (1-\varepsilon) \delta_{\theta_{t}}
\end{equation}
This is equivalent to splitting the state-action space into artificial
episodes. However, updating parameters asynchronously changes the policy only
slightly thus introducing less noise in the process. Again, parameters can be
updated at every time step by gradient ascent
\begin{equation}
	\begin{split}
		\mu_{i,t+1} &= \mu_{i,t} + \alpha \left[G_t(h) - b\right] (\theta_{i,t}
		- \mu_{i,t})\\
		\sigma_{i,t+1} &= \sigma_{i,t} + \alpha \left[G_t(h) - b\right] 
		\frac{(\theta_{i,t} -\mu_{i,t})^2}{\sigma_{i,t}}
	\end{split}
\end{equation}
Similarly to the episodic case, we can improve the gradient estimate by
symmetric sampling and gain normalization. 


\section{Risk-Sensitive Policy Gradient}
Let us consider a family of parametrized policies $\pi_\theta$, with $\theta
\in \Theta \subseteq \R^{D_\theta}$. The optimization problem then becomes
\begin{equation}
	\max_\lambda \min_\theta L(\theta, \lambda) = - \rho(\theta) + \lambda
	(\Lambda(\theta) - \alpha)
\end{equation}
Using a policy gradient approach, the policy parameters are updated following 
the gradient ascent direction
\begin{equation}
	\nabla_\theta L(\theta, \lambda) = - \nabla_\theta \rho(\theta) + \lambda
	\nabla_\theta \Lambda(\theta)
\end{equation}
while the Lagrange multiplier is updated following the gradient descent
direction  
\begin{equation}
	\nabla_\lambda L(\theta, \lambda) = \Lambda(\theta) - \alpha
\end{equation}
Since 
\begin{equation}
	\nabla_\theta \Lambda(\theta) = \nabla_\theta \eta(\theta) - 2 \rho(\theta)
	\nabla_\theta \rho(\theta)
\end{equation}
it is enough to compute $\nabla_\theta \eta(\theta)$ and $\nabla_\theta
\rho(\theta)$. These quantities are provided by the policy gradient theorem 
\begin{theorem}[Policy Gradient]
	\begin{equation}\label{eq:policy_gradient_theorem_Q}
		\begin{split}
			\nabla_\theta \rho(\theta) &= \E[\substack{S\sim d_\pi \\ 
			A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) Q_\theta(S,A)}\\
			&= \int_\S d_\pi(s) \int_\A \pi(s,a) \nabla_\theta \log
			\pi_\theta(s,a) Q_\theta(s,a) da ds	
		\end{split}
	\end{equation}
	\begin{equation}\label{eq:policy_gradient_theorem_W}
		\begin{split}
			\nabla_\theta \eta(\theta) &= \E[\substack{S\sim d_\pi \\ 
			A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) W_\theta(S,A)}\\
			&= \int_\S d_\pi(s) \int_\A \pi(s,a) \nabla_\theta \log
			\pi_\theta(s,a) W_\theta(s,a) da ds	
		\end{split}
	\end{equation}
\end{theorem}
\begin{proof}
	Eq. (\ref{eq:policy_gradient_theorem_Q}) is the standard policy gradient
	and its proof can be found in the literature, e.g. \cite{sutton1999policy}.
	Eq. (\ref{eq:policy_gradient_theorem_W}) can be shown in a similar fashion.
	TODO
\end{proof}
As in the standard risk-neutral case, a state-dependent baseline can be 
introduced in both gradients without changing the result. In particular, by 
using the average adjusted value functions as baseline, we can replace the 
average adjusted action-value functions with the following advantage functions
\begin{equation}
	A_\theta(s,a) = Q_\theta(s,a) - V_\theta(s)
\end{equation}
\begin{equation}
	B_\theta(s,a) = W_\theta(s,a) - U_\theta(s)
\end{equation}
Intuitively, the advantage functions measure how good is to take action $a$ in
state $s$ compared to simply following the policy $\pi_\theta$. The use of a
baseline also serves the purpose of reducing the variance of the gradient
estimates. Thus, the gradients can be written as 
\begin{equation}\label{eq:pg_advantage_Q}
	\nabla_\theta \rho(\theta) = \E[\substack{S\sim d_\pi \\ 
A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) A_\theta(S,A)}
\end{equation}
\begin{equation}\label{eq:pg_advantage_W}
	\nabla_\theta \eta(\theta) = \E[\substack{S\sim d_\pi \\ 
A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) B_\theta(S,A)}
\end{equation}
Results of this type form the basis of many actor-critic algorithms, which
employ an approximation of the advantage function to obtain a more accurate
estimate of the objective function. 

\subsection{Risk-Sensitive Actor-Critic Algorithm}
In \cite{prashanth2014actor}, starting from (\ref{eq:pg_advantage_Q}) and 
(\ref{eq:pg_advantage_W}), the authors derive a risk-sensitive actor-critic 
algorithm for the average reward setting. In the algorithm proposed, the
advantage functions are approximated using a temporal difference (TD) scheme in
which a critic maintains a linear approximation of the value functions. More in
detail, let $\delta_n^A$ and $\delta_n^B$ be the TD errors for residual value
and square value functions
\begin{equation}
	\begin{split}
		\delta_t^A &= R_{t+1} - \widehat{\rho}_{t+1} + \widehat{V}(S_{t+1}) -
		\widehat{V}(S_t)\\
		\delta_t^B &= R_{t+1}^2 - \widehat{\eta}_{t+1} + \widehat{U}(S_{t+1}) -
		\widehat{U}(S_t)\\
	\end{split}
\end{equation}
where $\widehat{V}$, $\widehat{U}$, $\widehat{\rho}$ and $\widehat{\eta}$ are
unbiased estimate of $V_\theta$, $U_\theta$, $\rho(\theta)$ and $\eta(\theta)$
respectively. It is easy to show that $\delta_t^A$ and $\delta_t^B$ are
unbiased estimates of the advantage functions.
\begin{proposition}[Temporal Difference Errors]
	\begin{equation}
		\begin{split}
			\E[\theta]{\delta_t^A|S_t = s, A_t = a} &= A_\theta(s, a)\\
			\E[\theta]{\delta_t^B|S_t = s, A_t = a} &= B_\theta(s, a)\\
		\end{split}
	\end{equation}
\end{proposition}
\begin{proof}
	TODO
\end{proof}
Denoting by $\psi_t = \psi(S_t, A_t) = \nabla_\theta \log \pi(S_t, A_t)$ the
compatible feature \cite{sutton1999policy}, we can easily obtain an unbiased
estimate of the gradients 
\begin{equation}
	\begin{split}
		\nabla_\theta \rho(\theta) &\approx \psi_t \delta_t^A\\
		\nabla_\theta \eta(\theta) &\approx \psi_t \delta_t^B\\
	\end{split}
\end{equation}
The value functions are linearly approximated using some veatures vectors
$\Phi_V: \S \to \R^{D_V}$ and $\Phi_U: \S \to \R^{D_U}$ as follows
\begin{equation}
	\begin{split}
		\widehat{V}(s) &= v^T \Phi_V(s)\\
		\widehat{U}(s) &= u^T \Phi_U(s)\\
	\end{split}
\end{equation}
Given all these ingredients, the authors propose a three time-scale stochastic
approximation algorithm whose pseudo-code is illustrated in Algorithm
%\ref{algo:average_reward_risk_sensitive_actor_critic_algorithm}.

% \begin{algorithm}
	%\caption{Average Reward Risk-Sensitive Actor-Critic Algorithm}
	% \label{algo:average_reward_risk_sensitive_actor_critic_algorithm}
	%\begin{algorithmic}
	%	test		
	%\end{algorithmic}
% \end{algorithm}
