\chapter{Introduction}

The impact of \gls{ATS} on financial markets is growing every year and the trades generated by an algorithm now account for the majority of orders that arrive at stock exchanges. Every year, investments banks and hedge funds invest large amount of resources to develop new systems to maximize their profits and stay ahead in the technological race. The goal of developing an automated system able to take financial decisions without the need for human supervision is not new. From the Technical Computer System developed by Commodities Corporation in the 1970, to the algorithm that caused Knight Capital Group to lose \$ 440 millions in few hours in 2012, many have been the attempts to trade on the markets using an algorithm. Typically, these approaches were based on advanced statistical methods and signal processing. However, the recent successes of \gls{AI}, and in particular \gls{DL}, have attracted the interest of the financial community. Many quantitative researchers asked themselves if the techniques that have proved so successful in classifying images or beating Go world champions could perform equally well on financial markets. In this thesis, we explore how to find a trading strategy via \gls{RL}, a branch of \gls{ML} that allows to find an optimal strategy for a sequential decision problem by directly interacting with the environment. In this introductory chapter, we briefly describe the current state of the financial markets, which are increasingly dominated by algorithms, and provide a quick overview of the recent advances of \gls{AI} and the huge impact that these new techniques are having on our day-to-day life. This chapter thus prepare the stage in which we will move during the entire exposition. 

\section{The Computerization of Finance}


The trading pit of a stock exchange is often imagined by outsiders as a frenzy place, with telephones constantly ringing and traders shouting orders across the room at a frenetic rhythm. This was probably the reality thirty years ago, when open outcry was still the main communication system between pit traders. Since then the floors have become more and more quiet as the majority of the orders moved to electronic trading systems. Notwithstanding, investment decisions were still made by humans who could now execute their orders without passing through the pit traders. In the last decade, the markets have witnessed the widespread adoption of \emph{Automated Trading Systems} (ATS), that can make investment decisions in a fully automatized way at speeds with orders of magnitude greater than any human equivalent. In 2014, more than $75\%$ of the stock shares traded on United States exchanges were originated from ATS orders and this amount kept growing since then. Quantitative hedge funds, such as Renaissance Technologies, D.E. Shaw, Citadel and many others, are employing mathematicians, physicists and other scientists to develop algorithms able to extract trading signals from large amount of data and automatically trade. These algorithms are typically based on advanced statistics, signal processing, machine learning and other fields of mathematics. However, few of these hedge funds publish their profit-generating ``secret sauce'' and not much can be found in the literature. In this project we develop an automated trading algorithm based on \emph{Reinforcement Learning} (RL), a branch of \emph{Machine Learning} (ML) which has recently been in the spotlight for being at the core of the system who beat the Go world champion in a 5-match series \cite{silver2016mastering}.\\
This document is organized as follows. In Section \ref{sec:basics_reinforcement_learning} we introduce the basic concepts of RL and present two learning algorithms that allow two determine an approximation for the optimal policy of a sequential decision problem. In section \ref{sec:application_to_systematic_trading} we discuss the asset allocation problem from a mathematical point of view and show how these learning algorithms can be applied in this setting. In Section \ref{sec:python_prototype} we start discussing the implementation of the model in Python, which has been used during the prototyping phase. In Section \ref{sec:c++_implementation} we discuss a more efficient C++ implementation. In Section \ref{sec:execution_pipeline} we describe the execution pipeline used to run the learning experiment. In Section \ref{sec:numerical_results} we present the numerical results for a synthetic asset, whose price follows a particular stochastic process. In Section \ref{sec:conclusion} we conclude with some final remarks and we discuss some future research directions. 



\section{A Brief History of Artificial Intelligence}

\section{Structure}
This section outlines the structure of this document, explaining the original contributions that were made in this thesis.\\
In Chapter \ref{ch:discrete_time_stochastic_optimal_control} we introduce the basic concepts of discrete-time stochastic optimal control, which is the standard theoretical framework used to model sequential decision problems. In particular, we describe the central feedback mechanism between a learning agent and a stochastic environment. Finally, we present the less traditional risk-sensitive framework for discrete-time stochastic optimal control.\\
In Chapter \ref{ch:reinforcement_learning} we present the main ideas of reinforcement learning, a general class of algorithms in the field of machine learning that allows an agent to learn how to behave in a stochastic and possibly unknown environment only by trial-and-error. We thoroughly discuss the RL problem and its characteristic features. Finally, we give a high-level overview of the different typologies of reinforcement learning algorithms.\\ 
In Chapter \ref{ch:policy_gradient} we give an in-depth presentation of policy gradient algorithms for the risk-neutral control problem. After introducing the key ideas of these methods, we provide a thorough review of the state-of-the art algorithms that can be found in the literature. In particular, we introduce the main result that will play a crucial role in the rest of this thesis, the policy gradient theorem.\\
In Chapter \ref{ch:risk_sensitive_policy_gradient} we discuss policy gradient methods for the risk-sensitive control problem, which is still an active field of research. In particular, we provide an extension of the policy gradient theorem to the risk-sensitive framework, both in the average reward and in the discounted reward formulations. To the best of our knowledge, this is the first time that a risk-sensitive policy gradient theorem is derived for a general discounted Markov decision process.\\
In Chapter \ref{ch:parameter_based_policy_gradient} we propose an original parameter-based version of the policy gradient theorem, both for the risk-neutral and the risk-sensitive formulation. These theorems allow to derive efficient online learning algorithms similar in spirit to the well-known PGPE algorithm, which was originally conceived only for episodic environments. Moreover, these new algorithms can be easily enhanced using a critic or the natural policy gradient idea. This chapter undoubtedly represents the most innovative contribution of this thesis.\\ 
In Chapter \ref{ch:financial_applications_of_reinforcement_learning}, after a brief discussion about why finance represent an extremely challenging field of research, we provide a bibliographical survey of successful applications of RL techniques to financial decision problems. In particular, we focus on the asset allocation problem with transaction cost, which is used as a test case for the numerical application of the learning algorithms proposed in the previous chapters.\\
In Chapter \ref{ch:numerical_results} we present the numerical results for the asset allocation problem. We show that the learning algorithms proposed in the previous chapters perform extremely well on synthetic data. On the other hand, the algorithms encounter more difficulties on historical data and we try to provide an explanation for this behavior.\\
Chapter \ref{ch:conlusions} summarizes the contributions of this thesis to the reinforcement learning literature and presents some interesting ideas for further developments and future work.\\
 


