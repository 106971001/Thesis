\chapter{Financial Applications of Reinforcement Learning}
\label{ch:financial_applications_of_reinforcement_learning}

Both in the academia and in the financial industry there has always been a strong interest in developing automated systems able to take financial decisions in the place of humans. The advent of computers made it possible to analyze the huge amount of data available in the markets and to discover patterns that can be exploited automatically by a program, in what is known as a statistical arbitrage. However, finding these signals and transform them in a profit is not straightforward. In Section  \ref{sec:efficient_market_hypothesis} we briefly discuss the \emph{Efficient Market Hypothesis} (EMH) which states that it is impossible to "beat the market" because stock market efficiency causes existing share prices to always incorporate and reflect all relevant information. However, the success of many quantitative hedge-funds over the years provides strong evidence that real markets are not so efficient as they are often considered in the literature. We thus discuss some weaker versions of market efficiency which should always be kept in mind when trying to develop automated trading systems able to outperform the market. In this chapter, out of the many techniques that can be tested to achieve this goal, we will focus on reinforcement learning. In Section \ref{sec:bibliographical_survey}, we describe the relevant works that can be found in the literature. This presentation will give an overview of the spectrum of financial problems that can be tackled with RL techniques. In Section \ref{sec:asset_allocation_with_transaction_costs}, we present in detail the problem of asset allocation with transaction costs, which will be used for the numerical applications.

\section{Efficient Market Hypothesis}  
\label{sec:efficient_market_hypothesis}

When trying to forecast future returns of speculative assets, one should always keep in mind the \gls{EMH} \cite{timmermann2004efficient}. In its most basic form, the EMH states that asset returns cannot be predicted, otherwise many investors would exploit them to generate unlimited profits. Such a ``money-machine'' producing unlimited wealth is impossible in a stable economy. This hypothesis seems intuitively sensible because as soon as an inconsistency appears in the market, there will be an agent who exploits it and makes the opportunity disappear. Hence, forecasting models ``self-destructs'' in an efficient market which auto-corrects the emerging anomalies. That might appear as the end of the forecasters' ambitions. However this idea is not completely convincing and papers continue to appear attempting to forecast stock returns, usually with very little success. On the other hand, a number of successful stories of quantitative hedge-funds which consistently made profits looking for statistical arbitrages make us doubt the EMH. In the next sections we give more formal definitions of the EMH in its various forms and discuss some of the critics.

\subsection{Formal Definitions of the EMH}
\begin{definition}[Efficient Market Hypothesis]
	A market is efficient with respect to information set $\calF_t$ if it is impossible to make economic profits by trading on the basis of information set $\calF_t$.
\end{definition}
In the previous definition, economic profits stand for risk adjusted returns net of all costs. In other words, financial markets are efficient if they do not allow investors to earn above-average risk-adjusted returns. The EMH is in essence an extension of the zero-profit competitive equilibrium condition from the certainty world of classical price theory to the dynamic behavior of prices in speculative markets under conditions of uncertainty \cite{jensen1978some}. The application of the zero profit condition to speculative markets under the assumption of zero storage costs and zero transactions costs gives us the result that asset prices (after adjustment for required returns) will behave as a martingales with respect to the information set $\{\calF_t\}$.\\
Several versions of the EMH have been proposed in the literature depending on the information set considered.
\begin{definition}[Weak Form of the EMH]
	$\calF_t$ represents only the information contained in the past price history of the market as of time $t$. 
\end{definition}

\begin{definition}[Semi-Strong Form of the EMH]
	$\calF_t$ represents all information publicly available at time $t$. 
\end{definition}

\begin{definition}[Strong Form of the EMH]
	$\calF_t$ represents all information (public and private) known to anyone at time $t$.
\end{definition}
Let us notice that it is not usually asserted that a market is efficient with respect to inside information since this information is not widely accessible and hence cannot be expected to be fully incorporated in the current price. The empirical evidence presented in \cite{malkiel1970efficient} is largely supportive of weak form and semi-strong form efficiency, while \cite{fama1991efficient} reports stronger evidence of predictability in returns based both on lagged values of returns and publicly available information.


\subsection{Critics to the EMH}
The intellectual dominance of the efficient-market revolution has been strongly challenged by economists who stress psychological and behavioral elements of stock-price determination and by econometricians who argue that stock returns are, to a considerable extent, predictable. Still, in \cite{malkiel2003efficient}, the author expresses the following pro-EMH view
\begin{quote}
	What I do not argue is that the market pricing is always perfect. After the fact, we
	know that markets have made egregious mistakes as I think occurred during the recent
	Internet bubble. Nor do I deny that psychological factors influence securities prices. But I am convinced that Benjamin Graham was correct in suggesting that while the stock market in the short run may be a voting mechanism, in the long run it is a weighing mechanism. True value will win out in the end. And before the fact, there is no way in which investors can reliably exploit any anomalies or patterns that might exist. I am skeptical that any of the “predictable patterns” that have been documented in the literature were ever sufficiently robust so as to have created profitable investment opportunities and after they have been discovered and publicized, they will certainly not allow investors to earn excess returns. 
\end{quote}
This seems to be contradicted by the numerous examples of successful quantitative hedge funds that would suggest that it is possible, even if extremely difficult, to identify consistent and profitable patterns in market dynamics. Medallion, the flagship fund of Renaissance Technologies, one of the most successful hedge fund ever, returned 39 percent per year on average between the end of 1989 and 2006. Its founder Jim Simons, a mathematician who gave notable contributions to string theory, a code breaker who worked worked at the Pentagon’s Institute for Defense Analyses during the cold war, a lifelong speculator and entrepreneur, assembled one of the most successful and secretive group of quantitative researchers ever. The following is the only information available on its website\footnote{\url{https://www.rentec.com/}} 
\begin{quote}
Renaissance Technologies LLC is an investment management company dedicated to producing superior returns for its clients and employees by adhering to mathematical and statistical methods
\end{quote}
This does not shed any light on how RenTech achieves its extraordinary results and the fact that almost nobody leaves the firm made its strategies one of the best-kept secret in the world. Some may argue that using successful hedge funds as a counterexample to market efficiency is affected by the well-known \emph{survivorship bias}, that is the logical error of concentrating on the people or things that ``survived'' some process and inadvertently overlooking those that did not because of their lack of visibility. This bias can lead to overly optimistic beliefs because failures are ignored and to the false belief that the successes in a group have some special property, rather than just coincidence. In our opinion, RenTech's success is not a coincidence and perfectly shows how difficult it is to identify and profitably exploit market inefficiencies. For a richer account of this (and other) legendary hedge fund, the reader may refer to \cite{mallaby2010more}.\\
There is another important point that is worth mentioning. It is often argued that there is a ``file drawer'' bias in published studies due to the difficulty associated with publishing empirical studies that find insignificant effects. In studies of market efficiency, a reverse bias may be present. A researcher who genuinely believes he or she has identified a method for predicting the market has little incentive to publish the method in an academic journal and would presumably be tempted to sell it to an investment bank or to an hedge fund.\\

\section{Bibliographical Survey}
\label{sec:bibliographical_survey}

Many financial applications can be represented as sequential decision problems and can be naturally tackled with the reinforcement learning techniques presented in the previous chapters. Consider for instance the process of trading, in which an investor tries to select the assets that will perform the best in the future and will allow him to realize a profit. This activity is well depicted as an online decision problem involving the two critical steps of \emph{market representation} and \emph{optimal action execution}.\\
Financial markets are extremely complex systems, typically characterized by a large degree of non-stationarity, non-linearity and low signal-to-noise ratios. The first challenge is thus how to summarize the financial environment or, put in more statistical terms, how to design powerful features to be used as input of predictive models. The search for good indicators has been extensively studied in quantitative finance and econometrics. In particular, \emph{technical analysis} is a security analysis methodology for forecasting the direction of prices through the study of past market data, primarily price and volume \cite{lo2000foundations}. Whether technical analysis actually works is a matter of controversy. Methods vary greatly, and different technical analysts can sometimes make contradictory predictions from the same data. Many investors claim that they experience positive returns, but academic appraisals often find that it has little predictive power. Another issue is that designing hand-crafted features requires a deep knowledge of the financial markets. A solution to this drawback is offered by \gls{DL}, a branch of machine learning based on a set of algorithms that attempt to model high level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations \cite{bengio2015deep}. In the last years, deep learning has been in the spotlight as its application to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics have produced state-of-the-art results on various tasks. These successes have attracted the interest of the financial community and the literature offers many examples of how deep neural network can be used to predict future prices and invest on these forecasts. For example, the interested reader may refer to \cite{kamijo1990stock}, \cite{saad1998comparative}, \cite{liang2011stock}.\\ 
The second challenge is how to select the best possible action based on this representation of the market. This is not a trivial task, as the action selected by the investor may influence the market and modify the opportunities that will be available to him in the future. A simple example of this situation is the phenomenon of slippage, which consists in a loss due to the difference between the expected price of a trade and the price at which the trade is executed. This is common in limit-order books when there is no sufficient liquidity to complete a market order at the best available price and the order ``walks the book'', obtaining progressively worse prices. Another difficulty is that the investor's actions may impact its profits in a non-trivial way. For example, frequently changing its positions or entering in short positions will lead to large transaction costs which will undermine the performance of even the most accurate predictive trading strategy, such as those based on deep neural networks. Hence, these approaches are typically viable only on long investment horizons as their performances quickly degrade on shorter horizons because of transaction costs. Therefore, an investor should take into account the dynamic behavior of the market and of the returns he obtains. Reinforcement learning appears as the natural approach to determine the optimal strategy to be employed. In the following sections we briefly present some of the financial applications of reinforcement learning that can be found in the literature. This will give us a better feeling for the intrinsic difficulties of the financial markets and for how they can be tackled. 

\subsection{Asset Allocation with Transaction Costs}
One of the first financial applications of reinforcement learning has been the problem of asset allocation with transaction costs. As discussed in the previous section, transaction costs introduce a dependence between the sequence of positions selected by the investor and its future returns. In this setting, predictive trading systems typically perform poorly as they do not explicitly take this mechanism into account. Therefore, a profitable trading system requires prior decisions as input in order to properly take into account the effects of transactions costs, market impact, and taxes. For this reason, in \cite{moody1997optimization} the authors propose to learn a trading strategy that maximizes various performance measures, such as profit, economic utility, the Sharpe ratio and a differential Sharpe ratio, by \gls{RRL} \cite{jaeger2002tutorial}. This consists in a policy gradient algorithm where the investor's policy is recurrent, since it considers the action selected at the previous step as an input. This introduces a recursive dependence on the parameters of the policy and the policy is updated iteratively using standard techniques for recurrent neural networks, such as \gls{BPTT} \cite{werbos1990backpropagation} or \gls{RTRL} \cite{williams1989learning}. The trading strategy learned in this way results profitable both on simulated and historical data. Moreover, the strategy reacts as expected to transaction costs by reducing the reallocation frequency. In \cite{moody2001learning}, the authors extend their previous work by comparing the performance of recurrent reinforcement learning with two standard value-based algorithms: TD-learning and Q-learning. These papers are among the first applications of reinforcement learning to finance to be published. Very similar approaches are presented in \cite{choey1997nonlineareltit}, \cite{chapados2001cost}, \cite{gold2003FX}, \cite{casqueiro2006neuro}, \cite{dempster2006automated} and \cite{li2007short}. In \cite{deng2016deep}, the authors improve the parametric strategy used by the trading system by introducing a deep neural network that uses some fuzzy representation of past returns as input. This approach greatly improves the representation ability of the policy which is able to extract much more complex features from historical data. This learned strategy is shown to be profitable both on historical data and outperforms purely predictive trading systems when transaction costs are considered. The asset allocation problem will be investigated further in the last section and will be used to test the state-of-the-art methods presented in the previous chapters. 

\subsection{Optimal Order Execution in Limit Order Book}
Another problem that has been tackled with reinforcement learning is the \emph{optimal order execution}, which consists in determining how to modify a portfolio from a given starting composition to a specified final composition within a specified period of time so as to maximize the expected revenue of trading (or equivalently minimizing the costs), with a suitable penalty for the uncertainty of revenue (or cost) \cite{almgren2001optimal}. In its simplest form, the problem can be formulated as follows: how to buy (or symmetrically sell) $V$ shares of a given stock withing a time horizon $T$ so as to minimize the total cost payed. The optimal execution problem can be viewed as an important horizontal aspect of quantitative trading, since virtually every strategy's profitability will depend on how well it is implemented.\\
This problem is of particular interest for a brokerage firm which may be asked by a client to execute an order for a certain number of shares before a given deadline. The broker will try to fulfill the client's order so as to maximize its revenue. Let us consider some naive strategies: the broker may decide to buy all the shares required straight away and wait until the deadline. Without considering market impact, the price to pay is known and no uncertainty is involved. However, the broker might obtain a better price before the client's deadline. This strategy involves more risk, since the price could move in the wrong direction and the broker could end up paying more than what it could have by following the previous strategy. If the order is sufficiently large compared to the liquidity available for a stock, market impact cannot be neglected and the broker will obtain progressively worse prices as the order is filled. Therefore, it is common to break large orders into smaller chunks so as to minimize slippage. In practice, the quality of execution is typically measured by the \emph{implementation shortfall}, that is the difference between the decision price and the final execution price (including commissions, taxes, etc.) \cite{johnson2010algorithmic}. It seems natural to model the optimal execution problem in the stochastic optimal control framework: the goal of the broker is to determine the execution schedule that minimize the implementation shortfall. Let us observe that the basic formulation of the problem given above is finite-horizon and therefore the optimal policy will be time-dependent.\\ 
In \cite{nevmyvaka2006reinforcement}, the authors present the first large-scale application of reinforcement learning to the optimal execution problem in an electronic limit order book and show that a custom version of Q-learning results in substantial improvements in performance with respect to other usual execution policies. This approach is extended in \cite{hendricks2014reinforcement}, where the authors propose a hybrid approach which enhances a given analytical solution with attributes from the market microstructure. Using the Almgren-Chriss (AC) model \cite{almgren2001optimal} as a base, for a finite liquidation horizon with discrete trading periods, the algorithm determines the proportion of the AC-suggested trajectory to trade based on prevailing volume/spread attributes.  

\subsection{Smart Order Routing Across Dark Pools}
Reinforcement learning has also been applied to the \gls{SOR} problem. While the optimal execution problem consists in how to split an order across time, in the smart order routing problem one looks for the optimal split of an order across different trading venues. An increasingly popular source of liquidity for traders is represented by \emph{dark pools}, a recent type of stock exchange in which information about outstanding orders is deliberately hidden in order to minimize the market impact of large trades \cite{cartea2015algorithmic}. In a typical dark pool, buyers and sellers submit unpriced orders to buy and sell securities, with the price derived exogenously from another market at a specified time. Upon submitting an order to buy (or sell) $V$ shares, a trader is put in a queue of buyers (or sellers) awaiting transaction. Matching between buyers and sellers occurs in sequential arrival of orders, similar to a lit exchange. However, unlike a lit exchange, no information is provided to traders about how many parties or shares might be available in the pool at any given moment. Thus in a given time period, a submission of $V$ shares results only in a report of how many shares up to $V$ were executed.\\
The SOR problem can be formulated as follows: given $K$ dark pools, what is the optimal way of splitting an order to buy $V$ shares of a stock across these venues so as to maximize the completion rate. In \cite{ganchev2010censored}, the authors propose to determine the optimal allocation using an approach similar to the reinforcement learning techniques used for \emph{multi-armed bandits} \cite{gittins2011multi}. What differentiates the SOR problem from most standard learning settings is that if an order for $V_k$ shares is submitted to venue $k$ and the order is completely filled, we only learn that at venue $k$ the sell capacity is at least $V_k$, not the precise number of shares that could have been bought there. This mechanism is known as censoring in the statistics literature. The good performances of this algorithm demonstrate once again the potential of reinforcement learning in the field of algorithmic trading, where one seeks to optimize properties of a specified trade rather than decide what to trade in the first place. For a more thorough presentation of this application, the reader may also refer to \cite{kearns2013machine} and the references therein.\\

\section{Asset Allocation with Transaction Costs}
\label{sec:asset_allocation_with_transaction_costs}

The asset allocation problem consists in determining how to dynamically invest the available capital in a portfolio of different assets in order to maximize the expected total return or another relevant performance measure. Let us consider a financial market consisting of $I+1$ different stocks that are traded only at discrete times $t \in \{0, 1, 2, \ldots\}$ and denote by ${Z}_t = {(Z_t^0, Z_t^1, \ldots, Z_t^I)}^T$ their prices at time $t$. Typically, $Z_t^0$ refers to a risk-free asset whose dynamic is given by $Z_t^0 = {(1 + X)}^t$ where $X$ is the deterministic risk-free interest rate. The investment process works as follows: at time $t$, the investor observes the state of the market $S_t$, consisting for example of the past asset prices and other relevant economic variables, and subsequently chooses how to rebalance his portfolio, by specifying the units of each stock ${n}_t = {(n_t^0 , n_t^1 , \ldots , n_t^I)}^T$ to be held between $t$ and $t+1$. In doing so, he needs to take into account the transaction costs that he has to pay to the broker to change his position.  At time $t+1$, the investor realizes a profit or a loss from his investment due to the stochastic variation of the stock values. The investor's goal is to maximize a given performance measure.

\subsection{Wealth Dynamics}
Let $W_t$ denote the wealth of the investor at time $t$. The profit realized between $t$ and $t+1$ is simply given by the difference between the trading results and the transaction costs payed to the broker. More formally
\begin{equation*}
	\Delta W_{t+1} = W_{t+1} - W_t = \text{TP}_{t+1} - \text{TC}_{t}	
\end{equation*}
where $\text{TP}_{t+1}$ denotes the profit due to the variation of the asset prices between $t$ and $t+1$
\begin{equation*}
	\text{TP}_{t+1} = {n}_t \cdot \Delta{Z}_{t+1} = \sum^{I}_{i=0} 
	n_t^i (Z_{t+1}^i - Z_t^i) 
\end{equation*}
and $\text{TC}_t$ denotes the fees payed to the broker to change the portfolio allocation and on the short positions
\begin{equation*}
	\text{TC}_t = \sum^{I}_{i=0} \delta_p^i \left| n_t^i - n_{t-1}^i\right| Z_t^i 
				- \delta_f W_t \ind{{n}_t \neq {n}_{t-1}} 
				- \sum^{I}_{i=0} \delta_s^i {(n_t^i)}^{-} Z_t^i
\end{equation*}
The transaction costs consist of three different components. The first term represents a transaction cost that is proportional to the change in value of the position in each asset. The second term is a fixed fraction of the total value of the portfolio which is payed only if the allocation is changed. The last term represents the fees payed to the broker for the shares borrowed to build a short position. The portfolio return between $t$ and $t+1$ is thus given by
\begin{equation}\label{eq:portfolio_return}
	X_{t+1} = \frac{\Delta W_{t+1}}{W_t} = \sum^{I}_{i=0} \left[ a_t^i
	X_{t+1}^i - \delta_i \left| a_t^i - \tilde{a}_t^i \right| - \delta_s
	{(a_t^i)}^- \right] - \delta_f \ind{{a}_t \neq \tilde{{a}}_{t-1}}  
\end{equation}
where 
\begin{equation*}
	X_{t+1}^i = \frac{\Delta Z_{t+1}^i}{Z_t^i}
\end{equation*}
is the return of the $i$-th stock between $t$ and $t+1$, 
\begin{equation*}
	a_t^i = \frac{n_t^i Z_t^i}{W_t}
\end{equation*}
is the fraction of wealth invested in the $i$-th stock between time $t$ and
$t+1$ and finally 
\begin{equation*}
	\tilde{a}_t^i = \frac{n_{t-1}^i Z_t^i}{W_t} = \frac{a_{t-1}^i (1+X_t^i)}
	{1 + X_t}
\end{equation*}
is the fraction of wealth invested in the $i$-th stock just before the 
reallocation. We assume that the agents invests all the available wealth at each step, so that $W_t$ can be also interpreted as the value of the portfolio. This assumption leads to the following constraint on the portfolio weights
\begin{equation}
	\sum^{I}_{i=0} a_t^i = 1 \;\;\;\;\; \forall t \in \{0, 1, 2, \ldots\}
\end{equation}
We notice that we are neglecting the typical margin requirements on the short
positions, which would reduce the available capital at time $t$. Considering
margin requirements would lead to a more complex constraint on the portfolio
weights which would be difficult to treat in the reinforcement learning
framework. Plugging this constraint into Eq. (\ref{eq:portfolio_return}), we
obtain
\begin{equation}\label{eq:portfolio_return_benchmark}
	X_{t+1} = X + \sum^{I}_{i=1} a_t^i (X_{t+1}^i - X) - \sum^{I}_{i=0}
	\left[\delta_i \left| a_t^i - \tilde{a}_t^i \right| - \delta_s^i
	{(a_t^i)}^-\right] - \delta_f \ind{{a}_t \neq \tilde{{a}}_{t-1}}   
\end{equation}
which highlights the role of the risk-free asset as a benchmark for the 
portfolio returns. The total profit realized by the investor between $t=0$ and
$T$ is 
\begin{equation*}
	\Pi_T = W_T - W_0 = \sum^{T}_{t=1} \Delta W_t = \sum^{T}_{t=1} W_t X_t  
\end{equation*}
The portfolio return between $t=0$ and $T$ is given by
\begin{equation*}
	X_{0,T} = \frac{W_T}{W_0} - 1 = \prod_{t=1}^T (1+X_t) - 1
\end{equation*}
In order to cast the asset allocation problem in the reinforcement learning
framework, we consider the log-return of the portfolio between $t=0$ and $T$
\begin{equation}
	R_{0,T} = \log \frac{W_T}{W_0} = \sum^{T}_{t=1} \log(1+X_t) = \sum_{t=1}^T
	R_t
\end{equation}
where $R_{t+1}$ is the log-return of the portfolio between $t$ and $t+1$
\begin{equation}
	\label{eq:portfolio_daily_logreturn}
	R_{t+1} = \log \left\{ 1 + \sum^{I}_{i=0} \left[ a_t^i X_{t+1}^i - \delta_i
	\left| a_t^i - \tilde{a}_t^i \right| - \delta_s {(a_t^i)}^- \right] -
	\delta_f \ind{{a}_t \neq \tilde{{a}}_{t-1}}\right\}
\end{equation}
The portfolio log-return can be directly used as the reward function of the asset allocation problem. However, more sophisticated approaches can be followed.

\subsection{Rewards and Objective Functions}
The results of the previous sections need to be adapted to the typical reinforcement learning formulation. Two distinct routes can be followed: first the asset allocation problem is seen as an infinite horizon task, either in the discounted or in the average reward formulation. Second, an episodic formulation is proposed. As discussed in the previous chapters, these scenarios require different learning algorithms. 

\subsubsection{Infinite Horizon Task}
The portfolio log-return between $t=0$ and a maturity $T$ is given by
\begin{equation*}
	R_{0,T} = \sum_{t=0}^{T-1} R_{t+1}
\end{equation*}
where $R_{t+1}$ is given in Eq. (\ref{eq:portfolio_daily_logreturn}). The maturity $T$ is arbitrary and one may be tempted to let $T \to \infty$. In addition to posing some convergence issues, this would be rather unrealistic as traders are typically evaluated on the short-run. As discussed in Chapter \ref{ch:discrete_time_stochastic_optimal_control}, the typical solution in this situation is to consider the discounted return
\begin{equation*}
	G_\gamma = \sum_{t=0}^{T-1} \gamma^t R_{t+1}
\end{equation*}
where $0 < \gamma < 1$ is the discount factor. This choice leads to the classical formulation for infinite horizon optimal control problems, which can be addressed in a fairly standard way using value-based or policy-based reinforcement learning algorithms. However, as discussed in the previous chapters, for the discounted rewards formulation it is difficult to extend the policy gradient theorem to the risk-sensitive framework. On the other hand, this is easier for the average reward formulation. In this case, the gain is measured as 
\begin{equation*}
	G_\text{avg} = \lim_{T\to\infty} \frac{1}{T} \sum_{t=0}^{T-1} R_{t+1} 
\end{equation*} 
The relevant risk-neutral and risk-sensitive performance measures can be defined as in the previous chapters.\\

\subsubsection{Episodic Task}
Both formulations discussed above treat the asset allocation problem on an infinite horizon. On the other hand, it would be interesting to formulate this problem as an episodic task, for which many policy gradient algorithms are available as discussed in  in Chapter \ref{ch:policy_gradient}. To achieve this, we can follow the idea of \cite{browne1995optimal}.\\
Given an initial capital $W_0$ and two constants $L < 1 < U$, the goal of the trader is to increase his wealth to $U W_0$, before going down to $L W_0$. Let us reformulate this goal in terms of log-returns. First, let $l = \log L$ and $u = \log U$. We introduce the first time when the portfolio log-return breaches these thresholds 
\begin{equation}
	\tau_u = \inf\{t > 0: R_{0,t} \geq u\}
\end{equation}
\begin{equation}
	\tau_l = \inf\{t > 0: R_{0,t} \leq l\}
\end{equation}
Hence, the goal of the agent can be formalized as follows 
\begin{equation}
	\max_\pi \P[\pi]{\tau_u < \tau_l} = \E[\pi]{\ind{\tau_u < \tau_l}}
\end{equation}
This control problem can be interpreted as an episodic task in which the agent receives a reward either $1$ if $R_{0,t} > u$ or $0$ if $R_{0,t} < l$ and then terminates. This mechanism is illustrated in Figure \ref{fig:episodic_asset_alloc}. The upper cumulative log-return hits the upper barrier before hitting the lower barrier and consequently the agent receives a reward of $1$. On the other hand, the lower path hits the lower barrier before and the agent thus receives zero reward.\\
This formulation may appear simple, but it offers a nice practical interpretation. The lower barrier can be seen as a stop-loss order, which forces the trader to exit from his position when the losses exceed a given threshold. We remark that the reward does not depend on the the time step at which one of the two barrier is achieved. This could lead to extremely conservative policies which maximize the probability of success taking however a long time to achieve it. To attenuate this issue we might introduce a discount factor to represent the trader's preference to succeed as soon as possible. Formally, the optimization problem becomes  
\begin{equation}
	\max_\pi \E[\pi]{\gamma^{\tau_u} \ind{\tau_u < \tau_l}}
\end{equation}
where $0 < \gamma < 1$ denotes as usual the discount factor. 

\begin{figure}[t]
\centering 
\begin{tikzpicture}
	\draw[help lines, step=1.0, color=gray!30, dashed] (-5,-3) grid (5,3);
	\draw[->,ultra thick] (-5,0)--(5,0) node[right]{$t$};
	\draw[->,ultra thick] (-5,-3)--(-5,3) node[above]{$R_{0,t}$};
	\draw[GloriousGreen, ultra thick, name path=upper] (-5, 2) node[left]{$u$} -- (5, 2) ; 
	\draw[RealRed, ultra thick, name path=lower] (-5, -2) node[left]{$l$} --(5, -2);  
	\draw[SteelBlue, ultra thick, name path=rw1] (-5, 0) -- (-4, 0.7) -- (-3, 0.2) -- (-2, 1.1) -- (-1, 0.2) -- (0, 1.0) -- (1, 2.5);
	\draw[dashed, ultra thick] (1,2.5) node[above]{$1$} -- (1,0) node[below]{$\tau_u$};
	\draw[SteelBlue, ultra thick, name path=rw2] (-5, 0) -- (-4, -0.2) -- (-3, -0.4) -- (-2, -1.1) -- (-1, -0.4) -- (0, -1.2) -- (1, -0.5) -- (2, -0.6) -- (3, -0.9) -- (4, -2.7);
	\draw[dashed, thick] (4,-2.7) node[below]{$0$} -- (4,0) node[above]{$\tau_l$};
	\fill[GloriousGreen, name intersections={of=rw1 and upper}] (intersection-1) circle (3pt);
	\fill[RealRed, name intersections={of=rw2 and lower}] (intersection-1) circle (3pt);
\end{tikzpicture}
\caption{Episodic formulation for the asset allocation problem.}
\label{fig:episodic_asset_alloc}
\end{figure}

\subsection{States}
At each time step, the agent observes the state of the system and subsequently selects an action. First, we consider the $P+1$ past returns of all risky assets $\{X_t, X_{t-1}, \ldots, X_{t-P}\}$. These input variables may be used to construct more complex features for example using some deep learning techniques, such as a deep auto-encoder.\\
In order to properly incorporate the effects of transaction costs into his decision process, the agent must keep track of its current position $\tilde{a}_t$. For simplicity, we consider the past action as frozen and we will neglect the dependence from the policy, which will lead to the recurrent formulation proposed in \cite{moody1997optimization}.\\
Finally, we might consider some external variables $Y_t$ that may be relevant to the trader, such as the common technical indicator used in practice. Summing up, the state of the system is given by
\begin{equation}
	S_t = \{X_t, X_{t-1}, \ldots, X_{t-P}, \tilde{a}_t, Y_t, Y_{t-1}, \ldots,
	Y_{t-P}\}
\end{equation}
In the episodic formulation, we might also include some feature measuring the distance of the cumulative log-return from the barriers
\begin{equation}
	Y_t^u = u - R_{0,t} \;\;\;\;\;\; Y_t^d = R_{0,t} - d
\end{equation}

\subsection{Actions}
The agent, or trading system, only specifies the portfolio weights $a_t = (a_t^0,
\ldots, a_t^I)^T$ and therefore determines the allocation for the time interval
$[t, t+1)$. If we assume that the agent invests all the available capital at each time
step and that short-selling is not allowed, then the portfolio weights should
satisfy, for every $t \in \{0, 1, \ldots\}$, the following constraints
\begin{equation}
	\begin{cases}
		a_t^i \geq 0 \;,\; \forall i \in \{0, \ldots, I\}\\
		\sum^{I}_{i=0} a_t^i = 1 
	\end{cases}
\end{equation}
This constraint can be easily enforced by considering a parametric softmax 
policy. If short-selling is allowed, weights might also be negative. A simple approach is to assume that, for every $t \in \{0, 1, \ldots\}$, the weights satisfy
\begin{equation}
	\begin{cases}
		a_t^i \in \R \;,\; \forall i \in \{1, \ldots, I\}\\
		a_t^0 = 1 - \sum^{I}_{i=1} a_t^i
	\end{cases}
\end{equation}
Since $a_t^0$ is uniquely determined by the other weights, it is enough to
define a policy that specifies the allocation in the risky assets, for example a
Gaussian policy in $\R^I$. The obvious shortcoming of this approach is that
the agent might enter in huge short positions, which is not realistic. A first
observation is that, if the stochastic policy is concentrated around the
origin, huge long or short positions would have very small probabilities of 
being selected. Moreover, we notice that the agent would pay large fees for 
entering into large short positions, independently of the trading profits. 
Therefore, we expect the agent to learn that short positions are very expensive 
and would therefore try to avoid them.\\
Working in a continuous action space is computationally difficult and only few
reinforcement learning algorithm are well-suited to this setting, for instance policy
gradient methods. A simpler approach is to reduce the action space to a discrete 
space. For instance, in the two assets scenario we might assume that $a_t \in
\{-1, 0, +1\}$. Thus the agent may be long ($+1$), neutral ($0$) or short
($-1$) on the risky-asset. Working in a discrete action space is more simple,
and standard value-based approaches might also be employed. 
