\chapter{Financial Applications of Reinforcement Learning}
\label{ch:financial_applications_of_reinforcement_learning}

Both in the academia and in the financial industry there has always been a strong interest in developing automated systems able to take financial decisions in the place of humans. The advent of computers made it possible to analyze the huge amount of data available in the markets and to discover patterns that can be exploited automatically by a program, in what is known as a statistical arbitrage. However, finding these signals and transform them in a profit is not straightforward. In Section  \ref{sec:efficient_market_hypothesis} we briefly discuss the \emph{Efficient Market Hypothesis} (EMH) which states that it is impossible to "beat the market" because stock market efficiency causes existing share prices to always incorporate and reflect all relevant information. However, the success of many quantitative hedge-funds over the years provides strong evidence that real markets are not so efficient as they are often considered in the literature. We thus discuss some weaker versions of market efficiency which should always be kept in mind when trying to develop automated trading systems able to outperform the market. In this chapter, out of the many techniques that can be tested to achieve this goal, we will focus on reinforcement learning. In Section \ref{sec:bibliographical_survey}, we describe the relevant works that can be found in the literature. This presentation will give an overview of the spectrum of financial problems that can be tackled with RL techniques. In Section \ref{sec:asset_allocation_with_transaction_costs}, we present in more detail the problem of asset allocation with transaction costs, which will be used for the numerical applications.

\section{Efficient Market Hypothesis}  
\label{sec:efficient_market_hypothesis}

When trying to forecast future returns of speculative assets, one should always keep in mind the \gls{EMH} \cite{timmermann2004efficient}. In its most basic form, the EMH states that asset returns cannot be predicted, otherwise many investors would exploit them to generate unlimited profits. Such a ``money-machine'' producing unlimited wealth is impossible in a stable economy. This hypothesis seems intuitively sensible because as soon as an inconsistency appears in the market, there will be an agent who exploits it and makes the opportunity disappear. Hence, forecasting models ``self-destructs'' in an efficient market which auto-corrects the emerging anomalies. Intellectually, that might appear as the end of the forecasters' ambitions. However this idea is not completely convincing and papers continue to appear attempting to forecast stock returns, usually with very little success. On the other hand, a number of successful stories of quantitative hedge-funds which consistently made profits looking for statistical arbitrages make us doubt the EMH. In the next sections we give more formal definitions of the EMH in its various forms and discuss some of the critics.

\subsection{Formal Definitions of the EMH}
\begin{definition}[Efficient Market Hypothesis]
	A market is efficient with respect to information set $\calF_t$ if it is impossible to make economic profits by trading on the basis of information set $\calF_t$.
\end{definition}
where by economic profits we mean the risk adjusted returns net of all costs. In other words, financial markets are efficient if they do not allow investors to earn above-average risk-adjusted returns. The EMH is in essence an extension of the zero-profit competitive equilibrium condition from the certainty world of classical price theory to the dynamic behavior of prices in speculative markets under conditions of uncertainty \cite{jensen1978some}. The application of the zero profit condition to speculative markets under the assumption of zero storage costs and zero transactions costs gives us the result that asset prices (after adjustment for required returns) will behave as a martingales with respect to the information set $\{\calF_t\}$.\\
Several versions of the EMH have been proposed in the literature depending on the information set considered.
\begin{definition}[Weak Form of the EMH]
	$\calF_t$ represents only the information contained in the past price history of the market as of time $t$. 
\end{definition}

\begin{definition}[Semi-Strong Form of the EMH]
	$\calF_t$ represents all information publicly available at time $t$. 
\end{definition}

\begin{definition}[Strong Form of the EMH]
	$\calF_t$ represents all information (public and private) known to anyone at time $t$	
\end{definition}
Let us notice that it is not usually asserted that a market is efficient with respect to inside information since this information is not widely accessible and hence cannot be expected to be fully incorporated in the current price. The empirical evidence presented in \cite{malkiel1970efficient} is largely supportive of weak form and semi-strong form efficiency, while \cite{fama1991efficient} reports stronger evidence of predictability in returns based both on lagged values of returns and publicly available information.


\subsection{Critics to the EMH}
The intellectual dominance of the efficient-market revolution has been strongly challenged by economists who stress psychological and behavioral elements of stock-price determination and by econometricians who argue that stock returns are, to a considerable extent, predictable. Still, in \cite{malkiel2003efficient}, the author expresses the following pro-EMH view
\begin{quote}
	What I do not argue is that the market pricing is always perfect. After the fact, we
	know that markets have made egregious mistakes as I think occurred during the recent
	Internet bubble. Nor do I deny that psychological factors influence securities prices. But I am convinced that Benjamin Graham was correct in suggesting that while the stock market in the short run may be a voting mechanism, in the long run it is a weighing mechanism. True value will win out in the end. And before the fact, there is no way in which investors can reliably exploit any anomalies or patterns that might exist. I am skeptical that any of the “predictable patterns” that have been documented in the literature were ever sufficiently robust so as to have created profitable investment opportunities and after they have been discovered and publicized, they will certainly not allow investors to earn excess returns. 
\end{quote}
This seems to be contradicted by the numerous examples of successful quantitative hedge funds that would suggest that it is possible, even if extremely difficult, to identify consistent and profitable patterns in market dynamics. Medallion, the flagship fund of Renaissance Technologies, one of the most successful hedge fund ever, returned 39 percent per year on average between the end of 1989 and 2006. Its founder Jim Simons, a mathematician who gave notable contributions to string theory, a code breaker who worked worked at the Pentagon’s Institute for Defense Analyses during the cold war, a lifelong speculator and entrepreneur, assembled one of the most successful and secretive group of quantitative researchers ever. The following is the only information available on its website\footnote{\url{https://www.rentec.com/}} 
\begin{quote}
Renaissance Technologies LLC is an investment management company dedicated to producing superior returns for its clients and employees by adhering to mathematical and statistical methods
\end{quote}
This doesn't shed any light on how RenTech achieves its extraordinary results and the fact that almost nobody leaves the firm made its strategies one of the best-kept secret in the world. Some may argue that using successful hedge funds as a counterexample to market efficiency is affected by the well-known \emph{survivorship bias}, that is the logical error of concentrating on the people or things that ``survived'' some process and inadvertently overlooking those that did not because of their lack of visibility. This bias can lead to overly optimistic beliefs because failures are ignored and to the false belief that the successes in a group have some special property, rather than just coincidence. In our opinion, RenTech's success is not a coincidence and perfectly shows how difficult it is to identify and profitably exploit market inefficiencies. For a richer account of this (and other) legendary hedge fund, the reader may refer to \cite{mallaby2010more}.\\
It is often argued that there is a ``file drawer'' bias in published studies due to the difficulty associated with publishing empirical studies that find insignificant effects. In studies of market efficiency, a reverse bias may be present. A researcher who genuinely believes he or she has identified a method for predicting the market has little incentive to publish the method in an academic journal and would presumably be tempted to sell it to an investment bank or to an hedge fund.\\

\section{Bibliographical Survey}
\label{sec:bibliographical_survey}

Many financial applications can be represented as sequential decision problems and can be naturally tackled with the reinforcement learning techniques presented in the previous chapters. Consider for instance the process of trading, in which an investor tries to select the assets that will perform the best in the future and will allow him to realize a profit. This activity is well depicted ad an online decision problem involving the two critical steps of \emph{market representation} and \emph{optimal action execution}.\\
Financial markets are extremely complex systems, typically characterized by a large degree of non-stationarity, non-linearity and low signal-to-noise ratios. The first challenge is thus how to summarize the financial environment or, put in more statistical terms, how to design powerful features to be used as input of predictive models. The search for good indicators has been extensively studied in quantitative finance and econometrics. In particular, \emph{technical analysis} is a security analysis methodology for forecasting the direction of prices through the study of past market data, primarily price and volume \cite{lo2000foundations}. Whether technical analysis actually works is a matter of controversy. Methods vary greatly, and different technical analysts can sometimes make contradictory predictions from the same data. Many investors claim that they experience positive returns, but academic appraisals often find that it has little predictive power. Another issue is that designing hand-crafted features requires a deep knowledge of the financial markets. A solution to this drawback is offered by \gls{DL}, a branch of machine learning based on a set of algorithms that attempt to model high level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations \cite{Goodfellow-et-al-2016-Book}. In the last years, deep learning has been in the spotlight as its application to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics have produced state-of-the-art results on various tasks. These successes have attracted the interest of the financial community and the literature offers many examples of how deep neural network can be used to predict future prices \cite{kamijo1990stock}, \cite{saad1998comparative}, \cite{liang2011stock} and invest on these forecasts.\\ 
The second challenge is how to select the best possible action based on this representation of the market. This is not a trivial task, as the action selected by the investor may influence the market and modify the opportunities that will be available to him in the future. A simple example of this situation is the phenomenon of slippage, which consists in a loss due to the difference between the expected price of a trade and the price at which the trade is executed. This is common in limit-order books when there is no sufficient liquidity to complete a market order at the best available price and the order ``walks the book'', obtaining progressively worse prices. Another difficulty is that the investor's actions may impact its profits in a non-trivial way. For example, frequently changing its positions or entering in short positions will lead to large transaction costs which will undermine the performance of even the most accurate predictive trading strategy, such as those based on deep neural networks. Hence, these approaches are typically viable only on long investment horizons as their performances quickly degrade on shorter horizons because of transaction costs. Therefore, an investor should take into account the dynamic behavior of the market and of the returns he obtains. Reinforcement learning appears as the natural approach to determine the optimal strategy to be employed. In the following sections we briefly present some of the financial applications of reinforcement learning that can be found in the literature. This will give us a better feeling for the intrinsic difficulties of the financial markets and for how they can be tackled. 

\subsection{Asset Allocation with Transaction Costs}
One of the first financial applications of reinforcement learning has been the problem of asset allocation with transaction costs. As discussed in the previous section, transaction costs introduce a dependence between the sequence of positions selected by the investor and its future returns. In this setting, predictive trading systems typically perform poorly as they do not explicitly take this mechanism into account. Therefore, a profitable trading system requires prior decisions as input in order to properly take into account the effects of transactions costs, market impact, and taxes. For this reason, in \cite{moody1997optimization} the authors propose to learn a trading strategy that maximizes various performance measures, such as profit, economic utility, the Sharpe ratio and a differential Sharpe ratio, by \gls{RRL} \cite{jaeger2002tutorial}. This consists in a policy gradient algorithm where the investor's policy is recurrent, since it considers the action selected at the previous step as an input. This introduces a recursive dependence on the parameters of the policy and the policy is updated iteratively using standard techniques for recurrent neural networks, such as \gls{BPTT} \cite{werbos1990backpropagation} or \gls{RTRL} \cite{williams1989learning}. The trading strategy learned in this way results profitable both on simulated and historical data. Moreover, the strategy reacts as expected to transaction costs by reducing the reallocation frequency. In \cite{moody2001learning}, the authors extend their previous work by comparing the performance of recurrent reinforcement learning with two standard value-based algorithms: TD-learning and Q-learning. These papers are among the first applications of reinforcement learning to finance to be published. Very similar approaches are presented in \cite{choey1997nonlineareltit}, \cite{chapados2001cost}, \cite{gold2003FX}, \cite{casqueiro2006neuro}, \cite{dempster2006automated} and \cite{li2007short}. In \cite{deng2016deep}, the authors improve the parametric strategy used used by the trading system by introducing a deep neural network that uses some fuzzy representation of past returns as input. This approach greatly improves the representation ability of the policy which is able to extract much more complex features from historical data. This learned strategy is shown to be profitable both on historical data and outperforms purely predictive trading systems when transaction costs are considered. The asset allocation problem will be investigated further in the last section and will be used test the state-of-the-art methods presented in the previous chapters. 

\subsection{Optimal Order Execution in Limit Order Book}
Another problem that has been tackled with reinforcement learning is the \emph{optimal order execution}, which consists in determining how to modify a portfolio from a given starting composition to a specified final composition within a specified period of time so as to maximize the expected revenue of trading (or equivalently minimizing the costs), with a suitable penalty for the uncertainty of revenue (or cost) \cite{almgren2001optimal}. In its simplest form, the problem can be formulated as follows: how to buy (or symmetrically sell) $V$ shares of a given stock withing a time horizon $T$ so as to minimize the total cost payed. The optimal execution problem can be viewed as an important horizontal aspect of quantitative trading, since virtually every strategy's profitability will depend on how well it is implemented.\\
This problem is of particular interest for a brokerage firm which may be asked by a client to execute an order for a certain number of shares before a given deadline. The broker will try to fulfill the client's order so as to maximize its revenue. Let us consider some naive strategies: the broker may decide to buy all the shares required straight away and wait until the deadline. Without considering market impact, the price to pay is known and no uncertainty is involved. However, the broker might obtain a better price before the client's deadline. This strategy involves more risk, since the price could move in the wrong direction and the broker could end up paying more than what it could have by following the previous strategy. If the order is sufficiently large compared to the liquidity available for a stock, market impact cannot be neglected and the broker will obtain progressively worse prices as the order is filled. Therefore, it is common to break large orders into smaller chunks so as to minimize slippage. In practice, the quality of execution is typically measured by the \emph{implementation shortfall}, that is the difference between the decision price and the final execution price (including commissions, taxes, etc.) \cite{johnson2010algorithmic}. It seems natural to model the optimal execution problem in the stochastic optimal control framework: the goal of the broker is to determine the execution schedule that minimize the implementation shortfall. Let us observe that the basic formulation of the problem given above is finite-horizon and therefore the optimal policy will be time-dependent.\\ 
In \cite{nevmyvaka2006reinforcement}, the authors present the first large-scale application of reinforcement learning to the optimal execution problem in an electronic limit order book and show that a custom version of Q-learning results in substantial improvements in performance with respect to other usual execution policies. This approach is extended in \cite{hendricks2014reinforcement}, where the authors propose a hybrid approach which enhances a given analytical solution with attributes from the market microstructure. Using the Almgren-Chriss (AC) model \cite{almgren2001optimal} as a base, for a finite liquidation horizon with discrete trading periods, the algorithm determines the proportion of the AC-suggested trajectory to trade based on prevailing volume/spread attributes.  

\subsection{Smart Order Routing Across Dark Pools}
Reinforcement learning has also been applied to the \gls{SOR} problem. While the optimal execution problem consists in how to split an order across time, in the smart order routing problem one looks for the optimal split of an order across different trading venues. An increasingly popular source of liquidity for traders is represented by \emph{dark pools}, a recent type of stock exchange in which information about outstanding orders is deliberately hidden in order to minimize the market impact of large trades \cite{cartea2015algorithmic}. In a typical dark pool, buyers and sellers submit unpriced orders to buy and sell securities, with the price derived exogenously from another market at a specified time. Upon submitting an order to buy (or sell) $V$ shares, a trader is put in a queue of buyers (or sellers) awaiting transaction. Matching between buyers and sellers occurs in sequential arrival of orders, similar to a lit exchange. However, unlike a lit exchange, no information is provided to traders about how many parties or shares might be available in the pool at any given moment. Thus in a given time period, a submission of $V$ shares results only in a report of how many shares up to $V$ were executed.\\
The SOR problem can be formulated as follows: given $K$ dark pools, what is the optimal way of splitting an order to buy $V$ shares of a stock across these venues so as to maximize the completion rate. In \cite{ganchev2010censored}, the authors propose to determine the optimal allocation using an approach similar to the reinforcement learning techniques used for \emph{multi-armed bandits} \cite{gittins2011multi}. What differentiates the SOR problem from most standard learning settings is that if an order for $V_k$ shares is submitted to venue $k$ and the order is completely filled, we only learn that at venue $k$ the sell capacity is at least $V_k$, not the precise number of shares that could have been bought there. This mechanism is known as censoring in the statistics literature. The good performances of this algorithm demonstrate once again the potential of reinforcement learning in the field of algorithmic trading, where one seeks to optimize properties of a specified trade rather than decide what to trade in the first place. For a more thorough presentation of this application, the reader may also refer to \cite{kearns2013machine} and the references therein.\\
The last sections gave an overview of the financial applications of reinforcement learning that can be found in the literature, highlighting the challenges of this domain. In the next section, we develop in the detail the asset allocation problem which will be used for the successive numerical applications.


\section{Asset Allocation with Transaction Costs}
\label{sec:asset_allocation_with_transaction_costs}

The asset allocation problem consists of determining how to dynamically invest
the available capital in a portfolio of different assets in order to maximize
the expected total return or another relevant performance measure. Let us
consider a financial market consisting of $I+1$ different stocks that are
traded only at discrete times $t \in \{0, 1, 2, \ldots\}$ and denote by
${Z}_t = {(Z_t^0, Z_t^1, \ldots, Z_t^I)}^T$ their prices at time $t$.
Typically, $Z_t^0$ refers to a riskless asset whose dynamic is given by $Z_t^0
= {(1 + X)}^t$ where $X$ is the deterministic risk-free interest rate. The
investment process works as follows: at time $t$, the investor observes the
state of the market $S_t$, consisting for example of the past asset prices and
other relevant economic variables, and subsequently chooses how to rebalance
his portfolio, by specifying the units of each stock ${n}_t = {(n_t^0 ,
n_t^1 , \ldots , n_t^I)}^T$ to be held between $t$ and $t+1$. In doing so, he
needs to take into account the transaction costs that he has to pay to the
broker to change his position.  At time $t+1$, the investor realizes a profit
or a loss from his investment due to the stochastic variation of the stock
values. The investor’s goal is to maximize a given performance measure.

\subsection{Reward Function}
Let $W_t$ denote the wealth of the investor at time $t$. The profit realized
between $t$ and $t+1$ is simply given by the difference between the trading
results and the transaction costs payed to the broker. More formally
\begin{equation*}
	\Delta W_{t+1} = W_{t+1} - W_t = \text{NL}_{t+1} - \text{TC}_{t}	
\end{equation*}
where $\text{PNL}_{t+1}$ denotes the profit due to the variation of the
portfolio asset prices between $t$ and $t+1$
\begin{equation*}
	\text{PNL}_{t+1} = {n}_t \cdot \Delta{Z}_{t+1} = \sum^{I}_{i=0} 
	n_t^i (Z_{t+1}^i - Z_t^i) 
\end{equation*}
and $\text{TC}_t$ denotes the fees payed to the broker to change the portfolio
allocation and on the short positions
\begin{equation*}
	\text{TC}_t = \sum^{I}_{i=0} \delta_p^i \left| n_t^i - n_{t-1}^i\right| Z_t^i 
				- \delta_f W_t \ind{{n}_t \neq {n}_{t-1}} 
				- \sum^{I}_{i=0} \delta_s^i {(n_t^i)}^{-} Z_t^i
\end{equation*}
The transaction costs consist of three different components. The first term 
represent a transaction cost that is proportional to the change in value of the 
position in each asset. The second term is a fixed fraction of the total value
of the portfolio which is payed only if the allocation is changed. The last
term represents the fees payed to the broker for the shares borrowed to build a
short position. The portfolio return between $t$ and $t+1$ is thus given by
\begin{equation}\label{eq:portfolio_return}
	X_{t+1} = \frac{\Delta W_{t+1}}{W_t} = \sum^{I}_{i=0} \left[ a_t^i
	X_{t+1}^i - \delta_i \left| a_t^i - \tilde{a}_t^i \right| - \delta_s
	{(a_t^i)}^- \right] - \delta_f \ind{{a}_t \neq \tilde{{a}}_{t-1}}  
\end{equation}
where 
\begin{equation*}
	X_{t+1}^i = \frac{\Delta Z_{t+1}^i}{Z_t^i}
\end{equation*}
is the return of the $i$-th stock between $t$ and $t+1$, 
\begin{equation*}
	a_t^i = \frac{n_t^i Z_t^i}{W_t}
\end{equation*}
is the fraction of wealth invested in the $i$-th stock between time $t$ and
$t+1$ and finally 
\begin{equation*}
	\tilde{a}_t^i = \frac{n_{t-1}^i Z_t^i}{W_t} = \frac{a_{t-1}^i (1+X_t^i)}
	{1 + X_t}
\end{equation*}
is the fraction of wealth invested in the $i$-th stock just before the 
reallocation. We assume that the agent invests all his wealth at each step, so 
that $W_t$ can be also interpreted as the value of his portfolio. This 
assumption leads to the following constraint on the portfolio weights
\begin{equation}
	\sum^{I}_{i=0} a_t^i = 1 \;\;\;\;\; \forall t \in \{0, 1, 2, \ldots\}
\end{equation}
We notice that we are neglecting the typical margin requirements on the short
positions, which would reduce the available capital at time $t$. Considering
margin requirements would lead to a more complex constraint on the portfolio
weights which would be difficult to treat in the reinforcement learning
framework. Plugging this constraint into Eq. (\ref{eq:portfolio_return}), we
obtain
\begin{equation}\label{eq:portfolio_return_benchmark}
	X_{t+1} = X + \sum^{I}_{i=1} a_t^i (X_{t+1}^i - X) - \sum^{I}_{i=0}
	\left[\delta_i \left| a_t^i - \tilde{a}_t^i \right| - \delta_s^i
	{(a_t^i)}^-\right] - \delta_f \ind{{a}_t \neq \tilde{{a}}_{t-1}}   
\end{equation}
which highlights the role of the risk-free asset as a benchmark for the 
portfolio returns. The total profit realized by the investor between $t=0$ and
$T$ is 
\begin{equation*}
	\Pi_T = W_T - W_0 = \sum^{T}_{t=1} \Delta W_t = \sum^{T}_{t=1} W_t X_t  
\end{equation*}
The portfolio return between $t=0$ and $T$ is given by
\begin{equation*}
	X_{0,T} = \frac{W_T}{W_0} - 1 = \prod_{t=1}^T (1+X_t) - 1
\end{equation*}
In order to cast the asset allocation problem in the reinforcement learning
framework, we consider the log-return of the portfolio between $t=0$ and $T$
\begin{equation}
	R_{0,T} = \log \frac{W_T}{W_0} = \sum^{T}_{t=1} \log(1+X_t) = \sum_{t=1}^T
	R_t
\end{equation}
where $R_{t+1}$ is the log-return of the portfolio between $t$ and $t+1$
\begin{equation}
	R_{t+1} = \log \left\{ 1 + \sum^{I}_{i=0} \left[ a_t^i X_{t+1}^i - \delta_i
	\left| a_t^i - \tilde{a}_t^i \right| - \delta_s {(a_t^i)}^- \right] -
	\delta_f \ind{{a}_t \neq \tilde{{a}}_{t-1}}\right\}
\end{equation}
The portfolio return and log-return can be used as the reward function of a
RL algorithm, either in a offline or in an online approach.

\subsection{States}
At each time step, the agent observes the state of the system to  make his decisions. First, we consider the $P+1$ past returns of all risky assets, i.e. $\{X_t, X_{t-1}, \ldots, X_{t-P}\}$. These input variables may be used to construct more complex features for example using some deep learning techniques, such as a deep auto-encoder. In order to properly incorporate the effects of transaction costs into his decision process, the agent must keep track of its current position $\tilde{a}_t$. Finally, we might consider some external variables $Y_t$ that may be relevant to the trader, such as the common technical indicator used in practice. Summing up, we assume that the state of the system is composed in the following way 
\begin{equation}
	S_t = \{X_t, X_{t-1}, \ldots, X_{t-P}, \tilde{a}_t, Y_t, Y_{t-1}, \ldots,
	Y_{t-P}\}
\end{equation}

\subsection{Actions}
The agent, or trading system, only specifies the portfolio weights $a_t = (a_t^0,
\ldots, a_t^I)^T$ and therefore determines the allocation for the time interval
$[t, t+1)$. If we assume that the agent invests all of his capital at each time
step and that short-selling is not allowed, then the portfolio weights should
satisfy, for every $t \in \{0, 1, \ldots\}$, the following constraints
\begin{equation}
	\begin{cases}
		a_t^i \geq 0 \;,\; \forall i \in \{0, \ldots, I\}\\
		\sum^{I}_{i=0} a_t^i = 1 
	\end{cases}
\end{equation}
This constraint can be easily enforced by considering a parametric softmax 
policy. If short-selling is allowed, weights might also be negative. A simple approach is to assume that, for every $t \in \{0, 1, \ldots\}$, the weights satisfy
\begin{equation}
	\begin{cases}
		a_t^i \in \R \;,\; \forall i \in \{1, \ldots, I\}\\
		a_t^0 = 1 - \sum^{I}_{i=1} a_t^i
	\end{cases}
\end{equation}
Since $a_t^0$ is uniquely determined by the other weights, it is enough to
define a policy that specifies the allocation in the risky assets, e.g.\ a
Gaussian policy in $\R^I$. The obvious shortcoming of this approach is that
the agent might enter in huge short positions, which is not realistic. A first
observation is that, if the stochastic policy is concentrated around the
origin, huge long or short positions would have very small probabilities of 
being selected. Moreover, we notice that the agent would pay large fees for 
entering into large short positions, independently of the trading profits. 
Therefore, we expect the agent to learn that short positions are very expensive 
and would therefore try to avoid them.\\
Working in a continuous action space is computationally difficult and only few
reinforcement learning algorithm are well-suited to this setting, e.g. policy
gradient methods. A simpler approach is to reduce the action space to a discrete 
space. For instance, in the two assets scenario we might assume that $a_t \in
\{-1, 0, +1\}$. Thus the agent may be long ($+1$), neutral ($0$) or short
($-1$) on the risky-asset. Working in a discrete action space is more simple,
and standard value-based approaches might also be employed. 
