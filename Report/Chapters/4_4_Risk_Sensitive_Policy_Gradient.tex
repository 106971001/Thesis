\section{Risk-Sensitive Policy Gradient Theorem}
In this section the policy gradient theorem is extended to the risk-sensitive framework. In the average reward formulation of the control problem, the result and its derivation are analogous to those presented in the last section for the risk-neutral framework. This will allow to derive in a trivial way the risk-sensitive versions of all the learning algorithms seen above. The risk-sensitive policy gradient theorem for the average-reward formulation was first derived in \cite{prashanth2014actor} and the presentation of this section closely follows the original article. Obtaining an equivalent theorem for the discounted reward formulation is more challenging. In the article cited above, the authors prove the theorem under a very strong assumption on the dependence of rewards obtained at different time steps which is not verified in many applications, among which is the asset allocation problem that we will consider in the next chapter. We will discuss the problems arising in the discounted setting and why it is not easy to derive a policy gradient theorem in this case. In the original article the authors mostly considered the mean-variance criterion since all the algorithms they propose are easily adapted to the Sharpe ratio criterion. Here we take the opposite direction and present the algorithms for the Sharpe ratio, referring to their article for the mean-variance counterparts.     


Let us consider a family of parametrized policies $\pi_\theta$, with $\theta
\in \Theta \subseteq \R^{D_\theta}$. The optimization problem then becomes
\begin{equation}
	\max_\lambda \min_\theta L(\theta, \lambda) = - \rho(\theta) + \lambda
	(\Lambda(\theta) - \alpha)
\end{equation}
Using a policy gradient approach, the policy parameters are updated following 
the gradient ascent direction
\begin{equation}
	\nabla_\theta L(\theta, \lambda) = - \nabla_\theta \rho(\theta) + \lambda
	\nabla_\theta \Lambda(\theta)
\end{equation}
while the Lagrange multiplier is updated following the gradient descent
direction  
\begin{equation}
	\nabla_\lambda L(\theta, \lambda) = \Lambda(\theta) - \alpha
\end{equation}
Since 
\begin{equation}
	\nabla_\theta \Lambda(\theta) = \nabla_\theta \eta(\theta) - 2 \rho(\theta)
	\nabla_\theta \rho(\theta)
\end{equation}
it is enough to compute $\nabla_\theta \eta(\theta)$ and $\nabla_\theta
\rho(\theta)$. These quantities are provided by the policy gradient theorem 
\begin{theorem}[Policy Gradient]
	\begin{equation}\label{eq:policy_gradient_theorem_Q}
		\begin{split}
			\nabla_\theta \rho(\theta) &= \E[\substack{S\sim d_\pi \\ 
			A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) Q_\theta(S,A)}\\
			&= \int_\S d_\pi(s) \int_\A \pi(s,a) \nabla_\theta \log
			\pi_\theta(s,a) Q_\theta(s,a) da ds	
		\end{split}
	\end{equation}
	\begin{equation}\label{eq:policy_gradient_theorem_W}
		\begin{split}
			\nabla_\theta \eta(\theta) &= \E[\substack{S\sim d_\pi \\ 
			A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) W_\theta(S,A)}\\
			&= \int_\S d_\pi(s) \int_\A \pi(s,a) \nabla_\theta \log
			\pi_\theta(s,a) W_\theta(s,a) da ds	
		\end{split}
	\end{equation}
\end{theorem}
\begin{proof}
	Eq. (\ref{eq:policy_gradient_theorem_Q}) is the standard policy gradient
	and its proof can be found in the literature, e.g. \cite{sutton1999policy}.
	Eq. (\ref{eq:policy_gradient_theorem_W}) can be shown in a similar fashion.
	TODO
\end{proof}
As in the standard risk-neutral case, a state-dependent baseline can be 
introduced in both gradients without changing the result. In particular, by 
using the average adjusted value functions as baseline, we can replace the 
average adjusted action-value functions with the following advantage functions
\begin{equation}
	A_\theta(s,a) = Q_\theta(s,a) - V_\theta(s)
\end{equation}
\begin{equation}
	B_\theta(s,a) = W_\theta(s,a) - U_\theta(s)
\end{equation}
Intuitively, the advantage functions measure how good is to take action $a$ in
state $s$ compared to simply following the policy $\pi_\theta$. The use of a
baseline also serves the purpose of reducing the variance of the gradient
estimates. Thus, the gradients can be written as 
\begin{equation}\label{eq:pg_advantage_Q}
	\nabla_\theta \rho(\theta) = \E[\substack{S\sim d_\pi \\ 
A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) A_\theta(S,A)}
\end{equation}
\begin{equation}\label{eq:pg_advantage_W}
	\nabla_\theta \eta(\theta) = \E[\substack{S\sim d_\pi \\ 
A \sim \pi}]{\nabla_\theta \log \pi_\theta(S,A) B_\theta(S,A)}
\end{equation}
Results of this type form the basis of many actor-critic algorithms, which
employ an approximation of the advantage function to obtain a more accurate
estimate of the objective function. 

\subsection{Risk-Sensitive Actor-Critic Algorithm}
In \cite{prashanth2014actor}, starting from (\ref{eq:pg_advantage_Q}) and 
(\ref{eq:pg_advantage_W}), the authors derive a risk-sensitive actor-critic 
algorithm for the average reward setting. In the algorithm proposed, the
advantage functions are approximated using a temporal difference (TD) scheme in
which a critic maintains a linear approximation of the value functions. More in
detail, let $\delta_n^A$ and $\delta_n^B$ be the TD errors for residual value
and square value functions
\begin{equation}
	\begin{split}
		\delta_t^A &= R_{t+1} - \widehat{\rho}_{t+1} + \widehat{V}(S_{t+1}) -
		\widehat{V}(S_t)\\
		\delta_t^B &= R_{t+1}^2 - \widehat{\eta}_{t+1} + \widehat{U}(S_{t+1}) -
		\widehat{U}(S_t)\\
	\end{split}
\end{equation}
where $\widehat{V}$, $\widehat{U}$, $\widehat{\rho}$ and $\widehat{\eta}$ are
unbiased estimate of $V_\theta$, $U_\theta$, $\rho(\theta)$ and $\eta(\theta)$
respectively. It is easy to show that $\delta_t^A$ and $\delta_t^B$ are
unbiased estimates of the advantage functions.
\begin{proposition}[Temporal Difference Errors]
	\begin{equation}
		\begin{split}
			\E[\theta]{\delta_t^A|S_t = s, A_t = a} &= A_\theta(s, a)\\
			\E[\theta]{\delta_t^B|S_t = s, A_t = a} &= B_\theta(s, a)\\
		\end{split}
	\end{equation}
\end{proposition}
\begin{proof}
	TODO
\end{proof}
Denoting by $\psi_t = \psi(S_t, A_t) = \nabla_\theta \log \pi(S_t, A_t)$ the
compatible feature \cite{sutton1999policy}, we can easily obtain an unbiased
estimate of the gradients 
\begin{equation}
	\begin{split}
		\nabla_\theta \rho(\theta) &\approx \psi_t \delta_t^A\\
		\nabla_\theta \eta(\theta) &\approx \psi_t \delta_t^B\\
	\end{split}
\end{equation}
The value functions are linearly approximated using some veatures vectors
$\Phi_V: \S \to \R^{D_V}$ and $\Phi_U: \S \to \R^{D_U}$ as follows
\begin{equation}
	\begin{split}
		\widehat{V}(s) &= v^T \Phi_V(s)\\
		\widehat{U}(s) &= u^T \Phi_U(s)\\
	\end{split}
\end{equation}
Given all these ingredients, the authors propose a three time-scale stochastic
approximation algorithm whose pseudo-code is illustrated in Algorithm
%\ref{algo:average_reward_risk_sensitive_actor_critic_algorithm}.

% \begin{algorithm}
	%\caption{Average Reward Risk-Sensitive Actor-Critic Algorithm}
	% \label{algo:average_reward_risk_sensitive_actor_critic_algorithm}
	%\begin{algorithmic}
	%	test		
	%\end{algorithmic}
% \end{algorithm}