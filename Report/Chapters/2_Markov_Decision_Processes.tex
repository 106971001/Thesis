\chapter{Markov Decision Processes}

We consider sequential decision-making tasks that can be formulated as a
reinforcement learning (RL) problem. In RL, an agent interacts with a dynamic,
stochastic, and incompleteley known environment, with the goal of optimizing
some measure of its long-term performance. The learning problem can be 
formalized using Markov decision processes and stochastic optimal control 
theory in discrete time.

\section{Markov Decision Processes}
\begin{definition}[Markov Decision Process]
	A Markov decision process (MDP) is a tuple $<\S, \A, \calP, \calR,
	\gamma>$, where
	\begin{enumerate}[label={\roman*)}]
		\item $(\S, \calS)$ is a measurable state space.
		\item $(\A, \calA)$ is a measurable action space. 
		\item $\calP: \S \times \A \times \calS \to \R$ is a Markov transition
			kernel, i.e.
			\begin{enumerate}[label={\alph*)}]
				\item for every $s\in\S$ and $a\in\A$, $B \mapsto \calP(s,a,B)$
					  is a probability distribution over $(\S, \calS)$.
				\item for every $B\in\calS$, $(s,a) \mapsto \calP(s,a,B)$ is
					  a measurable function on $\S \times \A$.
			\end{enumerate}
		\item $\calR: \S \times \A \to \R$ is a reward function.
		\item $\gamma \in (0,1)$ is a discount factor.
   %     \item $\mu: \calS \to \R$ is a probability distribution over $(\S,
			%\calS)$.
	\end{enumerate}
\end{definition}
%$\mu$ is the probability distribution of the initial state of the system. 
The kernel $\calP$ describes the random evolution of the system: suppose that
at time $t$ the system is in state $s$ and that the agent takes action $a$,
then, regardless of the previous history of the system, the probability to find
the system in a state belonging to $B\in\calS$ at time $t+1$ is given by
$\calP(s, a, B)$, i.e.
\begin{equation}
	\calP(s_t, a_t, B) = \P{S_{t+1} \in B | S_t = s, A_t = a}
\end{equation}
Following this random transition, the agent receives a stochastic reward
$R_{t+1}$. The reward function $\calR(s, a)$ gives the expected reward
obtained when action $a$ is taken in state $s$, i.e. 
\begin{equation}
	\calR(s_t, a_t) = \E{R_{t+1} | S_t = s, A_t = a}
\end{equation}

\section{Optimal Control Problem}
At any time step, the agent selects his actions according to a certain policy. 
\begin{definition}[Policy]
	A policy is a function $\pi: \S \times \calA \to \R$ such that
	\begin{enumerate}[label={\roman*)}]
		\item for every $s \in \S$, $C \mapsto \pi(s,C)$ is a probability
			  distribution over $(\A, \calA)$. 
		\item for every $C \in \calA$, $s \mapsto \pi(s, C)$ is a measurable
			  function. 
	\end{enumerate}
\end{definition}
Intuitively, a policy represents a stochastic mapping from the current state of
the system to actions. Deterministic policies are a particular case of this 
general definition. We assumed that the agent's policy is stationary and only
depends on the current state of the system. We might in fact  consider more 
general policies that depends on the whole history of the system. However, as
we will see, we can always find an optimal policy that depends only on the
current state, so that our definition is not restrictive. A policy $\pi$ and an
initial state $s_0 \in \S$ together determine a random state-action-reward 
sequence ${\{(S_t, A_t, R_{t+1})\}}_{t\geq 0}$ with values on $\S \times \A
\times \R$ following the mechanism described above. 
\begin{definition}[History]
	Given an initial state $s_0 \in \S$ and a policy $\pi$, a history (or
	equivalently trajectory or roll-out) of the system is a random sequence
	$H_\pi = {\{(S_t, A_t)\}}_{t\geq 0}$ with values on $\S \times \A$, defined
	on some probability space $(\Omega, \mathcal{F}, \mathbb{P})$, such that for 
	$t = 0, 1, \ldots$
	\begin{equation}
		\begin{cases}
			S_0 = s_0\\
			A_t \sim \pi(S_t, \cdot)\\
			S_{t+1} \sim \calP(S_t, A_t, \cdot)\\
		\end{cases}
	\end{equation}
	we will denote by $(\H, \calH)$ the measurable space of all possible
	histories. 
\end{definition}
Moreover, we observe that
\begin{enumerate}[label={\roman*)}]
	\item the state sequence ${\{S_t\}}_{t\geq 0}$ is a Markov process $<\S,
		  \calP_\pi>$
	  \item the state-reward sequence ${\{(S_t, R_t)\}}_{t\geq 0}$ is a Markov 
		  reward process $<\S, \calP_\pi, \calR_\pi, \gamma>$
\end{enumerate}
where we denoted 
\begin{equation}
	\begin{split}
		\calP_\pi(s, s') &= \int_\A \pi(s, a) \calP(s, a, s') da\\
		\calR_\pi(s) &= \int_\A \pi(s, a) \calR(s, a) da\\
	\end{split}
\end{equation}
The goal of the agent is to maximize a measure of his long-term performance.

\section{Discounted Reward Formulation}
In the discounted reward formulation, the agent's performance is measured using
the return obtained from the initial time step
\begin{definition}[Return]
	The return is the total discounted reward obtained by the agent starting 
	from $t$  
	\begin{equation}
		G_t = \sum^{\infty}_{t=0} \gamma^t R_{t+k+1} 
	\end{equation}
	where $0 < \gamma < 1$ is the discount factor.
\end{definition}
Traditionally, discounting has served two purposes. In some domains, such as
economics, discounting can be used to represent interest earned on rewards, so
that an action that generates an immediate reward will be preferred over one
that generates the same reward some steps into the future. Discounting thus
models the trade-off between immediate and delayed reward: if $\gamma = 0$ the
agent selects his actions in a myopic way, while if $\gamma \to 1$ he acts in a
far-sighted manner. There are other possilble reasons for discounting future
rewards. The first is because it is mathematically convenient, as it avoids
infinite returns and it solves many convergence issues. Another interpretation
is that it models the uncertainty about the future, which may not be fully
represented. Finally, the discount factor can also be seen as the probability
that the world does not stop at a given time step. Since the return is
stochastic, we consider its expected value.  
\begin{definition}[State-Value Function]
	The state-value function $V_\pi: \S \to \R$ is the expected return that can
	be obtained starting from a state and following policy $\pi$
	\begin{equation}
		V_\pi(s) = \E[\pi]{G_t|S_t = s}
	\end{equation}
\end{definition}
where $\mathbb{E}_{\pi}$ indicates that all the actions are selected according
to policy $\pi$. In reinforcement learning, it is useful to consider another
function 
\begin{definition}[Action-Value Function]
	The action-value function $Q_\pi: \S \times \A \to \R$ is the expected 
	return that can be obtained starting from a state, taking an action and
	then following policy $\pi$
	\begin{equation}
		Q_\pi(s,a) = \E[\pi]{G_t|S_t = s, A_t = a}
	\end{equation}
\end{definition}
\begin{definition}[Optimal State-Value Function]
	The optimal state-value function $V_*: \S \to \R$ is the largest expected 
	return that can be obtained starting from a state
	\begin{equation}
		V_*(s) = \sup_\pi V_\pi(s)
	\end{equation}
\end{definition}
\begin{definition}[Optimal Action-Value Function]
	The optimal action-value function $Q_*: \S \times \A \to \R$ is the largest
	expected return that can be obtained starting from a state and taking an
	action
	\begin{equation}
		Q_*(s,a) = \sup_\pi Q_\pi(s,a)
	\end{equation}
\end{definition}
The agent's goal is to select a policy $\pi_*$ that maximize his expected return
in all possible states. Such a policy is called \emph{optimal}. More formally,
we introduce the following partial ordering in the policy space
\begin{equation}
	\pi \succeq \pi' \Leftrightarrow V_\pi(s) \geq V_{\pi'}(s) \;\;\; \forall s \in \S
\end{equation}
Then the optimal policy $\pi_* \succeq \pi$, $\forall \pi$.

\subsection{Bellman Equations}
%TODO


\section{Average Reward Formulation}
Most of the research in RL has studied a problem formulation where agents
maximize the cumulative sum of rewards. However, this approach cannot handle
infinite horizon tasks, where there are no absorbing goal states, without
discounting future rewards. Clearly, discounting is only necessary in cyclical
tasks, where the cumulative reward sum can be unbounded. More natural long-term
measure of  optimality exists for such cyclical tasks, based on maximizing the
average reward per action \cite{mahadevan1996average}. In the average reward
setting, the goal of the agent is to find a policy that maximizes the expected
reward per step. 
\begin{definition}[Average Reward]
	The average reward $\rho_\pi$ associated to a policy $\pi$ is defined as  
	\begin{equation}
		\begin{split}
			\rho_\pi &= \lim_{T\to\infty} \frac{1}{T} \E[\pi]{ \sum^{T-1}_{t=0} R_{t+1}}\\
					 &= \E[\substack{S\sim d_\pi \\ A \sim \pi}]{\calR(S,A)}\\ 
					 &= \int_\S d_\pi(s) \int_\A \pi(s, a) \calR(s,a) da ds\\ 
		\end{split}
	\end{equation}
where $d_\pi$ is the stationary distribution of the Markov process induced by $\pi$.
\end{definition}
The agent aims to find an \emph{average optimal} policy
\begin{equation}
	\pi_* = \argsup_\pi \rho_\pi
\end{equation}
In this setting, we introduce the \emph{average adjusted} value and action-value 
functions. 
\begin{definition}[Average Adjusted State-Value Function]
	The average adjusted state-value function $V_\pi : \S \to \R$ is the
	expected residual return that can be obtained starting from a state and
	following policy $\pi$
	\begin{equation}
		V_\pi(s) = \E[\pi]{\sum^{\infty}_{t=0} \left(R_{t+1} - \rho_\pi\right)
		\bigg| S_0 = s}
	\end{equation}
\end{definition}
The term $V_\pi(s)$ is usually referred to as the \emph{bias} value, or the
\emph{relative} value, since it represents the relative difference in total
reward gained starting from a state $s$ as opposed to a generic state. 
$\rho_\pi$ serves as a baseline that allows to avoid divergence in the value
function definition.
\begin{definition}[Average Adjusted Action-Value Function]
	The average adjusted action-value function $Q_\pi : \S \times \A \to \R$ is 
	the expected residual return that can be obtained starting from a state,
	taking an action and then following policy $\pi$
	\begin{equation}
		Q_\pi(s, a) = \E[\pi]{\sum^{\infty}_{t=0} \left(R_{t+1} -
			\rho_\pi\right) \bigg| S_0 = s, A_0
		= a}
	\end{equation}
\end{definition}
We have the following relation between the state-value function and the
action-value function
\begin{equation}\label{eq:VQ_equality}
	V_\pi(s) = \int_\A \pi(s,a) Q_\pi(s,a)
\end{equation}

\subsection{Bellman Equations}
The action-value function satisfies the following Bellman equation 
\begin{equation}
	\rho_\pi + Q_\pi(s,a) = \calR(s,a) + \int_\S \calP(s,a,s') V_\pi(s') ds'
\end{equation}
Using equality (\ref{eq:VQ_equality}), we obtain the Bellman equation for the
state-value function
\begin{equation}
	\rho_\pi + V_\pi(s) = \int_\A \pi(s,a) \left[\calR(s,a) + \int_\S 
	\calP(s,a,s') V_\pi(s') ds'\right] da
\end{equation}
Denoting by $T_a$ (resp. $T_\pi$) the transition operator for action $a$ (resp. 
for policy $\pi$)
\begin{equation}
	\begin{split}
		T_a V(s) &= \int_\S \calP(s, a, s') V_\pi(s')\\
		T_\pi V(s) &= \int_\A \pi(s,a) \int_\S \calP(s,a,s') V_\pi(s')\\ 
	\end{split}
\end{equation}
The Bellman equations can be rewritten in the shorter form
\begin{equation}
	\begin{split}
		\rho_\pi + V_\pi(s) &= \calR_\pi(s) + T_\pi V_\pi(s)\\
		\rho_\pi + Q_\pi(s,a) &= \calR(s,a) + T_a V_\pi(s)\\
	\end{split}
\end{equation}
In the discrete case, where the transition operator correspond to matrices, 
these Bellman equations become linear systems that can be solved to obtain the
value functions. 

\section{Risk-Sensitive Average Reward Formulation}
In many application, in addition to maximining the average reward, the agent
may want to control risk by minimizing some measure of viariability in rewards.
In \cite{prashanth2014actor}, the authors consider the long-run variance of
$\pi$
\begin{definition}[Long-Run Variance]
	The long-run variance $\Lambda_\pi$ under policy $\pi$ is defined as
	\begin{equation}
		\begin{split}
			\Lambda_\pi &= \lim_{T \to \infty} \frac{1}{T} \E[\pi]{
			\sum^{T-1}_{t=0} (R_{t+1} - \rho_\pi)^2}\\
		\end{split}
	\end{equation}
\end{definition}
The long-run variance can be decomposed as follows
\begin{equation}
	\Lambda_\pi = \eta_\pi - \rho_\pi^2 
\end{equation}
where $\eta_\pi$ is the average square reward per step  
\begin{definition}[Average Square Reward]
	\begin{equation}
		\begin{split}
			\eta_\pi &= \lim_{T\to\infty}\frac{1}{T}\E[\pi]{\sum_{t=0}^{T-1} R_{t+1}^2}\\
					 &= \E[\substack{S\sim d_\pi \\ A \sim \pi}]{\calM(S,A)}\\	
					 &= \int_\S d_\pi(s) \int_\A \pi(s,a) \calM(s,a)\\
		\end{split}
	\end{equation}
\end{definition}
where we denoted by $\calM(s,a)$ the square reward function
\begin{equation}
	\calM(s,a) = \E{R_{t+1}^2 | S_t = s, A_t = a}
\end{equation}
As before, we introduce the residual state-value and action-value functions 
associated with the square reward under policy $\pi$
\begin{definition}[Average Adjusted Square State-Value Function]
	The average adjusted square state-value function $U_\pi : \S \to \R$ is the
	expected square residual return that can be obtained starting from a state 
	and following policy $\pi$
	\begin{equation}
		U_\pi(s) = \E[\pi]{\sum^{\infty}_{t=0} \left(R_{t+1}^2 - \eta_\pi\right)
		\bigg| S_0 = s}
	\end{equation}
\end{definition}
\begin{definition}[Average Adjusted Square Action-Value Function]
	The average adjusted square action-value function $Q_\pi : \S \times \A \to 
	\R$ is the expected residual square return that can be obtained starting from a
	state, taking an action and then following policy $\pi$
	\begin{equation}
		W_\pi(s, a) = \E[\pi]{\sum^{\infty}_{t=0} \left(R_{t+1}^2 -
			\eta_\pi\right) \bigg| S_0 = s, A_0 = a}
	\end{equation}
\end{definition}
We have the following relation between square state-value function and the
square action-value
\begin{equation}\label{eq:UW_equality}
	U_\pi(s) = \int_\A \pi(s,a) W_\pi(s,a)
\end{equation}

\subsection{Bellman Equations}
The average adjusted square action-value function satisfies the following 
Bellman equation
\begin{equation}
	\eta_\pi + W_\pi(s,a) = \calM(s,a) + \int_\S \calP(s,a,s') U_\pi(s') ds'\\
\end{equation}
Using equality (\ref{eq:UW_equality}), we obtain an analogous Bellman equation
for the square state-value function
\begin{equation}
	\eta_\pi + U_\pi(s) = \int_\A \pi(s,a)\left[\calM(s,a) + \int_\S 
	\calP(s,a,s') U_\pi(s') ds'\right]da
\end{equation}

\subsection{Risk-Sensitive Control Problem}
In the risk-sensitive setting, the agent wants to find a policy that solves the 
following optimization problem 
\begin{equation}\label{eq:risk_sensitive_problem}
	\begin{cases}
		\max_\pi \rho_\pi\\
		\text{subject to}\ \Lambda_\pi \leq \alpha\\
	\end{cases}
\end{equation}
for a given $\alpha > 0$. Using the Lagrangian relaxation procedure, we can 
recast (\ref{eq:risk_sensitive_problem}) to the following uncostrained problem
\begin{equation}
	\max_\lambda \min_\pi L(\pi, \lambda) = - \rho_\pi + \lambda 
	(\Lambda_\theta - \alpha)
\end{equation}
Let us observe that the discussion can be easily extended to other
risk-sensitive performance measures, such as the standard mean-variance
criterion or the Sharpe ratio. 
