\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}The March of the Machines}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Artificial Intelligence and Finance}{1}{section.1.2}
\contentsline {section}{\numberline {1.3}Structure}{1}{section.1.3}
\contentsline {chapter}{\numberline {2}Discrete-Time Stochastic Optimal Control}{2}{chapter.2}
\contentsline {section}{\numberline {2.1}Markov Decision Processes}{2}{section.2.1}
\contentsline {section}{\numberline {2.2}Policies}{3}{section.2.2}
\contentsline {section}{\numberline {2.3}Risk-Neutral Framework}{5}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Discounted Reward Formulation}{5}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}Average Reward Formulation}{8}{subsection.2.3.2}
\contentsline {section}{\numberline {2.4}Risk-Sensitive Framework}{9}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Discounted Reward Formulation}{10}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Average Reward Formulation}{13}{subsection.2.4.2}
\contentsline {section}{\numberline {2.5}Dynamic Programming Algorithms}{15}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Value Iteration}{15}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Policy Iteration}{16}{subsection.2.5.2}
\contentsline {chapter}{\numberline {3}Reinforcement Learning}{17}{chapter.3}
\contentsline {section}{\numberline {3.1}The Reinforcement Learning Problem}{18}{section.3.1}
\contentsline {section}{\numberline {3.2}Model-Free RL Methods}{18}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Model Approximation}{20}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Value Approximation}{20}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Policy Approximation}{21}{subsection.3.2.3}
\contentsline {chapter}{\numberline {4}Risk-Neutral Policy Gradient}{22}{chapter.4}
\contentsline {section}{\numberline {4.1}Basics of Policy Gradient Methods}{23}{section.4.1}
\contentsline {section}{\numberline {4.2}Risk-Neutral Objective Functions}{24}{section.4.2}
\contentsline {section}{\numberline {4.3}Finite Differences}{24}{section.4.3}
\contentsline {section}{\numberline {4.4}Likelihood Ratio Methods}{25}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Monte Carlo Policy Gradient}{26}{subsection.4.4.1}
\contentsline {subsubsection}{\numberline {4.4.1.1}Optimal Baseline}{27}{subsubsection.4.4.1.1}
\contentsline {subsection}{\numberline {4.4.2}GPOMDP}{28}{subsection.4.4.2}
\contentsline {subsection}{\numberline {4.4.3}Stochastic Policies}{29}{subsection.4.4.3}
\contentsline {subsubsection}{\numberline {4.4.3.1}Boltzmann Exploration Policy}{29}{subsubsection.4.4.3.1}
\contentsline {subsubsection}{\numberline {4.4.3.2}Gaussian Exploration Policy}{29}{subsubsection.4.4.3.2}
\contentsline {subsection}{\numberline {4.4.4}Policy Gradient with Parameter Exploration}{30}{subsection.4.4.4}
\contentsline {subsubsection}{\numberline {4.4.4.1}Episodic PGPE}{30}{subsubsection.4.4.4.1}
\contentsline {paragraph}{Independent Gaussian Parameter Distribution}{31}{section*.9}
\contentsline {paragraph}{Gaussian Parameter Distribution}{32}{section*.10}
\contentsline {paragraph}{Symmetric Sampling and Gain Normalization}{32}{section*.11}
\contentsline {subsubsection}{\numberline {4.4.4.2}Infinite Horizon PGPE}{33}{subsubsection.4.4.4.2}
\contentsline {section}{\numberline {4.5}Risk-Neutral Policy Gradient Theorem}{34}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Theorem Statement and Proof}{34}{subsection.4.5.1}
\contentsline {subsection}{\numberline {4.5.2}GPOMDP}{36}{subsection.4.5.2}
\contentsline {subsection}{\numberline {4.5.3}Actor-Critic Policy Gradient}{37}{subsection.4.5.3}
\contentsline {subsection}{\numberline {4.5.4}Compatible Function Approximation}{39}{subsection.4.5.4}
\contentsline {subsection}{\numberline {4.5.5}Natural Policy Gradient}{40}{subsection.4.5.5}
\contentsline {subsubsection}{\numberline {4.5.5.1}Formalism of Natural Policy Gradients}{41}{subsubsection.4.5.5.1}
\contentsline {chapter}{\numberline {5}Risk-Sensitive Policy Gradient}{43}{chapter.5}
\contentsline {section}{\numberline {5.1}Risk-Sensitive Framework}{43}{section.5.1}
\contentsline {section}{\numberline {5.2}Monte Carlo Policy Gradient}{45}{section.5.2}
\contentsline {section}{\numberline {5.3}Policy Gradient Theorem}{46}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}Average Reward Formulation}{47}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Risk-Sensitive Actor-Critic Algorithm}{49}{subsection.5.3.2}
\contentsline {subsection}{\numberline {5.3.3}Discounted Reward Formulation}{50}{subsection.5.3.3}
\contentsline {chapter}{\numberline {6}Parameter-Based Policy Gradient}{53}{chapter.6}
\contentsline {section}{\numberline {6.1}Risk-Neutral Framework}{53}{section.6.1}
\contentsline {subsection}{\numberline {6.1.1}Parameter-Based Natural Policy Gradient}{55}{subsection.6.1.1}
\contentsline {section}{\numberline {6.2}Risk-Sensitive Framework}{60}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}Parameter-Based Natural Policy Gradient}{60}{subsection.6.2.1}
\contentsline {chapter}{\numberline {7}Financial Applications of Reinforcement Learning}{62}{chapter.7}
\contentsline {section}{\numberline {7.1}Efficient Market Hypothesis}{62}{section.7.1}
\contentsline {subsection}{\numberline {7.1.1}Formal Definitions of the EMH}{63}{subsection.7.1.1}
\contentsline {subsection}{\numberline {7.1.2}Critics to the EMH}{64}{subsection.7.1.2}
\contentsline {section}{\numberline {7.2}Bibliographical Survey}{64}{section.7.2}
\contentsline {section}{\numberline {7.3}Asset Allocation with Transaction Costs}{64}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}Reward Function}{65}{subsection.7.3.1}
\contentsline {subsection}{\numberline {7.3.2}States}{67}{subsection.7.3.2}
\contentsline {subsection}{\numberline {7.3.3}Actions}{67}{subsection.7.3.3}
\contentsline {chapter}{\numberline {8}Numerical Results for the Asset Allocation Problem}{69}{chapter.8}
\contentsline {section}{\numberline {8.1}Synthetic Risky Asset}{69}{section.8.1}
\contentsline {subsection}{\numberline {8.1.1}Learning Algorithm Specifications}{70}{subsection.8.1.1}
\contentsline {subsubsection}{\numberline {8.1.1.1}ARAC}{70}{subsubsection.8.1.1.1}
\contentsline {subsubsection}{\numberline {8.1.1.2}PGPE}{70}{subsubsection.8.1.1.2}
\contentsline {subsubsection}{\numberline {8.1.1.3}NPGPE}{70}{subsubsection.8.1.1.3}
\contentsline {subsection}{\numberline {8.1.2}Experimental Setup}{71}{subsection.8.1.2}
\contentsline {subsection}{\numberline {8.1.3}Risk-Neutral Framework}{71}{subsection.8.1.3}
\contentsline {subsubsection}{\numberline {8.1.3.1}Convergence}{71}{subsubsection.8.1.3.1}
\contentsline {subsubsection}{\numberline {8.1.3.2}Performances}{72}{subsubsection.8.1.3.2}
\contentsline {subsubsection}{\numberline {8.1.3.3}Impact of Transaction Costs}{73}{subsubsection.8.1.3.3}
\contentsline {subsection}{\numberline {8.1.4}Risk-Sensitive Framework}{74}{subsection.8.1.4}
\contentsline {section}{\numberline {8.2}Historic Risky Asset}{77}{section.8.2}
\contentsline {section}{\numberline {8.3}Multiple Synthetic Risky Assets}{77}{section.8.3}
\contentsline {section}{\numberline {8.4}Historic Multiple Risky Assets}{77}{section.8.4}
\contentsline {chapter}{\numberline {9}Conclusions}{78}{chapter.9}
\contentsline {section}{\numberline {9.1}Summary}{78}{section.9.1}
\contentsline {section}{\numberline {9.2}Further Developments}{78}{section.9.2}
