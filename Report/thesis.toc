\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}The March of the Machines}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Artificial Intelligence and Finance}{1}{section.1.2}
\contentsline {section}{\numberline {1.3}Structure}{1}{section.1.3}
\contentsline {chapter}{\numberline {2}Discrete-Time Stochastic Optimal Control}{3}{chapter.2}
\contentsline {section}{\numberline {2.1}Sequential Decision Problems}{3}{section.2.1}
\contentsline {section}{\numberline {2.2}Markov Decision Processes}{4}{section.2.2}
\contentsline {section}{\numberline {2.3}Policies}{5}{section.2.3}
\contentsline {section}{\numberline {2.4}Risk-Neutral Framework}{6}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Discounted Reward Formulation}{6}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Average Reward Formulation}{9}{subsection.2.4.2}
\contentsline {section}{\numberline {2.5}Risk-Sensitive Framework}{10}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Discounted Reward Formulation}{11}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Average Reward Formulation}{14}{subsection.2.5.2}
\contentsline {section}{\numberline {2.6}Dynamic Programming Algorithms}{16}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Value Iteration}{16}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}Policy Iteration}{17}{subsection.2.6.2}
\contentsline {chapter}{\numberline {3}Reinforcement Learning}{19}{chapter.3}
\contentsline {section}{\numberline {3.1}The Reinforcement Learning Problem}{20}{section.3.1}
\contentsline {section}{\numberline {3.2}Model-Free RL Methods}{20}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Model Approximation}{22}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Value Approximation}{22}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Policy Approximation}{23}{subsection.3.2.3}
\contentsline {chapter}{\numberline {4}Policy Gradient}{25}{chapter.4}
\contentsline {section}{\numberline {4.1}Basics of Policy Gradient Methods}{26}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Risk-Neutral Framework}{27}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Risk-Sensitive Framework}{27}{subsection.4.1.2}
\contentsline {section}{\numberline {4.2}Finite Differences}{29}{section.4.2}
\contentsline {section}{\numberline {4.3}Likelihood Ratio Methods}{29}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Monte Carlo Policy Gradient}{30}{subsection.4.3.1}
\contentsline {subsubsection}{\numberline {4.3.1.1}Optimal Baseline}{32}{subsubsection.4.3.1.1}
\contentsline {subsection}{\numberline {4.3.2}GPOMDP}{32}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Risk-Sensitive Monte Carlo Policy Gradient}{33}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}Stochastic Policies}{33}{subsection.4.3.4}
\contentsline {subsubsection}{\numberline {4.3.4.1}Boltzmann Exploration Policy}{33}{subsubsection.4.3.4.1}
\contentsline {subsubsection}{\numberline {4.3.4.2}Gaussian Exploration Policy}{34}{subsubsection.4.3.4.2}
\contentsline {subsection}{\numberline {4.3.5}Policy Gradient with Parameter Exploration}{34}{subsection.4.3.5}
\contentsline {subsubsection}{\numberline {4.3.5.1}Episodic PGPE}{34}{subsubsection.4.3.5.1}
\contentsline {paragraph}{Independent Gaussian Parameter Distribution}{36}{section*.10}
\contentsline {paragraph}{Gaussian Parameter Distribution}{36}{section*.11}
\contentsline {paragraph}{Symmetric Sampling and Gain Normalization}{37}{section*.12}
\contentsline {subsubsection}{\numberline {4.3.5.2}Infinite Horizon PGPE}{38}{subsubsection.4.3.5.2}
\contentsline {section}{\numberline {4.4}Risk-Neutral Policy Gradient Theorem}{38}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Theorem Statement and Proof}{39}{subsection.4.4.1}
\contentsline {subsection}{\numberline {4.4.2}GPOMDP}{41}{subsection.4.4.2}
\contentsline {subsection}{\numberline {4.4.3}Actor-Critic Policy Gradient}{41}{subsection.4.4.3}
\contentsline {subsection}{\numberline {4.4.4}Compatible Function Approximation}{44}{subsection.4.4.4}
\contentsline {subsection}{\numberline {4.4.5}Natural Policy Gradient}{45}{subsection.4.4.5}
\contentsline {subsubsection}{\numberline {4.4.5.1}Formalism of Natural Policy Gradients}{45}{subsubsection.4.4.5.1}
\contentsline {section}{\numberline {4.5}Risk-Sensitive Policy Gradient Theorem}{47}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Average Reward Formulation}{48}{subsection.4.5.1}
\contentsline {subsection}{\numberline {4.5.2}Risk-Sensitive Actor-Critic Algorithm}{50}{subsection.4.5.2}
\contentsline {subsection}{\numberline {4.5.3}Discounted Reward Formulation}{51}{subsection.4.5.3}
\contentsline {section}{\numberline {4.6}Parameter-Based Policy Gradient}{53}{section.4.6}
\contentsline {subsection}{\numberline {4.6.1}Risk-Neutral Setting}{54}{subsection.4.6.1}
\contentsline {subsection}{\numberline {4.6.2}Parameter-Based Natural Policy Gradient}{56}{subsection.4.6.2}
\contentsline {subsubsection}{\numberline {4.6.2.1}NPGPE}{56}{subsubsection.4.6.2.1}
\contentsline {subsection}{\numberline {4.6.3}Risk-Sensitive Setting}{58}{subsection.4.6.3}
\contentsline {subsubsection}{\numberline {4.6.3.1}Risk-Sensitive NPGPE}{59}{subsubsection.4.6.3.1}
\contentsline {chapter}{\numberline {5}Financial Applications of Reinforcement Learning}{61}{chapter.5}
\contentsline {section}{\numberline {5.1}Bibliographical Survey}{61}{section.5.1}
\contentsline {section}{\numberline {5.2}Asset Allocation}{61}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Reward Function}{61}{subsection.5.2.1}
\contentsline {subsection}{\numberline {5.2.2}States}{64}{subsection.5.2.2}
\contentsline {subsection}{\numberline {5.2.3}Actions}{64}{subsection.5.2.3}
\contentsline {chapter}{\numberline {6}Numerical Results for the Asset Allocation Problem}{67}{chapter.6}
\contentsline {section}{\numberline {6.1}Synthetic Risky Asset}{67}{section.6.1}
\contentsline {subsection}{\numberline {6.1.1}Learning Algorithm Specifications}{68}{subsection.6.1.1}
\contentsline {subsubsection}{\numberline {6.1.1.1}ARAC}{68}{subsubsection.6.1.1.1}
\contentsline {subsubsection}{\numberline {6.1.1.2}PGPE}{68}{subsubsection.6.1.1.2}
\contentsline {subsubsection}{\numberline {6.1.1.3}NPGPE}{68}{subsubsection.6.1.1.3}
\contentsline {subsection}{\numberline {6.1.2}Experimental Setup}{69}{subsection.6.1.2}
\contentsline {subsection}{\numberline {6.1.3}Risk-Neutral Framework}{69}{subsection.6.1.3}
\contentsline {subsubsection}{\numberline {6.1.3.1}Convergence}{69}{subsubsection.6.1.3.1}
\contentsline {subsubsection}{\numberline {6.1.3.2}Performances}{70}{subsubsection.6.1.3.2}
\contentsline {subsubsection}{\numberline {6.1.3.3}Impact of Transaction Costs}{71}{subsubsection.6.1.3.3}
\contentsline {subsection}{\numberline {6.1.4}Risk-Sensitive Framework}{72}{subsection.6.1.4}
\contentsline {section}{\numberline {6.2}Historic Risky Asset}{75}{section.6.2}
\contentsline {section}{\numberline {6.3}Multiple Synthetic Risky Assets}{75}{section.6.3}
\contentsline {section}{\numberline {6.4}Historic Multiple Risky Assets}{75}{section.6.4}
\contentsline {chapter}{\numberline {7}Conclusions}{77}{chapter.7}
\contentsline {section}{\numberline {7.1}Summary}{77}{section.7.1}
\contentsline {section}{\numberline {7.2}Further Developments}{77}{section.7.2}
