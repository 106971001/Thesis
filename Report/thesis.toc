\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {chapter}{\numberline {2}Discrete-Time Stochastic Optimal Control}{3}{chapter.2}
\contentsline {section}{\numberline {2.1}Sequential Decision Problems}{3}{section.2.1}
\contentsline {section}{\numberline {2.2}Markov Decision Processes}{4}{section.2.2}
\contentsline {section}{\numberline {2.3}Policies}{5}{section.2.3}
\contentsline {section}{\numberline {2.4}Discounted Reward Formulation}{6}{section.2.4}
\contentsline {section}{\numberline {2.5}Average Reward Formulation}{9}{section.2.5}
\contentsline {section}{\numberline {2.6}Risk-Sensitive Formulation}{10}{section.2.6}
\contentsline {section}{\numberline {2.7}Policy Evaluation and Policy Iteration}{12}{section.2.7}
\contentsline {chapter}{\numberline {3}Reinforcement Learning}{13}{chapter.3}
\contentsline {section}{\numberline {3.1}The Reinforcement Learning Problem}{14}{section.3.1}
\contentsline {section}{\numberline {3.2}Model-Free RL Methods}{14}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Model Approximation}{16}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Value Approximation}{16}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Policy Approximation}{17}{subsection.3.2.3}
\contentsline {chapter}{\numberline {4}Policy Gradient}{19}{chapter.4}
\contentsline {section}{\numberline {4.1}Basics of Policy Gradient Methods}{20}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Risk-Neutral Framework}{21}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Risk-Sensitive Framework}{21}{subsection.4.1.2}
\contentsline {section}{\numberline {4.2}Finite Differences}{23}{section.4.2}
\contentsline {section}{\numberline {4.3}Likelihood Ratio Methods}{23}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Monte Carlo Policy Gradient}{24}{subsection.4.3.1}
\contentsline {subsubsection}{\numberline {4.3.1.1}Optimal Baseline}{26}{subsubsection.4.3.1.1}
\contentsline {subsection}{\numberline {4.3.2}GPOMDP}{26}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Risk-Sensitive Monte Carlo Policy Gradient}{27}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}Stochastic Policies}{27}{subsection.4.3.4}
\contentsline {subsubsection}{\numberline {4.3.4.1}Boltzmann Exploration Policy}{27}{subsubsection.4.3.4.1}
\contentsline {subsubsection}{\numberline {4.3.4.2}Gaussian Exploration Policy}{28}{subsubsection.4.3.4.2}
\contentsline {subsection}{\numberline {4.3.5}Policy Gradient with Parameter Exploration}{28}{subsection.4.3.5}
\contentsline {subsubsection}{\numberline {4.3.5.1}Episodic PGPE}{28}{subsubsection.4.3.5.1}
\contentsline {paragraph}{Independent Gaussian Parameter Distribution}{30}{section*.8}
\contentsline {paragraph}{Gaussian Parameter Distribution}{30}{section*.9}
\contentsline {paragraph}{Symmetric Sampling and Gain Normalization}{31}{section*.10}
\contentsline {subsubsection}{\numberline {4.3.5.2}Infinite Horizon PGPE}{32}{subsubsection.4.3.5.2}
\contentsline {section}{\numberline {4.4}Policy Gradient Theorem}{32}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Risk-Neutral Setting}{32}{subsection.4.4.1}
\contentsline {subsection}{\numberline {4.4.2}Risk-Sensitive Setting}{34}{subsection.4.4.2}
\contentsline {subsection}{\numberline {4.4.3}GPOMDP from the Policy Gradient Theorem}{34}{subsection.4.4.3}
\contentsline {subsection}{\numberline {4.4.4}Compatible Function Approximation}{34}{subsection.4.4.4}
\contentsline {subsection}{\numberline {4.4.5}Actor-Critic Policy Gradient}{34}{subsection.4.4.5}
\contentsline {subsubsection}{\numberline {4.4.5.1}QAC}{35}{subsubsection.4.4.5.1}
\contentsline {subsubsection}{\numberline {4.4.5.2}ARRSAC}{35}{subsubsection.4.4.5.2}
\contentsline {subsection}{\numberline {4.4.6}Natural Policy Gradient}{35}{subsection.4.4.6}
\contentsline {section}{\numberline {4.5}Risk-Sensitive Policy Gradient}{35}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Risk-Sensitive Actor-Critic Algorithm}{36}{subsection.4.5.1}
\contentsline {section}{\numberline {4.6}Parameter-Based Policy Gradient Theorem}{37}{section.4.6}
\contentsline {subsection}{\numberline {4.6.1}Risk-Neutral Setting}{37}{subsection.4.6.1}
\contentsline {subsection}{\numberline {4.6.2}Risk-Sensitive Setting}{37}{subsection.4.6.2}
\contentsline {subsection}{\numberline {4.6.3}Natural Policy Gradient}{37}{subsection.4.6.3}
\contentsline {subsubsection}{\numberline {4.6.3.1}NPGPE}{37}{subsubsection.4.6.3.1}
\contentsline {subsubsection}{\numberline {4.6.3.2}Risk-Sensitive NPGPE}{37}{subsubsection.4.6.3.2}
\contentsline {chapter}{\numberline {5}Asset Allocation}{39}{chapter.5}
\contentsline {section}{\numberline {5.1}Reward Function}{39}{section.5.1}
\contentsline {section}{\numberline {5.2}States}{41}{section.5.2}
\contentsline {section}{\numberline {5.3}Actions}{42}{section.5.3}
