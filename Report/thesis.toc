\select@language {english}
\select@language {italian}
\select@language {english}
\select@language {italian}
\contentsline {chapter}{List of Figures}{vii}{chapter*.3}
\contentsline {chapter}{List of Tables}{ix}{chapter*.4}
\contentsline {chapter}{List of Algorithms}{xi}{chapter*.5}
\contentsline {chapter}{Acronyms}{xiii}{chapter*.7}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}The Computerization of Finance}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}The New Dawn of Artificial Intelligence}{2}{section.1.2}
\contentsline {section}{\numberline {1.3}Structure}{3}{section.1.3}
\contentsline {chapter}{\numberline {2}Discrete-Time Stochastic Optimal Control}{5}{chapter.2}
\contentsline {section}{\numberline {2.1}Markov Decision Processes}{5}{section.2.1}
\contentsline {section}{\numberline {2.2}Policies}{6}{section.2.2}
\contentsline {section}{\numberline {2.3}Risk-Neutral Framework}{7}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Discounted Reward Formulation}{8}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}Average Reward Formulation}{10}{subsection.2.3.2}
\contentsline {section}{\numberline {2.4}Risk-Sensitive Framework}{12}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Discounted Reward Formulation}{12}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Average Reward Formulation}{15}{subsection.2.4.2}
\contentsline {section}{\numberline {2.5}Dynamic Programming Algorithms}{16}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Value Iteration}{17}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Policy Iteration}{17}{subsection.2.5.2}
\contentsline {chapter}{\numberline {3}Reinforcement Learning}{19}{chapter.3}
\contentsline {section}{\numberline {3.1}The Reinforcement Learning Problem}{19}{section.3.1}
\contentsline {section}{\numberline {3.2}Model-Free RL Methods}{20}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Model Approximation}{21}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Value Approximation}{22}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Policy Approximation}{22}{subsection.3.2.3}
\contentsline {chapter}{\numberline {4}Risk-Neutral Policy Gradient}{23}{chapter.4}
\contentsline {section}{\numberline {4.1}Basics of Policy Gradient Methods}{23}{section.4.1}
\contentsline {section}{\numberline {4.2}Risk-Neutral Objective Functions}{24}{section.4.2}
\contentsline {section}{\numberline {4.3}Finite Differences}{25}{section.4.3}
\contentsline {section}{\numberline {4.4}Likelihood Ratio Methods}{25}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Monte Carlo Policy Gradient}{26}{subsection.4.4.1}
\contentsline {subsubsection}{\numberline {4.4.1.1}Optimal Baseline}{28}{subsubsection.4.4.1.1}
\contentsline {subsection}{\numberline {4.4.2}GPOMDP}{28}{subsection.4.4.2}
\contentsline {subsection}{\numberline {4.4.3}Stochastic Policies}{29}{subsection.4.4.3}
\contentsline {subsubsection}{\numberline {4.4.3.1}Boltzmann Exploration Policy}{29}{subsubsection.4.4.3.1}
\contentsline {subsubsection}{\numberline {4.4.3.2}Gaussian Exploration Policy}{29}{subsubsection.4.4.3.2}
\contentsline {subsection}{\numberline {4.4.4}Policy Gradient with Parameter Exploration}{30}{subsection.4.4.4}
\contentsline {subsubsection}{\numberline {4.4.4.1}Episodic PGPE}{30}{subsubsection.4.4.4.1}
\contentsline {paragraph}{Independent Gaussian Parameter Distribution}{31}{section*.11}
\contentsline {paragraph}{Gaussian Parameter Distribution}{32}{section*.12}
\contentsline {paragraph}{Symmetric Sampling and Gain Normalization}{32}{section*.13}
\contentsline {subsubsection}{\numberline {4.4.4.2}Infinite Horizon PGPE}{33}{subsubsection.4.4.4.2}
\contentsline {section}{\numberline {4.5}Risk-Neutral Policy Gradient Theorem}{33}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Theorem Statement and Proof}{34}{subsection.4.5.1}
\contentsline {subsection}{\numberline {4.5.2}GPOMDP}{36}{subsection.4.5.2}
\contentsline {subsection}{\numberline {4.5.3}Actor-Critic Policy Gradient}{36}{subsection.4.5.3}
\contentsline {subsection}{\numberline {4.5.4}Compatible Function Approximation}{39}{subsection.4.5.4}
\contentsline {subsection}{\numberline {4.5.5}Natural Policy Gradient}{39}{subsection.4.5.5}
\contentsline {subsubsection}{\numberline {4.5.5.1}Formalism of Natural Policy Gradients}{40}{subsubsection.4.5.5.1}
\contentsline {chapter}{\numberline {5}Risk-Sensitive Policy Gradient}{43}{chapter.5}
\contentsline {section}{\numberline {5.1}Risk-Sensitive Framework}{43}{section.5.1}
\contentsline {section}{\numberline {5.2}Monte Carlo Policy Gradient}{45}{section.5.2}
\contentsline {section}{\numberline {5.3}Policy Gradient Theorem}{46}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}Average Reward Formulation}{47}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Risk-Sensitive Actor-Critic Algorithm}{48}{subsection.5.3.2}
\contentsline {subsection}{\numberline {5.3.3}Discounted Reward Formulation}{49}{subsection.5.3.3}
\contentsline {chapter}{\numberline {6}Parameter-Based Policy Gradient}{53}{chapter.6}
\contentsline {section}{\numberline {6.1}Risk-Neutral Framework}{53}{section.6.1}
\contentsline {subsection}{\numberline {6.1.1}Parameter-Based Natural Policy Gradient}{55}{subsection.6.1.1}
\contentsline {section}{\numberline {6.2}Risk-Sensitive Framework}{60}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}Parameter-Based Natural Policy Gradient}{60}{subsection.6.2.1}
\contentsline {chapter}{\numberline {7}Financial Applications of Reinforcement Learning}{63}{chapter.7}
\contentsline {section}{\numberline {7.1}Efficient Market Hypothesis}{63}{section.7.1}
\contentsline {subsection}{\numberline {7.1.1}Formal Definitions of the EMH}{64}{subsection.7.1.1}
\contentsline {subsection}{\numberline {7.1.2}Critics to the EMH}{65}{subsection.7.1.2}
\contentsline {section}{\numberline {7.2}Bibliographical Survey}{66}{section.7.2}
\contentsline {subsection}{\numberline {7.2.1}Asset Allocation with Transaction Costs}{67}{subsection.7.2.1}
\contentsline {subsection}{\numberline {7.2.2}Optimal Order Execution in Limit Order Book}{68}{subsection.7.2.2}
\contentsline {subsection}{\numberline {7.2.3}Smart Order Routing Across Dark Pools}{69}{subsection.7.2.3}
\contentsline {section}{\numberline {7.3}Asset Allocation with Transaction Costs}{70}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}Wealth Dynamics}{70}{subsection.7.3.1}
\contentsline {subsection}{\numberline {7.3.2}Rewards and Objective Functions}{72}{subsection.7.3.2}
\contentsline {subsubsection}{\numberline {7.3.2.1}Infinite Horizon Task}{72}{subsubsection.7.3.2.1}
\contentsline {subsubsection}{\numberline {7.3.2.2}Episodic Task}{73}{subsubsection.7.3.2.2}
\contentsline {subsection}{\numberline {7.3.3}States}{73}{subsection.7.3.3}
\contentsline {subsection}{\numberline {7.3.4}Actions}{74}{subsection.7.3.4}
\contentsline {chapter}{\numberline {8}Numerical Results for the Asset Allocation Problem}{77}{chapter.8}
\contentsline {section}{\numberline {8.1}Synthetic Risky Asset}{77}{section.8.1}
\contentsline {subsection}{\numberline {8.1.1}Specifications of the Learning Algorithms}{78}{subsection.8.1.1}
\contentsline {paragraph}{ARAC}{78}{section*.16}
\contentsline {paragraph}{PGPE}{78}{section*.17}
\contentsline {paragraph}{NPGPE}{78}{section*.18}
\contentsline {subsection}{\numberline {8.1.2}Experimental Setup}{78}{subsection.8.1.2}
\contentsline {subsection}{\numberline {8.1.3}Risk-Neutral Framework}{79}{subsection.8.1.3}
\contentsline {subsubsection}{\numberline {8.1.3.1}Convergence}{79}{subsubsection.8.1.3.1}
\contentsline {subsubsection}{\numberline {8.1.3.2}Performances}{79}{subsubsection.8.1.3.2}
\contentsline {subsubsection}{\numberline {8.1.3.3}Impact of Transaction Costs}{80}{subsubsection.8.1.3.3}
\contentsline {subsection}{\numberline {8.1.4}Risk-Sensitive Framework}{82}{subsection.8.1.4}
\contentsline {subsubsection}{\numberline {8.1.4.1}Risk-Neutral vs. Risk-Sensitive}{85}{subsubsection.8.1.4.1}
\contentsline {subsubsection}{\numberline {8.1.4.2}Impact of Transaction Costs}{85}{subsubsection.8.1.4.2}
\contentsline {section}{\numberline {8.2}Historical Risky Asset}{87}{section.8.2}
\contentsline {subsection}{\numberline {8.2.1}Risk-Neutral Framwork}{87}{subsection.8.2.1}
\contentsline {subsection}{\numberline {8.2.2}Risk-Sensitive Framework}{89}{subsection.8.2.2}
\contentsline {subsection}{\numberline {8.2.3}The Challenge of Historical Data}{89}{subsection.8.2.3}
\contentsline {section}{\numberline {8.3}Multiple Synthetic Risky Assets}{91}{section.8.3}
\contentsline {subsection}{\numberline {8.3.1}Specifications of the Learning Algorithms}{92}{subsection.8.3.1}
\contentsline {chapter}{\numberline {9}Conclusions}{95}{chapter.9}
\contentsline {section}{\numberline {9.1}Summary}{95}{section.9.1}
\contentsline {section}{\numberline {9.2}Further Developments}{95}{section.9.2}
\contentsline {chapter}{Appendices}{97}{section*.37}
\contentsline {chapter}{\numberline {A}Implementation}{97}{Appendix.1.A}
\contentsline {section}{\numberline {A.1}Python Prototype}{97}{section.1.A.1}
\contentsline {section}{\numberline {A.2}C++ Implementation}{98}{section.1.A.2}
\contentsline {subsection}{\numberline {A.2.1}\lstinline {Environment}, \lstinline {Task}, \lstinline {Agent} and \lstinline {Experiment}}{99}{subsection.1.A.2.1}
\contentsline {subsection}{\numberline {A.2.2}\lstinline {ARACAgent}}{101}{subsection.1.A.2.2}
\contentsline {section}{\numberline {A.3}Execution Pipeline}{104}{section.1.A.3}
\contentsline {subsection}{\numberline {A.3.1}Compilation}{104}{subsection.1.A.3.1}
\contentsline {subsection}{\numberline {A.3.2}\lstinline {generate_synthetic_series.py}}{104}{subsection.1.A.3.2}
\contentsline {subsection}{\numberline {A.3.3}\lstinline {experiment_launcher.py}}{104}{subsection.1.A.3.3}
\contentsline {subsection}{\numberline {A.3.4}\lstinline {main_thesis}}{104}{subsection.1.A.3.4}
\contentsline {subsection}{\numberline {A.3.5}\lstinline {postprocessing.py}}{105}{subsection.1.A.3.5}
\contentsline {chapter}{Bibliography}{107}{chapter*.42}
